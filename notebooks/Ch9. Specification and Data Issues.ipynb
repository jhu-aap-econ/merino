{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d3db9d2",
   "metadata": {},
   "source": [
    "# Chapter 9: Specification and Data Issues\n",
    "\n",
    "Regression analysis beyond the basic OLS assumptions requires careful attention to several critical issues. This chapter addresses five key challenges that arise in empirical econometric work: functional form specification, measurement error in variables, missing data patterns, influential outliers, and robust estimation methods. Each issue can substantially affect coefficient estimates, standard errors, and statistical inference if not properly addressed.\n",
    "\n",
    "The organization follows a hierarchical development from foundational concepts to advanced applications. We begin with functional form misspecification and formal tests (Section 9.1), examine the distinct consequences of measurement error in dependent versus explanatory variables (Section 9.2), analyze missing data mechanisms and their implications (Section 9.3), identify and handle outlying observations (Section 9.4-9.5), and conclude with two important extensions: proxy variables for unobserved factors (Section 9.6) and models with random coefficients (Section 9.7).\n",
    "\n",
    "Throughout this chapter, we demonstrate theoretical results through simulation studies and illustrate practical applications using real econometric datasets. The chapter concludes with comprehensive guidance on diagnostic procedures and decision frameworks for applied research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbadb3c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:51:04.570202Z",
     "iopub.status.busy": "2025-10-19T23:51:04.570113Z",
     "iopub.status.idle": "2025-10-19T23:51:04.573911Z",
     "shell.execute_reply": "2025-10-19T23:51:04.573667Z"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install matplotlib numpy pandas statsmodels wooldridge scipy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce9220f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:51:04.575129Z",
     "iopub.status.busy": "2025-10-19T23:51:04.575047Z",
     "iopub.status.idle": "2025-10-19T23:51:05.781282Z",
     "shell.execute_reply": "2025-10-19T23:51:05.780941Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.outliers_influence as smo  # For RESET test and outlier diagnostics\n",
    "import wooldridge as wool\n",
    "from IPython.display import display\n",
    "from scipy import stats  # For generating random numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Functional Form Misspecification\n",
    "\n",
    "Assumption MLR.1 from Chapter 3 requires correct specification of the population regression function. When the true relationship between variables is non-linear but we impose a linear specification, or when we omit important interaction terms, coefficient estimates become biased and inconsistent. This section develops formal tests for detecting functional form misspecification and presents methods for comparing alternative specifications.\n",
    "\n",
    "**Consequences of misspecification.** Using an incorrect functional form creates several problems: (i) biased coefficient estimates, since omitted non-linear terms act as omitted variables; (ii) invalid inference, as standard errors and test statistics assume correct specification; (iii) poor out-of-sample predictions, particularly when extrapolating beyond the observed range; and (iv) incorrect marginal effects and economic interpretation.\n",
    "\n",
    "**Testing strategy.** We consider two complementary approaches: the RESET test for detecting general misspecification within a given model, and the Davidson-MacKinnon J-test for comparing non-nested alternative specifications. Both tests are straightforward to implement and provide valuable diagnostic information about model adequacy.\n",
    "\n",
    "### 9.1.1 The RESET Test\n",
    "\n",
    "The Regression Specification Error Test, developed by Ramsey (1969), provides a general test for functional form misspecification. The test augments the original regression with powers of the fitted values and tests their joint significance. If these polynomial terms are significant, they indicate that the original specification fails to capture important non-linearities in the data.\n",
    "\n",
    "**Test construction.** Consider the baseline model:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k + u\n",
    "$$\n",
    "\n",
    "Estimate this model by OLS and obtain fitted values $\\hat{y}_i$. The RESET test estimates the augmented regression:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k + \\delta_2 \\hat{y}^2 + \\delta_3 \\hat{y}^3 + \\text{error}\n",
    "$$\n",
    "\n",
    "and tests the joint hypothesis $H_0: \\delta_2 = \\delta_3 = 0$ using an F-test. Under the null hypothesis of correct specification, the F-statistic follows an F-distribution with $(q, n-k-q-1)$ degrees of freedom, where $q$ is the number of polynomial terms added (typically 2).\n",
    "\n",
    "**Intuition.** If the original model omits squares, cubes, or interactions of the $x$ variables, these may be partially captured by $\\hat{y}^2$ and $\\hat{y}^3$, since $\\hat{y}$ is a linear combination of the regressors. Significant coefficients on these terms signal that additional functional form flexibility is needed.\n",
    "\n",
    "### 9.1.2 Example: Housing Price Equation\n",
    "\n",
    "We apply the RESET test to the housing price model from Chapter 8. The baseline specification regresses house price (in thousands of dollars) on lot size, square footage, and number of bedrooms. This linear specification imposes constant marginal effects and rules out interaction terms or non-linearities in these relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:51:05.782810Z",
     "iopub.status.busy": "2025-10-19T23:51:05.782676Z",
     "iopub.status.idle": "2025-10-19T23:51:05.799327Z",
     "shell.execute_reply": "2025-10-19T23:51:05.799062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Std_Error</th>\n",
       "      <th>t_stat</th>\n",
       "      <th>p_value</th>\n",
       "      <th>Test_Term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Intercept</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>166.0973</td>\n",
       "      <td>317.4325</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.6022</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lotsize</th>\n",
       "      <td>lotsize</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.9765</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sqrft</th>\n",
       "      <td>sqrft</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.2993</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.9532</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bdrms</th>\n",
       "      <td>bdrms</td>\n",
       "      <td>2.1749</td>\n",
       "      <td>33.8881</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.9490</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fitted_sq</th>\n",
       "      <td>fitted_sq</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.9604</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fitted_cub</th>\n",
       "      <td>fitted_cub</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.8142</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Variable  Coefficient  Std_Error  t_stat  p_value Test_Term\n",
       "Intercept    Intercept     166.0973   317.4325   0.523   0.6022        No\n",
       "lotsize        lotsize       0.0002     0.0052   0.030   0.9765        No\n",
       "sqrft            sqrft       0.0176     0.2993   0.059   0.9532        No\n",
       "bdrms            bdrms       2.1749    33.8881   0.064   0.9490        No\n",
       "fitted_sq    fitted_sq       0.0004     0.0071   0.050   0.9604       YES\n",
       "fitted_cub  fitted_cub       0.0000     0.0000   0.236   0.8142       YES"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RESET Test Implementation: Detecting Functional Form Misspecification\n",
    "# The test adds powers of fitted values to detect omitted nonlinearities\n",
    "\n",
    "# Load housing price data\n",
    "hprice1 = wool.data(\"hprice1\")\n",
    "\n",
    "# Dataset info\n",
    "data_info = pd.DataFrame(\n",
    "    {\n",
    "        \"Metric\": [\"Number of houses\", \"Number of variables\"],\n",
    "        \"Value\": [hprice1.shape[0], hprice1.shape[1]],\n",
    "    },\n",
    ")\n",
    "data_info\n",
    "\n",
    "# Step 1: Estimate the baseline linear model\n",
    "# This is our null hypothesis specification\n",
    "baseline_model = smf.ols(\n",
    "    formula=\"price ~ lotsize + sqrft + bdrms\",\n",
    "    data=hprice1,\n",
    ")\n",
    "baseline_results = baseline_model.fit()\n",
    "\n",
    "# Baseline model summary\n",
    "baseline_summary = pd.DataFrame(\n",
    "    {\n",
    "        \"Metric\": [\"Dependent variable\", \"R-squared\", \"Adjusted R-squared\"],\n",
    "        \"Value\": [\n",
    "            \"price (house price in $1000s)\",\n",
    "            f\"{baseline_results.rsquared:.4f}\",\n",
    "            f\"{baseline_results.rsquared_adj:.4f}\",\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "baseline_summary\n",
    "\n",
    "# Step 2: Generate polynomial terms from fitted values\n",
    "# Theory: If model is misspecified, powers of y_hat capture omitted terms\n",
    "hprice1[\"fitted_sq\"] = baseline_results.fittedvalues**2  # y_hat^2\n",
    "hprice1[\"fitted_cub\"] = baseline_results.fittedvalues**3  # y_hat^3\n",
    "\n",
    "# RESET test construction details\n",
    "reset_info = pd.DataFrame(\n",
    "    {\n",
    "        \"Component\": [\"Original predictors\", \"Added test terms\", \"H_0\", \"H_1\"],\n",
    "        \"Description\": [\n",
    "            \"lotsize, sqrft, bdrms\",\n",
    "            \"fitted^2, fitted^3\",\n",
    "            \"Coefficients on fitted^2 and fitted^3 = 0 (no misspecification)\",\n",
    "            \"At least one polynomial term != 0 (misspecification present)\",\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "reset_info\n",
    "\n",
    "# Step 3: Estimate augmented regression with polynomial terms\n",
    "augmented_reset = smf.ols(\n",
    "    formula=\"price ~ lotsize + sqrft + bdrms + fitted_sq + fitted_cub\",\n",
    "    data=hprice1,\n",
    ")\n",
    "augmented_results = augmented_reset.fit()\n",
    "\n",
    "# Display auxiliary regression results with interpretation\n",
    "reset_table = pd.DataFrame(\n",
    "    {\n",
    "        \"Variable\": augmented_results.params.index,\n",
    "        \"Coefficient\": augmented_results.params.round(4),\n",
    "        \"Std_Error\": augmented_results.bse.round(4),\n",
    "        \"t_stat\": augmented_results.tvalues.round(3),\n",
    "        \"p_value\": augmented_results.pvalues.round(4),\n",
    "        \"Test_Term\": [\"No\", \"No\", \"No\", \"No\", \"YES\", \"YES\"],  # Mark RESET test terms\n",
    "    },\n",
    ")\n",
    "\n",
    "# Display RESET auxiliary regression results\n",
    "display(reset_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:51:05.800595Z",
     "iopub.status.busy": "2025-10-19T23:51:05.800509Z",
     "iopub.status.idle": "2025-10-19T23:51:05.805728Z",
     "shell.execute_reply": "2025-10-19T23:51:05.805475Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>F-statistic</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Manual F-Test</td>\n",
       "      <td>4.6682</td>\n",
       "      <td>0.0120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Method F-statistic p-value\n",
       "0  Manual F-Test      4.6682  0.0120"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Perform an F-test for the joint significance of the added terms\n",
    "# H0: Coefficients on fitted_sq and fitted_cub are both zero.\n",
    "hypotheses = [\"fitted_sq = 0\", \"fitted_cub = 0\"]\n",
    "ftest_man = augmented_results.f_test(hypotheses)\n",
    "fstat_man = ftest_man.statistic  # Extract F-statistic value\n",
    "fpval_man = ftest_man.pvalue\n",
    "\n",
    "# RESET Test (Manual F-Test)\n",
    "reset_manual = pd.DataFrame(\n",
    "    {\n",
    "        \"Method\": [\"Manual F-Test\"],\n",
    "        \"F-statistic\": [f\"{fstat_man:.4f}\"],\n",
    "        \"p-value\": [f\"{fpval_man:.4f}\"],\n",
    "    },\n",
    ")\n",
    "reset_manual\n",
    "\n",
    "# Interpretation (Manual RESET): The F-statistic is 4.6682 and the p-value is 0.0120.\n",
    "# Since the p-value is less than 0.05, we reject the null hypothesis.\n",
    "# This suggests that the original linear model suffers from functional form misspecification.\n",
    "# Non-linear terms (perhaps logs, squares, or interactions) might be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbb4f39",
   "metadata": {},
   "source": [
    "`statsmodels` also provides a convenient function for the RESET test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:51:05.808357Z",
     "iopub.status.busy": "2025-10-19T23:51:05.808252Z",
     "iopub.status.idle": "2025-10-19T23:51:05.817455Z",
     "shell.execute_reply": "2025-10-19T23:51:05.817144Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RESET F-statistic</td>\n",
       "      <td>4.6682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RESET p-value</td>\n",
       "      <td>0.0120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Metric   Value\n",
       "0  RESET F-statistic  4.6682\n",
       "1      RESET p-value  0.0120"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload data if needed\n",
    "hprice1 = wool.data(\"hprice1\")\n",
    "\n",
    "# Estimate the original linear regression again\n",
    "reg = smf.ols(formula=\"price ~ lotsize + sqrft + bdrms\", data=hprice1)\n",
    "results = reg.fit()\n",
    "\n",
    "# Perform automated RESET test using statsmodels.stats.outliers_influence\n",
    "# Pass the results object and specify the maximum degree of the fitted values to include (degree=3 means ^2 and ^3)\n",
    "# --- RESET Test (Automated) ---\n",
    "reset_output = smo.reset_ramsey(res=results, degree=3)\n",
    "fstat_auto = reset_output.statistic\n",
    "fpval_auto = reset_output.pvalue\n",
    "\n",
    "# RESET Test Results (Automated)\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Metric\": [\"RESET F-statistic\", \"RESET p-value\"],\n",
    "        \"Value\": [f\"{fstat_auto:.4f}\", f\"{fpval_auto:.4f}\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Interpretation (Automated RESET): The automated test yields the same F-statistic (4.6682)\n",
    "# and p-value (0.0120), confirming the rejection of the null hypothesis and indicating\n",
    "# functional form misspecification in the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aec1f53",
   "metadata": {},
   "source": [
    "### Non-nested Tests (Davidson-MacKinnon)\n",
    "\n",
    "When we have two competing, **non-nested** models (meaning neither model is a special case of the other), we can use tests like the Davidson-MacKinnon test to see if one model provides significant explanatory power beyond the other.\n",
    "\n",
    "The test involves augmenting one model (Model 1) with the fitted values from the other model (Model 2). If the fitted values from Model 2 are significant when added to Model 1, it suggests Model 1 does not adequately encompass Model 2. The roles are then reversed.\n",
    "\n",
    "*   Possible outcomes: Neither model rejected, one rejected, both rejected.\n",
    "\n",
    "Here, we compare the linear housing price model (Model 1) with a log-log model (Model 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:51:05.818863Z",
     "iopub.status.busy": "2025-10-19T23:51:05.818757Z",
     "iopub.status.idle": "2025-10-19T23:51:05.832953Z",
     "shell.execute_reply": "2025-10-19T23:51:05.832717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>df_resid</th>\n",
       "      <th>ssr</th>\n",
       "      <th>df_diff</th>\n",
       "      <th>ss_diff</th>\n",
       "      <th>F</th>\n",
       "      <th>Pr(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84.0</td>\n",
       "      <td>300723.805123</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82.0</td>\n",
       "      <td>252340.364481</td>\n",
       "      <td>2.0</td>\n",
       "      <td>48383.440642</td>\n",
       "      <td>7.861291</td>\n",
       "      <td>0.000753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   df_resid            ssr  df_diff       ss_diff         F    Pr(>F)\n",
       "0      84.0  300723.805123      0.0           NaN       NaN       NaN\n",
       "1      82.0  252340.364481      2.0  48383.440642  7.861291  0.000753"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload data if needed\n",
    "hprice1 = wool.data(\"hprice1\")\n",
    "\n",
    "# Define the two competing, non-nested models:\n",
    "# Model 1: Linear levels model\n",
    "reg1 = smf.ols(formula=\"price ~ lotsize + sqrft + bdrms\", data=hprice1)\n",
    "results1 = reg1.fit()\n",
    "\n",
    "# Model 2: Log-log model (except for bdrms)\n",
    "reg2 = smf.ols(\n",
    "    formula=\"price ~ np.log(lotsize) +np.log(sqrft) + bdrms\",\n",
    "    data=hprice1,\n",
    ")\n",
    "results2 = reg2.fit()\n",
    "\n",
    "# --- Davidson-MacKinnon Test (Implementation via encompassing model F-test) ---\n",
    "# An alternative way to perform these tests is to create a comprehensive model\n",
    "# that includes *all* non-redundant regressors from both models.\n",
    "# Then, test the exclusion restrictions corresponding to each original model.\n",
    "\n",
    "# Comprehensive model including levels and logs (where applicable)\n",
    "reg3 = smf.ols(\n",
    "    formula=\"price ~ lotsize + sqrft + bdrms + np.log(lotsize) + np.log(sqrft)\",\n",
    "    data=hprice1,\n",
    ")\n",
    "results3 = reg3.fit()\n",
    "\n",
    "# Test Model 1 vs Comprehensive Model:\n",
    "# H0: Coefficients on np.log(lotsize) and np.log(sqrft) are zero (i.e., Model 1 is adequate)\n",
    "# This tests if Model 2's unique terms add significant explanatory power to Model 1.\n",
    "# --- Testing Model 1 (Levels) vs Comprehensive Model ---\n",
    "# anova_lm performs an F-test comparing the restricted model (results1) to the unrestricted (results3)\n",
    "anovaResults1 = sm.stats.anova_lm(results1, results3)\n",
    "# F-test (Model 1 vs Comprehensive)\n",
    "anovaResults1\n",
    "# Look at the p-value (Pr(>F)) in the second row.\n",
    "\n",
    "# Interpretation (Model 1 vs Comprehensive): The p-value is 0.000753.\n",
    "# We strongly reject the null hypothesis. This means the log terms (from Model 2)\n",
    "# add significant explanatory power to the linear model (Model 1).\n",
    "# Model 1 appears misspecified relative to the comprehensive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:51:05.834075Z",
     "iopub.status.busy": "2025-10-19T23:51:05.833996Z",
     "iopub.status.idle": "2025-10-19T23:51:05.838725Z",
     "shell.execute_reply": "2025-10-19T23:51:05.838425Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>df_resid</th>\n",
       "      <th>ssr</th>\n",
       "      <th>df_diff</th>\n",
       "      <th>ss_diff</th>\n",
       "      <th>F</th>\n",
       "      <th>Pr(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84.0</td>\n",
       "      <td>295735.273607</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82.0</td>\n",
       "      <td>252340.364481</td>\n",
       "      <td>2.0</td>\n",
       "      <td>43394.909126</td>\n",
       "      <td>7.05076</td>\n",
       "      <td>0.001494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   df_resid            ssr  df_diff       ss_diff        F    Pr(>F)\n",
       "0      84.0  295735.273607      0.0           NaN      NaN       NaN\n",
       "1      82.0  252340.364481      2.0  43394.909126  7.05076  0.001494"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Model 2 vs Comprehensive Model:\n",
    "# H0: Coefficients on lotsize and sqrft are zero (i.e., Model 2 is adequate)\n",
    "# This tests if Model 1's unique terms add significant explanatory power to Model 2.\n",
    "# --- Testing Model 2 (Logs) vs Comprehensive Model ---\n",
    "anovaResults2 = sm.stats.anova_lm(results2, results3)\n",
    "# F-test (Model 2 vs Comprehensive)\n",
    "anovaResults2\n",
    "# Look at the p-value (Pr(>F)) in the second row.\n",
    "\n",
    "# Interpretation (Model 2 vs Comprehensive): The p-value is 0.001494.\n",
    "# We also reject this null hypothesis at the 5% level. This means the level terms\n",
    "# (lotsize, sqrft from Model 1) add significant explanatory power to the log-log model (Model 2).\n",
    "# Model 2 also appears misspecified relative to the comprehensive model.\n",
    "\n",
    "# Overall Conclusion (Davidson-MacKinnon): Both the simple linear model and the log-log model\n",
    "# seem to be misspecified according to this test. Neither model encompasses the other fully.\n",
    "# This might suggest exploring a more complex functional form, perhaps including both levels and logs,\n",
    "# or other non-linear terms, although the comprehensive model itself might be hard to interpret.\n",
    "# Often, the log model is preferred based on goodness-of-fit or interpretability (elasticities),\n",
    "# even if the formal test rejects it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Measurement Error\n",
    "\n",
    "Measurement error occurs when the variables used in our regression analysis are measured with error, meaning the observed variable differs from the true, underlying variable of interest. This is common in applied econometrics (e.g., self-reported income, education, or health measures). The consequences depend critically on whether the measurement error is in the dependent or independent variable.\n",
    "\n",
    "**Classical Measurement Error Assumptions:**\n",
    "- Mean zero: $E(e) = 0$\n",
    "- Uncorrelated with true value: $\\text{Cov}(e, \\text{true value}) = 0$\n",
    "- Uncorrelated with other variables and error term: $\\text{Cov}(e, X) = 0$, $\\text{Cov}(e, u) = 0$\n",
    "\n",
    "### Measurement Error in the Dependent Variable ($y$)\n",
    "\n",
    "**Setup:** Suppose the true model is:\n",
    "$$y^* = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k + u$$\n",
    "\n",
    "but we observe $y = y^* + e_0$, where $e_0$ is classical measurement error in $y$.\n",
    "\n",
    "**Consequences:** \n",
    "- **Unbiasedness preserved:** OLS estimates of $\\beta_0, \\ldots, \\beta_k$ remain **unbiased** and **consistent** because $e_0$ simply becomes part of the composite error term: $y = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k + (u + e_0)$.\n",
    "- **Increased variance:** The error variance increases from $\\text{Var}(u)$ to $\\text{Var}(u + e_0) = \\text{Var}(u) + \\text{Var}(e_0)$ (assuming $\\text{Cov}(u, e_0) = 0$). This leads to **larger standard errors** and **less precise estimates** (wider confidence intervals, lower power).\n",
    "- **No bias, only loss of efficiency**\n",
    "\n",
    "### Measurement Error in an Independent Variable ($x$)\n",
    "\n",
    "**Setup:** Suppose the true model is:\n",
    "$$y = \\beta_0 + \\beta_1 x_1^* + \\cdots + \\beta_k x_k^* + u$$\n",
    "\n",
    "but we observe $x_j = x_j^* + e_j$ for some variable $j$, where $e_j$ is classical measurement error in $x_j$.\n",
    "\n",
    "**Consequences:**\n",
    "- **Bias and inconsistency:** OLS estimates are generally **biased** and **inconsistent** because measurement error in $x_j$ violates the zero conditional mean assumption (MLR.4). The observed $x_j$ is correlated with the composite error term.\n",
    "- **Attenuation bias:** The coefficient on the mismeasured variable $\\hat{\\beta}_j$ is typically biased **toward zero**. In a simple regression, the bias factor is:\n",
    "  $$E(\\hat{\\beta}_j) \\approx \\beta_j \\cdot \\frac{\\text{Var}(x_j^*)}{\\text{Var}(x_j^*) + \\text{Var}(e_j)} = \\beta_j \\cdot \\frac{\\text{Var}(x_j^*)}{\\text{Var}(x_j)} < \\beta_j \\text{ (if } \\beta_j > 0\\text{)}$$\n",
    "- **Spillover bias:** Coefficients on other variables ($\\beta_1, \\ldots, \\beta_{j-1}, \\beta_{j+1}, \\ldots, \\beta_k$) can also be biased if they are correlated with the mismeasured $x_j$.\n",
    "- **More serious problem:** Measurement error in regressors is more problematic than in the dependent variable because it causes both bias and inconsistency.\n",
    "\n",
    "We use simulations to illustrate these effects.\n",
    "\n",
    "### Simulation: Measurement Error in $y$\n",
    "\n",
    "We simulate data where the true model is $y^* = \\beta_0 + \\beta_1 x + u$, but we observe $y = y^* + e_0$. We compare the OLS estimate of $\\beta_1$ from regressing $y^*$ on $x$ (no ME) with the estimate from regressing $y$ on $x$ (ME in $y$). The true $\\beta_1 = 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:51:05.839959Z",
     "iopub.status.busy": "2025-10-19T23:51:05.839879Z",
     "iopub.status.idle": "2025-10-19T23:51:33.641086Z",
     "shell.execute_reply": "2025-10-19T23:51:33.640810Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Average beta_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Measurement Error</td>\n",
       "      <td>0.5002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Measurement Error in y</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model Average beta_1\n",
       "0    No Measurement Error         0.5002\n",
       "1  Measurement Error in y         0.5000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "np.random.seed(1234567)\n",
    "\n",
    "# Simulation parameters\n",
    "n = 1000  # Sample size\n",
    "r = 10000  # Number of simulation repetitions\n",
    "\n",
    "# True parameters\n",
    "beta0 = 1\n",
    "beta1 = 0.5  # True slope coefficient\n",
    "\n",
    "# Generate a fixed sample of the independent variable x\n",
    "x = stats.norm.rvs(4, 1, size=n)  # Mean=4, SD=1\n",
    "\n",
    "# Vectorized simulation: Generate all r replications at once\n",
    "# Generate true errors u for all replications: shape (r, n)\n",
    "u_all = stats.norm.rvs(0, 1, size=(r, n))\n",
    "\n",
    "# Calculate the true dependent variable y* for all replications\n",
    "ystar_all = beta0 + beta1 * x + u_all  # Broadcasting: (r, n)\n",
    "\n",
    "# Generate classical measurement error e0 for y for all replications\n",
    "e0_all = stats.norm.rvs(0, 1, size=(r, n))\n",
    "\n",
    "# Create the observed, mismeasured y for all replications\n",
    "y_all = ystar_all + e0_all  # shape (r, n)\n",
    "\n",
    "# Prepare design matrix for regression: add intercept\n",
    "X_design = np.column_stack([np.ones(n), x])  # shape (n, 2)\n",
    "\n",
    "# Compute OLS estimates for all replications using vectorized operations\n",
    "# OLS formula: beta_hat = (X'X)^(-1) X'y\n",
    "XtX_inv = np.linalg.inv(X_design.T @ X_design)  # (2, 2)\n",
    "\n",
    "# For no ME case: regress ystar on x\n",
    "# beta_hat = (X'X)^(-1) X' ystar for each replication\n",
    "# ystar_all.T is shape (n, r), so X'ystar_all.T gives (2, r)\n",
    "betas_star = XtX_inv @ (X_design.T @ ystar_all.T)  # shape (2, r)\n",
    "b1 = betas_star[1, :]  # Extract slope coefficients (second row)\n",
    "\n",
    "# For ME in y case: regress y on x\n",
    "betas_me = XtX_inv @ (X_design.T @ y_all.T)  # shape (2, r)\n",
    "b1_me = betas_me[1, :]  # Extract slope coefficients\n",
    "\n",
    "# Analyze the simulation results: Average estimated beta1 across repetitions\n",
    "b1_mean = np.mean(b1)\n",
    "b1_me_mean = np.mean(b1_me)\n",
    "# --- Simulation Results: Measurement Error in y ---\n",
    "# Measurement error effect on estimates\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": [\"No Measurement Error\", \"Measurement Error in y\"],\n",
    "        \"Average beta_1\": [f\"{b1_mean:.4f}\", f\"{b1_me_mean:.4f}\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Interpretation (Bias): Both average estimates are very close to the true value (0.5).\n",
    "# This confirms that classical measurement error in the dependent variable does not\n",
    "# cause bias in the OLS coefficient estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:51:33.642398Z",
     "iopub.status.busy": "2025-10-19T23:51:33.642297Z",
     "iopub.status.idle": "2025-10-19T23:51:33.646023Z",
     "shell.execute_reply": "2025-10-19T23:51:33.645717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Variance of beta_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Measurement Error</td>\n",
       "      <td>0.001034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Measurement Error in y</td>\n",
       "      <td>0.002044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model Variance of beta_1\n",
       "0    No Measurement Error           0.001034\n",
       "1  Measurement Error in y           0.002044"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze the simulation results: Variance of the estimated beta1 across repetitions\n",
    "b1_var = np.var(b1, ddof=1)  # Use ddof=1 for sample variance\n",
    "b1_me_var = np.var(b1_me, ddof=1)\n",
    "# Variance comparison\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": [\"No Measurement Error\", \"Measurement Error in y\"],\n",
    "        \"Variance of beta_1\": [f\"{b1_var:.6f}\", f\"{b1_me_var:.6f}\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Interpretation (Variance): The variance of the beta1 estimate is larger when there is\n",
    "# measurement error in y (0.002044) compared to when there is no measurement error (0.001034).\n",
    "# This confirms that ME in y reduces the precision of the OLS estimates (increases standard errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9beac7",
   "metadata": {},
   "source": [
    "### Simulation: Measurement Error in $x$\n",
    "\n",
    "Now, we simulate data where the true model is $y = \\beta_0 + \\beta_1 x^* + u$, but we observe $x = x^* + e_1$. We compare the OLS estimate of $\\beta_1$ from regressing $y$ on $x^*$ (no ME) with the estimate from regressing $y$ on $x$ (ME in $x$). The true $\\beta_1 = 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:51:33.647299Z",
     "iopub.status.busy": "2025-10-19T23:51:33.647216Z",
     "iopub.status.idle": "2025-10-19T23:52:01.201352Z",
     "shell.execute_reply": "2025-10-19T23:52:01.201092Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Average beta_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Measurement Error</td>\n",
       "      <td>0.5002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Measurement Error in x</td>\n",
       "      <td>0.2445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model Average beta_1\n",
       "0    No Measurement Error         0.5002\n",
       "1  Measurement Error in x         0.2445"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the random seed\n",
    "np.random.seed(1234567)\n",
    "\n",
    "# Simulation parameters (same as before)\n",
    "n = 1000\n",
    "r = 10000\n",
    "beta0 = 1\n",
    "beta1 = 0.5\n",
    "\n",
    "# Generate a fixed sample of the true independent variable x*\n",
    "xstar = stats.norm.rvs(4, 1, size=n)\n",
    "\n",
    "# Vectorized simulation: Generate all r replications at once\n",
    "# Generate true errors u for all replications: shape (r, n)\n",
    "u_all = stats.norm.rvs(0, 1, size=(r, n))\n",
    "\n",
    "# Calculate the dependent variable y (no ME in y here)\n",
    "y_all = beta0 + beta1 * xstar + u_all  # Broadcasting: (r, n)\n",
    "\n",
    "# Generate classical measurement error e1 for x for all replications\n",
    "e1_all = stats.norm.rvs(0, 1, size=(r, n))\n",
    "\n",
    "# Create the observed, mismeasured x for all replications\n",
    "x_all = xstar + e1_all  # shape (r, n)\n",
    "\n",
    "# Prepare design matrices for regression: add intercept\n",
    "X_star_design = np.column_stack([np.ones(n), xstar])  # True x*, shape (n, 2)\n",
    "\n",
    "# For no ME case: regress y on xstar\n",
    "# Compute (X'X)^(-1) for xstar\n",
    "XtX_star_inv = np.linalg.inv(X_star_design.T @ X_star_design)\n",
    "# beta_hat = (X'X)^(-1) X'y for all replications\n",
    "betas_star = XtX_star_inv @ (X_star_design.T @ y_all.T)  # shape (2, r)\n",
    "b1 = betas_star[1, :]  # Extract slope coefficients\n",
    "\n",
    "# For ME in x case: regress y on x (different x for each replication)\n",
    "# Need to compute OLS for each replication since X changes\n",
    "# More efficient: use vectorized least squares for each replication\n",
    "# For each replication i, we compute beta_hat_i = (X_i'X_i)^(-1) X_i'y_i\n",
    "\n",
    "# Stack all design matrices: shape (r, n, 2)\n",
    "X_me_all = np.stack([np.column_stack([np.ones(n), x_all[i, :]]) for i in range(r)])\n",
    "\n",
    "# Vectorized computation using einsum for matrix operations\n",
    "# Compute X'X for each replication: (r, 2, 2)\n",
    "XtX_me = np.einsum('rni,rnj->rij', X_me_all, X_me_all)\n",
    "# Compute (X'X)^(-1) for each replication\n",
    "XtX_me_inv = np.linalg.inv(XtX_me)  # shape (r, 2, 2)\n",
    "# Compute X'y for each replication: (r, 2)\n",
    "Xty_me = np.einsum('rni,rn->ri', X_me_all, y_all)\n",
    "# Compute beta_hat = (X'X)^(-1) X'y for each replication\n",
    "betas_me = np.einsum('rij,rj->ri', XtX_me_inv, Xty_me)  # shape (r, 2)\n",
    "b1_me = betas_me[:, 1]  # Extract slope coefficients\n",
    "\n",
    "# Analyze the simulation results: Average estimated beta1\n",
    "b1_mean = np.mean(b1)\n",
    "b1_me_mean = np.mean(b1_me)\n",
    "# --- Simulation Results: Measurement Error in x ---\n",
    "# Measurement error in x: effect on estimates\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": [\"No Measurement Error\", \"Measurement Error in x\"],\n",
    "        \"Average beta_1\": [f\"{b1_mean:.4f}\", f\"{b1_me_mean:.4f}\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Interpretation (Bias): The average estimate without ME is close to the true value (0.5).\n",
    "# However, the average estimate with ME in x (0.2445) is substantially smaller than 0.5.\n",
    "# This demonstrates the attenuation bias caused by classical measurement error in an\n",
    "# independent variable. The estimate is biased towards zero.\n",
    "# Theoretical bias factor: Var(x*)/(Var(x*) + Var(e1)). Here Var(x*)=1, Var(e1)=1.\n",
    "# Expected estimate = beta1 * (1 / (1+1)) = 0.5 * 0.5 = 0.25. The simulation matches this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:52:01.202621Z",
     "iopub.status.busy": "2025-10-19T23:52:01.202539Z",
     "iopub.status.idle": "2025-10-19T23:52:01.206410Z",
     "shell.execute_reply": "2025-10-19T23:52:01.206141Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Variance of beta_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Measurement Error</td>\n",
       "      <td>0.001034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Measurement Error in x</td>\n",
       "      <td>0.000544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model Variance of beta_1\n",
       "0    No Measurement Error           0.001034\n",
       "1  Measurement Error in x           0.000544"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze the simulation results: Variance of the estimated beta1\n",
    "b1_var = np.var(b1, ddof=1)\n",
    "b1_me_var = np.var(b1_me, ddof=1)\n",
    "# Variance comparison for measurement error in x\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": [\"No Measurement Error\", \"Measurement Error in x\"],\n",
    "        \"Variance of beta_1\": [f\"{b1_var:.6f}\", f\"{b1_me_var:.6f}\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Interpretation (Variance): Interestingly, the variance of the estimate with ME in x (0.000544)\n",
    "# is smaller than the variance without ME (0.001034). While the estimate is biased,\n",
    "# the presence of ME in x (which adds noise) can sometimes reduce the variance of the\n",
    "# *biased* estimator compared to the variance of the *unbiased* estimator using the true x*.\n",
    "# However, this smaller variance is around the wrong (biased) value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Missing Data and Nonrandom Samples\n",
    "\n",
    "Missing data is a common problem in empirical research. Values for certain variables might be missing for some observations. How missing data is handled can significantly impact the results.\n",
    "\n",
    "*   **NaN (Not a Number)** and **Inf (Infinity)**: These are special floating-point values used to represent undefined results (e.g., log(-1) -> NaN, 1/0 -> Inf) or missing numeric data. NumPy and pandas have functions to detect and handle them.\n",
    "*   **Listwise Deletion:** Most statistical software, including `statsmodels` by default, handles missing data by **listwise deletion**. This means if an observation is missing a value for *any* variable included in the regression (dependent or independent), the entire observation is dropped from the analysis.\n",
    "*   **Potential Bias:** Listwise deletion is acceptable if data are **Missing Completely At Random (MCAR)**. However, if the missingness is related to the values of other variables in the model (Missing At Random, MAR) or related to the missing value itself (Missing Not At Random, MNAR), listwise deletion can lead to **biased and inconsistent estimates** due to sample selection issues. More advanced techniques (like imputation) might be needed in such cases, but are beyond the scope here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:52:01.207713Z",
     "iopub.status.busy": "2025-10-19T23:52:01.207628Z",
     "iopub.status.idle": "2025-10-19T23:52:01.212629Z",
     "shell.execute_reply": "2025-10-19T23:52:01.212201Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rg/vmn_nq41613gkxt0_9spwzx80000gp/T/ipykernel_46714/1368694614.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  logx = np.log(x)  # log(-1)=NaN, log(0)=-Inf\n",
      "/var/folders/rg/vmn_nq41613gkxt0_9spwzx80000gp/T/ipykernel_46714/1368694614.py:3: RuntimeWarning: invalid value encountered in log\n",
      "  logx = np.log(x)  # log(-1)=NaN, log(0)=-Inf\n",
      "/var/folders/rg/vmn_nq41613gkxt0_9spwzx80000gp/T/ipykernel_46714/1368694614.py:4: RuntimeWarning: divide by zero encountered in divide\n",
      "  invx = 1 / x  # 1/0=Inf, 1/NaN=NaN, 1/Inf=0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>log(x)</th>\n",
       "      <th>1/x</th>\n",
       "      <th>Normal CDF</th>\n",
       "      <th>Is NaN?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.158655</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.841345</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x  log(x)  1/x  Normal CDF  Is NaN?\n",
       "0 -1.0     NaN -1.0    0.158655    False\n",
       "1  0.0    -inf  inf    0.500000    False\n",
       "2  1.0     0.0  1.0    0.841345    False\n",
       "3  NaN     NaN  NaN         NaN     True\n",
       "4  inf     inf  0.0    1.000000    False\n",
       "5 -inf     NaN -0.0    0.000000    False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstrate how NumPy handles NaN and Inf in calculations\n",
    "x = np.array([-1, 0, 1, np.nan, np.inf, -np.inf])\n",
    "logx = np.log(x)  # log(-1)=NaN, log(0)=-Inf\n",
    "invx = 1 / x  # 1/0=Inf, 1/NaN=NaN, 1/Inf=0\n",
    "ncdf = stats.norm.cdf(x)  # cdf handles Inf, -Inf, NaN appropriately\n",
    "isnanx = np.isnan(x)  # Detect NaN values\n",
    "\n",
    "# Display results in a pandas DataFrame\n",
    "results_np_handling = pd.DataFrame(\n",
    "    {\"x\": x, \"log(x)\": logx, \"1/x\": invx, \"Normal CDF\": ncdf, \"Is NaN?\": isnanx},\n",
    ")\n",
    "# --- NumPy Handling of NaN/Inf ---\n",
    "# Comparison of NaN Handling Methods\n",
    "results_np_handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af14c952",
   "metadata": {},
   "source": [
    "Now, let's examine missing data in a real dataset (`lawsch85`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:52:01.213894Z",
     "iopub.status.busy": "2025-10-19T23:52:01.213808Z",
     "iopub.status.idle": "2025-10-19T23:52:01.223750Z",
     "shell.execute_reply": "2025-10-19T23:52:01.223503Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>School_Index</th>\n",
       "      <th>LSAT_Score</th>\n",
       "      <th>Is_Missing</th>\n",
       "      <th>Data_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120</td>\n",
       "      <td>156.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121</td>\n",
       "      <td>159.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>122</td>\n",
       "      <td>157.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123</td>\n",
       "      <td>167.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>MISSING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>125</td>\n",
       "      <td>158.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>126</td>\n",
       "      <td>155.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>127</td>\n",
       "      <td>157.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>MISSING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>129</td>\n",
       "      <td>163.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Present</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   School_Index  LSAT_Score  Is_Missing Data_Status\n",
       "0           120       156.0       False     Present\n",
       "1           121       159.0       False     Present\n",
       "2           122       157.0       False     Present\n",
       "3           123       167.0       False     Present\n",
       "4           124         NaN        True     MISSING\n",
       "5           125       158.0       False     Present\n",
       "6           126       155.0       False     Present\n",
       "7           127       157.0       False     Present\n",
       "8           128         NaN        True     MISSING\n",
       "9           129       163.0       False     Present"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing Data Analysis: Law School Dataset\n",
    "# Demonstrates detection and handling of missing values\n",
    "\n",
    "# Load law school dataset\n",
    "lawsch85 = wool.data(\"lawsch85\")\n",
    "# Dataset dimensions\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Dimension\": [\"Schools (rows)\", \"Variables (columns)\"],\n",
    "        \"Count\": [lawsch85.shape[0], lawsch85.shape[1]],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Extract LSAT scores to analyze missingness pattern\n",
    "lsat_scores = lawsch85[\"LSAT\"]  # Law School Admission Test scores\n",
    "\n",
    "# Create missing data indicator (True = missing, False = present)\n",
    "lsat_missing = lsat_scores.isna()  # pandas method for NaN detection\n",
    "\n",
    "# Examine specific observations to see missing pattern\n",
    "observation_range = slice(119, 129)  # Schools 120-129\n",
    "missing_preview = pd.DataFrame(\n",
    "    {\n",
    "        \"School_Index\": range(120, 130),\n",
    "        \"LSAT_Score\": lsat_scores.iloc[observation_range].values,\n",
    "        \"Is_Missing\": lsat_missing.iloc[observation_range].values,\n",
    "        \"Data_Status\": [\n",
    "            \"MISSING\" if m else \"Present\" for m in lsat_missing.iloc[observation_range]\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "\n",
    "# MISSING DATA DETECTION EXAMPLE\n",
    "# Preview of schools 120-129:\n",
    "missing_preview\n",
    "# Note: NaN indicates missing LSAT scores for some schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:52:01.224968Z",
     "iopub.status.busy": "2025-10-19T23:52:01.224885Z",
     "iopub.status.idle": "2025-10-19T23:52:01.232435Z",
     "shell.execute_reply": "2025-10-19T23:52:01.232205Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSAT</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0  count\n",
       "LSAT        \n",
       "False    150\n",
       "True       6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate frequencies of missing vs. non-missing LSAT scores\n",
    "freq_missLSAT = pd.crosstab(lsat_missing, columns=\"count\")\n",
    "# Frequency of Missing LSAT\n",
    "freq_missLSAT\n",
    "# Shows 7 schools have missing LSAT scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:52:01.233706Z",
     "iopub.status.busy": "2025-10-19T23:52:01.233617Z",
     "iopub.status.idle": "2025-10-19T23:52:01.236963Z",
     "shell.execute_reply": "2025-10-19T23:52:01.236775Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salary</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cost</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSAT</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPA</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>libvol</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faculty</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clsize</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>north</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>south</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>east</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>west</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lsalary</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>studfac</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top10</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r11_25</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r26_40</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r41_60</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llibvol</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lcost</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Missing Count\n",
       "rank                 0\n",
       "salary               8\n",
       "cost                 6\n",
       "LSAT                 6\n",
       "GPA                  7\n",
       "libvol               1\n",
       "faculty              4\n",
       "age                 45\n",
       "clsize               3\n",
       "north                0\n",
       "south                0\n",
       "east                 0\n",
       "west                 0\n",
       "lsalary              8\n",
       "studfac              6\n",
       "top10                0\n",
       "r11_25               0\n",
       "r26_40               0\n",
       "r41_60               0\n",
       "llibvol              1\n",
       "lcost                6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missings across all variables in the DataFrame\n",
    "miss_all = lawsch85.isna()  # Creates a boolean DataFrame of the same shape\n",
    "colsums = miss_all.sum(\n",
    "    axis=0,\n",
    ")  # Sum boolean columns (True=1, False=0) to count missings per variable\n",
    "# --- Missing Counts per Variable ---\n",
    "# Missing values per column\n",
    "colsums.to_frame(\"Missing Count\")\n",
    "# Shows several variables have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:52:01.238179Z",
     "iopub.status.busy": "2025-10-19T23:52:01.238088Z",
     "iopub.status.idle": "2025-10-19T23:52:01.243184Z",
     "shell.execute_reply": "2025-10-19T23:52:01.242937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0  count\n",
       "row_0       \n",
       "False     66\n",
       "True      90"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the number of complete cases (no missing values in any column for that row)\n",
    "# Sum missings across rows (axis=1). If sum is 0, the case is complete.\n",
    "complete_cases = miss_all.sum(axis=1) == 0\n",
    "freq_complete_cases = pd.crosstab(complete_cases, columns=\"count\")\n",
    "# --- Frequency of Complete Cases ---\n",
    "# Complete cases distribution\n",
    "freq_complete_cases\n",
    "# Shows 131 out of 156 observations are complete cases (have no missing values).\n",
    "# The remaining 25 observations have at least one missing value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a7fa8e",
   "metadata": {},
   "source": [
    "How do standard functions handle missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:52:01.244402Z",
     "iopub.status.busy": "2025-10-19T23:52:01.244321Z",
     "iopub.status.idle": "2025-10-19T23:52:01.249487Z",
     "shell.execute_reply": "2025-10-19T23:52:01.249256Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>np.mean(LSAT)</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>np.nanmean(LSAT)</td>\n",
       "      <td>158.2933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Method    Result\n",
       "0     np.mean(LSAT)       nan\n",
       "1  np.nanmean(LSAT)  158.2933"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data again if needed\n",
    "lawsch85 = wool.data(\"lawsch85\")\n",
    "\n",
    "# --- Missing value handling in NumPy ---\n",
    "x_np = np.array(lawsch85[\"LSAT\"])  # Convert pandas Series to NumPy array\n",
    "# np.mean() calculates mean including NaN, resulting in NaN\n",
    "x_np_bar1 = np.mean(x_np)\n",
    "# np.nanmean() calculates mean ignoring NaN values\n",
    "x_np_bar2 = np.nanmean(x_np)\n",
    "# --- NumPy Mean Calculation with NaNs ---\n",
    "# NumPy mean comparison\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Method\": [\"np.mean(LSAT)\", \"np.nanmean(LSAT)\"],\n",
    "        \"Result\": [f\"{x_np_bar1:.4f}\", f\"{x_np_bar2:.4f}\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:52:01.250670Z",
     "iopub.status.busy": "2025-10-19T23:52:01.250595Z",
     "iopub.status.idle": "2025-10-19T23:52:01.254170Z",
     "shell.execute_reply": "2025-10-19T23:52:01.253895Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>LSAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pandas .mean()</td>\n",
       "      <td>158.2933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>np.nanmean()</td>\n",
       "      <td>158.2933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Method      LSAT\n",
       "0  pandas .mean()  158.2933\n",
       "1    np.nanmean()  158.2933"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Missing value handling in pandas ---\n",
    "x_pd = lawsch85[\"LSAT\"]  # Keep as pandas Series\n",
    "# By default, pandas methods often skip NaNs\n",
    "x_pd_bar1 = x_pd.mean()  # Equivalent to np.nanmean()\n",
    "# We can explicitly use np.nanmean on pandas Series too\n",
    "x_pd_bar2 = np.nanmean(x_pd)\n",
    "# --- pandas Mean Calculation with NaNs ---\n",
    "# Pandas mean comparison\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Method\": [\"pandas .mean()\", \"np.nanmean()\"],\n",
    "        \"LSAT\": [f\"{x_pd_bar1:.4f}\", f\"{x_pd_bar2:.4f}\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e915457",
   "metadata": {},
   "source": [
    "How does `statsmodels` handle missing data during regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:52:01.255353Z",
     "iopub.status.busy": "2025-10-19T23:52:01.255281Z",
     "iopub.status.idle": "2025-10-19T23:52:01.258080Z",
     "shell.execute_reply": "2025-10-19T23:52:01.257853Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dimension</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Original shape</td>\n",
       "      <td>(156, 21) (rows, columns)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dimension                      Value\n",
       "0  Original shape  (156, 21) (rows, columns)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the dimensions of the full dataset\n",
    "# Original dataset shape\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Dimension\": [\"Original shape\"],\n",
    "        \"Value\": [f\"{lawsch85.shape} (rows, columns)\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:52:01.259692Z",
     "iopub.status.busy": "2025-10-19T23:52:01.259613Z",
     "iopub.status.idle": "2025-10-19T23:52:01.265708Z",
     "shell.execute_reply": "2025-10-19T23:52:01.265484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Observations used in regression</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Metric  Count\n",
       "0  Observations used in regression     95"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Regression with statsmodels and Missing Data ---\n",
    "# Estimate a model for log(salary) using LSAT, cost, and age.\n",
    "# Some of these variables have missing values.\n",
    "reg = smf.ols(formula=\"np.log(salary) ~ LSAT + cost + age\", data=lawsch85)\n",
    "results = reg.fit()\n",
    "\n",
    "# Check the number of observations used in the regression\n",
    "# --- Statsmodels Regression with Missing Data ---\n",
    "# Regression observations\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Metric\": [\"Observations used in regression\"],\n",
    "        \"Count\": [int(results.nobs)],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Interpretation: The original dataset had 156 observations. The regression only used 131.\n",
    "# This confirms that statsmodels performed listwise deletion, dropping the 25 observations\n",
    "# that had missing values in salary, LSAT, cost, or age. This is the default behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 Outlying Observations\n",
    "\n",
    "**Outliers** are observations that are far away from the bulk of the data. They can arise from data entry errors or represent genuinely unusual cases. Outliers can have a disproportionately large influence on OLS estimates, potentially distorting the results (**influential observations**).\n",
    "\n",
    "**Studentized residuals** (or externally studentized residuals) are a useful diagnostic tool. They are calculated for each observation by fitting the model without that observation and then standardizing the difference between the actual and predicted value using the estimated standard error from the model excluding that observation.\n",
    "*   Observations with large studentized residuals (e.g., absolute value > 2 or 3) are potential outliers that warrant investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:52:01.266888Z",
     "iopub.status.busy": "2025-10-19T23:52:01.266807Z",
     "iopub.status.idle": "2025-10-19T23:52:01.278838Z",
     "shell.execute_reply": "2025-10-19T23:52:01.278599Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maximum studentized residual</td>\n",
       "      <td>4.5550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Minimum studentized residual</td>\n",
       "      <td>-1.8180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Metric    Value\n",
       "0  Maximum studentized residual   4.5550\n",
       "1  Minimum studentized residual  -1.8180"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load R&D intensity data\n",
    "rdchem = wool.data(\"rdchem\")\n",
    "\n",
    "# Estimate the OLS model: R&D intensity vs sales and profit margin\n",
    "reg = smf.ols(formula=\"rdintens ~ sales + profmarg\", data=rdchem)\n",
    "results = reg.fit()\n",
    "\n",
    "# Calculate studentized residuals using statsmodels influence methods\n",
    "infl = results.get_influence()\n",
    "studres = infl.resid_studentized_external  # Externally studentized residuals\n",
    "\n",
    "# Find the maximum and minimum studentized residuals\n",
    "studres_max = np.max(studres)\n",
    "studres_min = np.min(studres)\n",
    "# --- Outlier Detection using Studentized Residuals ---\n",
    "# Studentized residuals summary\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Metric\": [\"Maximum studentized residual\", \"Minimum studentized residual\"],\n",
    "        \"Value\": [f\"{studres_max:.4f}\", f\"{studres_min:.4f}\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Interpretation: The maximum value (4.5550) and minimum value (-1.8180) are both relatively\n",
    "# large in absolute terms, especially the maximum (roughly 4.5 standard deviations from zero).\n",
    "# This suggests these observations might be outliers and potentially influential. Further investigation\n",
    "# (e.g., examining the data for these specific firms) might be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2f6660",
   "metadata": {},
   "source": [
    "Visualizing the distribution of studentized residuals can also be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:52:01.280251Z",
     "iopub.status.busy": "2025-10-19T23:52:01.280159Z",
     "iopub.status.idle": "2025-10-19T23:52:01.303619Z",
     "shell.execute_reply": "2025-10-19T23:52:01.303353Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rg/vmn_nq41613gkxt0_9spwzx80000gp/T/ipykernel_46714/489714166.py:30: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Plot a histogram of the studentized residuals with an overlaid kernel density estimate\n",
    "\n",
    "# Fit kernel density estimator\n",
    "kde = sm.nonparametric.KDEUnivariate(studres)\n",
    "kde.fit()  # Estimate the density\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(\n",
    "    studres,\n",
    "    bins=\"auto\",\n",
    "    color=\"grey\",\n",
    "    density=True,\n",
    "    alpha=0.7,\n",
    "    label=\"Histogram\",\n",
    ")  # Use automatic binning\n",
    "plt.plot(\n",
    "    kde.support,\n",
    "    kde.density,\n",
    "    color=\"black\",\n",
    "    linewidth=2,\n",
    "    label=\"Kernel Density Estimate\",\n",
    ")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Studentized Residuals\")\n",
    "plt.title(\"Distribution of Studentized Residuals\")\n",
    "plt.axvline(0, color=\"red\", linestyle=\"--\", linewidth=1, label=\"Zero\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Interpretation: The histogram shows most residuals cluster around zero, but the density plot\n",
    "# highlights the presence of observations in the tails (around +3 and -3), consistent\n",
    "# with the min/max values found earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 Least Absolute Deviations (LAD) Estimation\n",
    "\n",
    "OLS minimizes the sum of *squared* residuals, which makes it sensitive to large outliers (since squaring magnifies large deviations). **Least Absolute Deviations (LAD)** estimation offers a robust alternative. LAD minimizes the sum of the *absolute values* of the residuals.\n",
    "$$ \\min_{\\beta_0, ..., \\beta_k} \\sum_{i=1}^n |y_i - \\beta_0 - \\beta_1 x_{i1} - ... - \\beta_k x_{ik}| $$\n",
    "*   LAD estimates are less sensitive to large outliers in the *dependent variable* $y$.\n",
    "*   LAD estimates the effect of $x$ on the *conditional median* of $y$, whereas OLS estimates the effect on the *conditional mean*. These can differ if the error distribution is skewed.\n",
    "*   LAD is a special case of **quantile regression** (estimating the median, i.e., the 0.5 quantile).\n",
    "\n",
    "We compare OLS and LAD estimates for the R&D intensity model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:52:01.305113Z",
     "iopub.status.busy": "2025-10-19T23:52:01.305016Z",
     "iopub.status.idle": "2025-10-19T23:52:01.313180Z",
     "shell.execute_reply": "2025-10-19T23:52:01.312907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b</th>\n",
       "      <th>se</th>\n",
       "      <th>t</th>\n",
       "      <th>pval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Intercept</th>\n",
       "      <td>2.6253</td>\n",
       "      <td>0.5855</td>\n",
       "      <td>4.4835</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I(sales / 1000)</th>\n",
       "      <td>0.0534</td>\n",
       "      <td>0.0441</td>\n",
       "      <td>1.2111</td>\n",
       "      <td>0.2356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>profmarg</th>\n",
       "      <td>0.0446</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.9661</td>\n",
       "      <td>0.3420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      b      se       t    pval\n",
       "Intercept        2.6253  0.5855  4.4835  0.0001\n",
       "I(sales / 1000)  0.0534  0.0441  1.2111  0.2356\n",
       "profmarg         0.0446  0.0462  0.9661  0.3420"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data if needed\n",
    "rdchem = wool.data(\"rdchem\")\n",
    "\n",
    "# --- OLS Regression ---\n",
    "# Rescale sales for easier coefficient interpretation (sales in $billions)\n",
    "reg_ols = smf.ols(formula=\"rdintens ~ I(sales/1000) + profmarg\", data=rdchem)\n",
    "results_ols = reg_ols.fit()\n",
    "\n",
    "# --- OLS Estimation Results ---\n",
    "table_ols = pd.DataFrame(\n",
    "    {\n",
    "        \"b\": round(results_ols.params, 4),\n",
    "        \"se\": round(results_ols.bse, 4),\n",
    "        \"t\": round(results_ols.tvalues, 4),\n",
    "        \"pval\": round(results_ols.pvalues, 4),\n",
    "    },\n",
    ")\n",
    "# OLS Estimates\n",
    "table_ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:52:01.314462Z",
     "iopub.status.busy": "2025-10-19T23:52:01.314379Z",
     "iopub.status.idle": "2025-10-19T23:52:01.336385Z",
     "shell.execute_reply": "2025-10-19T23:52:01.336145Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b</th>\n",
       "      <th>se</th>\n",
       "      <th>t</th>\n",
       "      <th>pval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Intercept</th>\n",
       "      <td>1.6231</td>\n",
       "      <td>0.7012</td>\n",
       "      <td>2.3148</td>\n",
       "      <td>0.0279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I(sales / 1000)</th>\n",
       "      <td>0.0186</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.3529</td>\n",
       "      <td>0.7267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>profmarg</th>\n",
       "      <td>0.1179</td>\n",
       "      <td>0.0553</td>\n",
       "      <td>2.1320</td>\n",
       "      <td>0.0416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      b      se       t    pval\n",
       "Intercept        1.6231  0.7012  2.3148  0.0279\n",
       "I(sales / 1000)  0.0186  0.0528  0.3529  0.7267\n",
       "profmarg         0.1179  0.0553  2.1320  0.0416"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- LAD Regression (Quantile Regression at the Median) ---\n",
    "# Use smf.quantreg and specify the quantile q=0.5 for LAD.\n",
    "reg_lad = smf.quantreg(formula=\"rdintens ~ I(sales/1000) + profmarg\", data=rdchem)\n",
    "results_lad = reg_lad.fit(q=0.5)  # Fit for the median\n",
    "\n",
    "# Display LAD results (statsmodels calculates SEs using appropriate methods for quantile regression)\n",
    "# --- LAD (Median Regression) Estimation Results ---\n",
    "table_lad = pd.DataFrame(\n",
    "    {\n",
    "        \"b\": round(results_lad.params, 4),  # LAD Coefficients\n",
    "        \"se\": round(results_lad.bse, 4),  # LAD Standard Errors\n",
    "        \"t\": round(results_lad.tvalues, 4),  # LAD t-statistics\n",
    "        \"pval\": round(results_lad.pvalues, 4),  # LAD p-values\n",
    "    },\n",
    ")\n",
    "# LAD Estimates\n",
    "table_lad\n",
    "\n",
    "# Interpretation (OLS vs LAD):\n",
    "# - The coefficient on sales/1000 is 0.0534 (OLS) vs 0.0186 (LAD).\n",
    "# - The coefficient on profit margin is 0.0446 (OLS) vs 0.1179 (LAD).\n",
    "# - The intercept is also different.\n",
    "# The differences suggest that the relationship might differ between the conditional mean (OLS)\n",
    "# and the conditional median (LAD), possibly due to outliers or skewness in the conditional\n",
    "# distribution of rdintens. The profit margin effect seems quite different across methods (LAD shows\n",
    "# a larger coefficient and higher significance), while the sales effect is much smaller and insignificant\n",
    "# in LAD. Since we identified potential outliers earlier, the LAD estimates might be considered more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ac6ba",
   "metadata": {},
   "source": [
    "## 9.6 Using Proxy Variables for Unobserved Explanatory Variables\n",
    "\n",
    "A common problem in empirical work is **omitted variable bias** (OVB): we want to include a variable in our regression that theoretically belongs there, but we cannot observe or measure it directly. A **proxy variable** is an observable variable that is related to the unobserved variable and can help reduce omitted variable bias.\n",
    "\n",
    "### 9.6.1 The Proxy Variable Solution\n",
    "\n",
    "**Setup:**\n",
    "\n",
    "True model includes an unobserved variable $a^*$:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k + \\beta_{k+1} a^* + u\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $a^*$ is unobserved (e.g., ability, quality, managerial talent)\n",
    "- $u$ is the error term, uncorrelated with $(x_1, \\ldots, x_k, a^*)$\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "If we simply omit $a^*$ and regress $y$ on $(x_1, \\ldots, x_k)$, the OLS estimators will be biased if any $x_j$ is correlated with $a^*$.\n",
    "\n",
    "**Proxy variable solution:**\n",
    "\n",
    "We observe a **proxy** $a$ that is related to $a^*$:\n",
    "\n",
    "$$\n",
    "a^* = \\delta_0 + \\delta_1 a + v\n",
    "$$\n",
    "\n",
    "where $v$ is uncorrelated with $(a, x_1, \\ldots, x_k, u)$.\n",
    "\n",
    "**Key assumption:** The proxy $a$ captures all the variation in $a^*$ that is correlated with the $x$ variables and $y$.\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "Instead of the true model, we estimate:\n",
    "\n",
    "$$\n",
    "y = \\alpha_0 + \\alpha_1 x_1 + \\cdots + \\alpha_k x_k + \\alpha_{k+1} a + \\text{error}\n",
    "$$\n",
    "\n",
    "**Result:**\n",
    "\n",
    "If the proxy variable assumptions hold, the OLS estimators of $\\alpha_1, \\ldots, \\alpha_k$ from this regression are **consistent** for the true parameters $\\beta_1, \\ldots, \\beta_k$.\n",
    "\n",
    "### 9.6.2 Properties of a Good Proxy Variable\n",
    "\n",
    "A good proxy variable should:\n",
    "\n",
    "1. **Be correlated with the unobserved variable** ($\\delta_1 \\neq 0$)\n",
    "   - If $a$ is uncorrelated with $a^*$, it provides no information\n",
    "   \n",
    "2. **Conditional on the proxy, the unobserved part is uncorrelated with explanatory variables**\n",
    "   - Formally: $\\text{Cov}(v, x_j) = 0$ for all $j$\n",
    "   - This means $a$ \"soaks up\" all the problematic correlation between $a^*$ and $x$\n",
    "   \n",
    "3. **Be measured accurately** (no measurement error in $a$)\n",
    "\n",
    "**Examples of proxy variables:**\n",
    "\n",
    "- **Ability** (unobserved): Use IQ score, test scores, or educational attainment as proxies\n",
    "- **Firm quality** (unobserved): Use past profitability, market share, or credit rating\n",
    "- **Health status** (partially observed): Use self-reported health, BMI, or smoking status\n",
    "- **Neighborhood quality**: Use median house prices, crime rates, or school test scores\n",
    "\n",
    "### 9.6.3 Example: Returns to Education with Ability Proxy\n",
    "\n",
    "Consider estimating the return to education on wages:\n",
    "\n",
    "$$\n",
    "\\log(\\text{wage}) = \\beta_0 + \\beta_1 \\text{educ} + \\beta_2 \\text{exper} + \\beta_3 \\text{ability} + u\n",
    "$$\n",
    "\n",
    "**Problem:** Ability is unobserved, and likely correlated with education (more able people get more education).\n",
    "\n",
    "**Proxy solution:** Use IQ score as a proxy for ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c468c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated example: Returns to education with ability proxy\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "# Generate unobserved ability\n",
    "ability_star = np.random.normal(100, 15, n)\n",
    "\n",
    "# Generate IQ as proxy for ability (with some noise)\n",
    "# IQ = ability* + noise\n",
    "iq = ability_star + np.random.normal(0, 10, n)\n",
    "\n",
    "# Generate education (correlated with ability)\n",
    "# More able people get more education\n",
    "educ = 12 + 0.05 * ability_star + np.random.normal(0, 2, n)\n",
    "educ = np.clip(educ, 8, 20)  # Cap between 8 and 20 years\n",
    "\n",
    "# Generate experience (independent of ability for simplicity)\n",
    "exper = np.random.uniform(0, 40, n)\n",
    "\n",
    "# Generate log wages (depends on educ, exper, and ability)\n",
    "log_wage = (\n",
    "    1.5  # Intercept\n",
    "    + 0.08 * educ  # True return to education\n",
    "    + 0.02 * exper  # Experience effect\n",
    "    + 0.005 * ability_star  # Ability effect\n",
    "    + np.random.normal(0, 0.3, n)  # Error term\n",
    ")\n",
    "\n",
    "# Create DataFrame\n",
    "wage_data = pd.DataFrame(\n",
    "    {\n",
    "        \"log_wage\": log_wage,\n",
    "        \"educ\": educ,\n",
    "        \"exper\": exper,\n",
    "        \"ability_star\": ability_star,  # Unobserved in practice\n",
    "        \"iq\": iq,  # Observed proxy\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"Education-Ability correlation:\", np.corrcoef(educ, ability_star)[0, 1].round(3))\n",
    "print(\"IQ-Ability correlation:\", np.corrcoef(iq, ability_star)[0, 1].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0401315",
   "metadata": {},
   "source": [
    "**Model 1: Naive regression (omitting ability)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c1302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model without ability (omitted variable bias)\n",
    "model_naive = smf.ols(formula=\"log_wage ~ educ + exper\", data=wage_data)\n",
    "results_naive = model_naive.fit()\n",
    "\n",
    "print(\"\\nModel 1: Naive (Omitting Ability)\")\n",
    "print(f\"Education coefficient: {results_naive.params['educ']:.4f}\")\n",
    "print(f\"  (True value: 0.0800)\")\n",
    "print(f\"  Bias: {results_naive.params['educ'] - 0.08:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31097013",
   "metadata": {},
   "source": [
    "**Model 2: Using IQ as proxy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9599da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with IQ as proxy for ability\n",
    "model_proxy = smf.ols(formula=\"log_wage ~ educ + exper + iq\", data=wage_data)\n",
    "results_proxy = model_proxy.fit()\n",
    "\n",
    "print(\"\\nModel 2: With IQ Proxy\")\n",
    "print(f\"Education coefficient: {results_proxy.params['educ']:.4f}\")\n",
    "print(f\"  (True value: 0.0800)\")\n",
    "print(f\"  Bias: {results_proxy.params['educ'] - 0.08:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe46394",
   "metadata": {},
   "source": [
    "**Model 3: Oracle (using true ability - infeasible in practice)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192eda2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with true ability (infeasible in reality)\n",
    "model_oracle = smf.ols(\n",
    "    formula=\"log_wage ~ educ + exper + ability_star\",\n",
    "    data=wage_data,\n",
    ")\n",
    "results_oracle = model_oracle.fit()\n",
    "\n",
    "print(\"\\nModel 3: Oracle (True Ability - Infeasible)\")\n",
    "print(f\"Education coefficient: {results_oracle.params['educ']:.4f}\")\n",
    "print(f\"  (True value: 0.0800)\")\n",
    "\n",
    "# Comparison table\n",
    "comparison_proxy = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": [\"Naive (OVB)\", \"With IQ Proxy\", \"Oracle (True Ability)\"],\n",
    "        \"Educ Coef\": [\n",
    "            results_naive.params[\"educ\"],\n",
    "            results_proxy.params[\"educ\"],\n",
    "            results_oracle.params[\"educ\"],\n",
    "        ],\n",
    "        \"Educ SE\": [\n",
    "            results_naive.bse[\"educ\"],\n",
    "            results_proxy.bse[\"educ\"],\n",
    "            results_oracle.bse[\"educ\"],\n",
    "        ],\n",
    "        \"Bias\": [\n",
    "            results_naive.params[\"educ\"] - 0.08,\n",
    "            results_proxy.params[\"educ\"] - 0.08,\n",
    "            results_oracle.params[\"educ\"] - 0.08,\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "print(\"\\nComparison of Education Coefficient Estimates:\")\n",
    "comparison_proxy.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86939d5d",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "\n",
    "- The **naive model** overestimates the return to education due to omitted ability bias (able people earn more AND get more education)\n",
    "- Using **IQ as a proxy** substantially reduces the bias compared to the naive model\n",
    "- The proxy model gets closer to the **oracle** estimate (using true ability), though not perfectly\n",
    "- The quality of the proxy determines how much bias reduction we achieve\n",
    "\n",
    "### 9.6.4 Limitations of Proxy Variables\n",
    "\n",
    "1. **Finding a good proxy is difficult**\n",
    "   - Must satisfy strong assumptions (captures all relevant variation)\n",
    "   - Most proxies are imperfect\n",
    "   \n",
    "2. **Proxy variables with measurement error**\n",
    "   - If the proxy itself has measurement error, bias reduction is limited\n",
    "   \n",
    "3. **Multiple unobservables**\n",
    "   - If multiple unobserved variables matter, need multiple proxies\n",
    "   - Joint proxy assumptions become harder to satisfy\n",
    "   \n",
    "4. **Alternative solutions may be better**\n",
    "   - **Instrumental variables** (Chapter 15) can handle correlation without needing proxies\n",
    "   - **Fixed effects** (Chapter 14) for panel data can eliminate time-invariant unobservables\n",
    "   - **Randomized experiments** eliminate omitted variable bias entirely\n",
    "\n",
    "**When to use proxy variables:**\n",
    "\n",
    "- When a theoretically important variable is unobserved\n",
    "- When you have a credible proxy (e.g., test scores for ability)\n",
    "- As a robustness check alongside other methods\n",
    "- When IV or fixed effects are not available\n",
    "\n",
    "## 9.7 Models with Random Slopes\n",
    "\n",
    "In standard regression, we assume coefficients are **constant** across all observations:\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik} + u_i\n",
    "$$\n",
    "\n",
    "But in many applications, the **effect** of $x$ on $y$ may **vary across individuals, firms, or time periods**. This leads to **random coefficient models** (also called **random slope models** or **varying coefficient models**).\n",
    "\n",
    "### 9.7.1 Random Coefficient Specification\n",
    "\n",
    "A simple random coefficient model:\n",
    "\n",
    "$$\n",
    "y_i = \\beta_{0i} + \\beta_{1i} x_i + u_i\n",
    "$$\n",
    "\n",
    "where $\\beta_{0i}$ and $\\beta_{1i}$ vary across observations.\n",
    "\n",
    "**Decomposition:**\n",
    "\n",
    "We can decompose the random coefficients into:\n",
    "\n",
    "$$\n",
    "\\beta_{0i} = \\beta_0 + a_{0i}, \\quad \\beta_{1i} = \\beta_1 + a_{1i}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\beta_0, \\beta_1$ are the **average** coefficients\n",
    "- $a_{0i}, a_{1i}$ are individual-specific deviations from the average\n",
    "\n",
    "**Substituting:**\n",
    "\n",
    "$$\n",
    "y_i = (\\beta_0 + a_{0i}) + (\\beta_1 + a_{1i}) x_i + u_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_i + \\underbrace{(a_{0i} + a_{1i} x_i + u_i)}_{\\text{composite error } v_i}\n",
    "$$\n",
    "\n",
    "### 9.7.2 Implications for OLS\n",
    "\n",
    "**Key question:** Can we estimate $\\beta_0$ and $\\beta_1$ (the average effects) using OLS?\n",
    "\n",
    "**Assumptions needed:**\n",
    "\n",
    "1. **Independence:** $E(a_{0i} | x_i) = 0$ and $E(a_{1i} | x_i) = 0$\n",
    "   - The individual deviations are uncorrelated with $x$\n",
    "   - This is the **random coefficients assumption**\n",
    "   \n",
    "2. **Zero conditional mean:** $E(u_i | x_i) = 0$ (standard assumption)\n",
    "\n",
    "**Under these assumptions:**\n",
    "\n",
    "$$\n",
    "E(y_i | x_i) = \\beta_0 + \\beta_1 x_i\n",
    "$$\n",
    "\n",
    "So OLS is **unbiased** and **consistent** for $\\beta_0$ and $\\beta_1$ (the average coefficients).\n",
    "\n",
    "**However:**\n",
    "\n",
    "The composite error $v_i = a_{0i} + a_{1i} x_i + u_i$ is generally **heteroskedastic**:\n",
    "\n",
    "$$\n",
    "\\text{Var}(v_i | x_i) = \\text{Var}(a_{0i}) + x_i^2 \\text{Var}(a_{1i}) + 2x_i \\text{Cov}(a_{0i}, a_{1i}) + \\sigma_u^2\n",
    "$$\n",
    "\n",
    "This variance **depends on** $x_i$ (specifically, quadratically through $x_i^2$).\n",
    "\n",
    "**Consequences:**\n",
    "\n",
    "1. **OLS is still consistent** for average effects $\\beta_0, \\beta_1$\n",
    "2. **Standard errors are incorrect** - must use **heteroskedasticity-robust SEs** (Chapter 8)\n",
    "3. **OLS is not efficient** - GLS or WLS could do better\n",
    "4. **Individual predictions** $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$ predict the **conditional mean**, not individual $y_i$\n",
    "\n",
    "### 9.7.3 Example: Returns to Education Varying by Ability\n",
    "\n",
    "Suppose the return to education varies by unobserved ability:\n",
    "\n",
    "$$\n",
    "\\log(\\text{wage}_i) = \\beta_0 + (\\beta_1 + a_i) \\cdot \\text{educ}_i + u_i\n",
    "$$\n",
    "\n",
    "where $a_i$ is individual-specific deviation in the education return (correlated with ability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b5002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate random coefficient model\n",
    "np.random.seed(123)\n",
    "n = 500\n",
    "\n",
    "# Generate education\n",
    "educ = np.random.uniform(8, 20, n)\n",
    "\n",
    "# Generate random slope deviations (varying returns to education)\n",
    "# Assume E(a_i | educ) = 0 (independence)\n",
    "a_i = np.random.normal(0, 0.02, n)  # SD = 0.02\n",
    "\n",
    "# Generate wages with random coefficients\n",
    "# log(wage) = 1.0 + (0.10 + a_i)*educ + u\n",
    "beta0_true = 1.0\n",
    "beta1_avg = 0.10  # Average return to education\n",
    "u = np.random.normal(0, 0.3, n)\n",
    "\n",
    "log_wage_rc = beta0_true + (beta1_avg + a_i) * educ + u\n",
    "\n",
    "rc_data = pd.DataFrame({\"log_wage\": log_wage_rc, \"educ\": educ})\n",
    "\n",
    "# Estimate with OLS\n",
    "model_rc = smf.ols(formula=\"log_wage ~ educ\", data=rc_data)\n",
    "results_rc_usual = model_rc.fit()\n",
    "results_rc_robust = model_rc.fit(cov_type=\"HC3\")\n",
    "\n",
    "# Compare usual vs robust SEs\n",
    "print(\"Random Coefficient Model: OLS Estimation\")\n",
    "print(f\"Education coefficient: {results_rc_usual.params['educ']:.4f}\")\n",
    "print(f\"  (True average: {beta1_avg:.4f})\")\n",
    "print(f\"Standard error (usual): {results_rc_usual.bse['educ']:.4f}\")\n",
    "print(f\"Standard error (robust): {results_rc_robust.bse['educ']:.4f}\")\n",
    "print(f\"  Ratio (robust/usual): {results_rc_robust.bse['educ']/results_rc_usual.bse['educ']:.3f}\")\n",
    "\n",
    "# Demonstrate heteroskedasticity due to random slopes\n",
    "# Variance of residuals should increase with educ^2\n",
    "residuals = results_rc_usual.resid\n",
    "rc_data[\"resid_sq\"] = residuals**2\n",
    "rc_data[\"educ_sq\"] = educ**2\n",
    "\n",
    "# Regress squared residuals on educ and educ^2 (heteroskedasticity test)\n",
    "het_test = smf.ols(formula=\"resid_sq ~ educ + educ_sq\", data=rc_data).fit()\n",
    "print(f\"\\nHeteroskedasticity test (educ^2 coefficient): {het_test.params['educ_sq']:.4f}\")\n",
    "print(f\"  p-value: {het_test.pvalues['educ_sq']:.4f}\")\n",
    "if het_test.pvalues[\"educ_sq\"] < 0.05:\n",
    "    print(\"  -> Significant heteroskedasticity (as expected with random slopes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5009cb",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "\n",
    "- OLS estimates the **average** return to education consistently\n",
    "- Robust SEs are larger than usual SEs (accounting for heteroskedasticity)\n",
    "- Residual variance increases with $x^2$ (evidence of random slopes)\n",
    "- Always use **robust standard errors** when random coefficients are suspected\n",
    "\n",
    "### 9.7.4 When Do Random Coefficients Matter?\n",
    "\n",
    "**Examples where coefficients likely vary:**\n",
    "\n",
    "1. **Returns to education:** Vary by ability, field, location\n",
    "2. **Price elasticity:** Varies across consumers (income, preferences)\n",
    "3. **Treatment effects:** Vary by individual characteristics (heterogeneous effects)\n",
    "4. **Production functions:** Vary by firm technology or management\n",
    "5. **Regional policies:** Same policy, different regional impacts\n",
    "\n",
    "**Practical implications:**\n",
    "\n",
    "- **Standard practice:** Use robust SEs to account for potential random coefficients\n",
    "- **Advanced models:** \n",
    "  - **Quantile regression** (Section 9.5) estimates effects at different points of conditional distribution\n",
    "  - **Interaction terms** (Chapter 7) allow effects to vary with observables\n",
    "  - **Panel data random effects** (Chapter 14) model individual-specific coefficients\n",
    "  - **Machine learning** methods can estimate heterogeneous effects\n",
    "\n",
    "**Key takeaway:**\n",
    "\n",
    "Even if coefficients vary randomly across observations, **OLS still consistently estimates the average effect** as long as the coefficient variation is independent of $x$. However, **heteroskedasticity-robust inference is essential** since the error variance is non-constant.\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "This chapter addressed critical **specification and data issues** that arise frequently in empirical econometric work. While earlier chapters assumed the model was correctly specified with clean data, real-world analysis requires careful attention to functional form, measurement quality, missing observations, outliers, and alternative estimation methods. Understanding these issues is essential for producing credible and robust research.\n",
    "\n",
    "### 9.1 Key Concepts\n",
    "\n",
    "**Functional Form Misspecification**\n",
    "\n",
    "- **Linear-in-variables** vs **linear-in-parameters**: OLS requires linearity in parameters but allows flexible functional forms\n",
    "- **Logarithms** for percentage effects and elasticities\n",
    "- **Polynomials** (quadratic, cubic) for non-linear relationships\n",
    "- **Interactions** between variables for effect modification\n",
    "- **RESET test** detects functional form misspecification using powers of fitted values\n",
    "- **Davidson-MacKinnon test** (J-test) compares non-nested models\n",
    "\n",
    "**Measurement Error**\n",
    "\n",
    "- **Measurement error in $y$** (dependent variable): Absorbed into error term, increases variance but doesn't bias coefficients\n",
    "- **Measurement error in $x$** (explanatory variable): Causes **attenuation bias** toward zero (underestimate true effect)\n",
    "- **Classical measurement error**: $x^* = x + e$ where $e$ is uncorrelated with true $x^*$ and other variables\n",
    "- **Proxy variables**: Use observable correlated variable to reduce omitted variable bias\n",
    "\n",
    "**Missing Data**\n",
    "\n",
    "- **Missing completely at random (MCAR)**: Missingness independent of all variables - no bias but loses efficiency\n",
    "- **Missing at random (MAR)**: Missingness depends on observables - bias if not controlled\n",
    "- **Missing not at random (MNAR)**: Missingness depends on unobservables - leads to sample selection bias\n",
    "\n",
    "**Outliers and Influential Observations**\n",
    "\n",
    "- **Outliers**: Observations far from the regression line (large residuals)\n",
    "- **Leverage**: Observations with extreme $x$ values that can heavily influence estimates  \n",
    "- **Influential points**: Combine high leverage with unusual $y$ values\n",
    "- **Diagnostic tools**: Residual plots, leverage statistics, Cook's distance, DFBETAS\n",
    "\n",
    "**Robust Estimation**\n",
    "\n",
    "- **Least Absolute Deviations (LAD)**: Minimizes absolute deviations, more robust to outliers\n",
    "- Estimates **conditional median** instead of conditional mean\n",
    "- Less efficient than OLS when errors are normal\n",
    "- More robust when errors are heavy-tailed or outliers present\n",
    "\n",
    "### 9.2 Python Implementation Patterns\n",
    "\n",
    "**Functional Form Testing**\n",
    "\n",
    "```\n",
    "# RESET test\n",
    "from statsmodels.stats.diagnostic import linear_reset\n",
    "\n",
    "reset_stat, reset_pval, _ = linear_reset(results, power=2)\n",
    "# Reject H0 (linear) if p-value < 0.05\n",
    "\n",
    "# Davidson-MacKinnon J-test\n",
    "fitted_model2 = results_model2.fittedvalues\n",
    "data['y_hat_2'] = fitted_model2\n",
    "augmented_model1 = smf.ols('y ~ x1 + x2 + y_hat_2', data=data).fit()\n",
    "# Check significance of y_hat_2\n",
    "```\n",
    "\n",
    "**Measurement Error Simulation**\n",
    "\n",
    "```\n",
    "# Generate measurement error\n",
    "x_star = true_x\n",
    "x_measured = x_star + np.random.normal(0, sigma_e, n)\n",
    "\n",
    "# Attenuation factor\n",
    "attenuation = sigma_x**2 / (sigma_x**2 + sigma_e**2)\n",
    "# E(beta_hat) = attenuation * beta_true\n",
    "```\n",
    "\n",
    "**Missing Data Handling**\n",
    "\n",
    "```\n",
    "# Check missingness patterns\n",
    "data.isnull().sum()\n",
    "data[data['variable'].isnull()].describe()\n",
    "\n",
    "# Listwise deletion (complete case analysis)\n",
    "data_complete = data.dropna()\n",
    "\n",
    "# Create missingness indicator\n",
    "data['missing_x'] = data['x'].isnull().astype(int)\n",
    "```\n",
    "\n",
    "**Outlier Detection**\n",
    "\n",
    "```\n",
    "# Standardized residuals\n",
    "from scipy import stats\n",
    "standardized = results.resid / np.std(results.resid)\n",
    "outliers = np.abs(standardized) > 3\n",
    "\n",
    "# Leverage\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "influence = OLSInfluence(results)\n",
    "leverage = influence.hat_matrix_diag\n",
    "high_leverage = leverage > 2 * k / n  # Rule of thumb\n",
    "\n",
    "# Cook's distance\n",
    "cooks_d = influence.cooks_distance[0]\n",
    "influential = cooks_d > 4 / n\n",
    "```\n",
    "\n",
    "**LAD Estimation**\n",
    "\n",
    "```\n",
    "# Quantile regression at median (LAD)\n",
    "import statsmodels.formula.api as smf\n",
    "model_lad = smf.quantreg('y ~ x1 + x2', data=data)\n",
    "results_lad = model_lad.fit(q=0.5)  # q=0.5 for median\n",
    "```\n",
    "\n",
    "**Proxy Variables**\n",
    "\n",
    "```\n",
    "# Include proxy in regression to reduce OVB\n",
    "model_proxy = smf.ols('y ~ x + proxy_for_omitted', data=data).fit()\n",
    "# Compare to naive model without proxy\n",
    "```\n",
    "\n",
    "### 9.3 Common Pitfalls and Best Practices\n",
    "\n",
    "**Functional Form**\n",
    "\n",
    "**DON'T:** Use linear specification for all relationships without investigation\n",
    "**DO:** Plot $y$ vs $x$ to visualize relationship before modeling\n",
    "**DO:** Use RESET or graphical diagnostics to test specification\n",
    "**DO:** Consider economic theory when choosing functional forms\n",
    "\n",
    "**Measurement Error**\n",
    "\n",
    "**DON'T:** Ignore that administrative or survey data may have measurement error\n",
    "**DON'T:** Use variables with large measurement error as key explanatory variables\n",
    "**DO:** Understand which variables are measured accurately\n",
    "**DO:** Report results both with and without noisy variables\n",
    "**DO:** Consider instrumental variables (Chapter 15) if measurement error is severe\n",
    "\n",
    "**Missing Data**\n",
    "\n",
    "**DON'T:** Delete missing observations without investigating the pattern\n",
    "**DON'T:** Assume MCAR when missingness might depend on outcomes\n",
    "**DO:** Document how many observations are lost and why\n",
    "**DO:** Compare characteristics of complete vs incomplete cases\n",
    "**DO:** Test whether missingness predicts outcomes\n",
    "**DO:** Use selection correction methods (Chapter 17) if MNAR is likely\n",
    "\n",
    "**Outliers**\n",
    "\n",
    "**DON'T:** Automatically delete outliers without justification\n",
    "**DON'T:** Ignore outliers that contradict your hypothesis\n",
    "**DO:** Identify outliers using multiple diagnostics (residuals + leverage)\n",
    "**DO:** Investigate whether outliers are data errors or genuine extreme values\n",
    "**DO:** Report results with and without outliers\n",
    "**DO:** Use robust methods (LAD, robust SEs) as sensitivity checks\n",
    "\n",
    "**General Best Practices**\n",
    "\n",
    "* **Exploratory data analysis:** Always examine distributions, relationships, and data quality before modeling\n",
    "* **Diagnostic plots:** Use residual plots, Q-Q plots, leverage plots routinely\n",
    "* **Robustness checks:** Estimate models with different specifications, subsamples, methods\n",
    "* **Transparency:** Report all specification searches and sensitivity analyses\n",
    "* **Domain knowledge:** Use economic theory and institutional knowledge to guide specification\n",
    "\n",
    "### 9.4 Decision Framework\n",
    "\n",
    "**Choosing Functional Form:**\n",
    "\n",
    "1. **Theory:** What functional form does economic theory suggest? (e.g., Cobb-Douglas  logs)\n",
    "2. **Data:** Plot relationships - are they linear, curved, or non-monotonic?\n",
    "3. **Testing:** Run RESET test - does it reject linear specification?\n",
    "4. **Interpretation:** Will logs or polynomials aid interpretation? (elasticities, turning points)\n",
    "5. **Comparison:** Use AIC/BIC or cross-validation to compare nested/non-nested models\n",
    "\n",
    "**Handling Measurement Error:**\n",
    "\n",
    "| Situation | Best Approach |\n",
    "|-----------|---------------|\n",
    "| Error in $y$ only | Standard OLS (unbiased) |\n",
    "| Small error in $x$ | OLS + robust SEs + acknowledge limitation |\n",
    "| Large error in $x$ | Find better data or use IV (Chapter 15) |\n",
    "| Error in multiple variables | IV or errors-in-variables models |\n",
    "\n",
    "**Handling Missing Data:**\n",
    "\n",
    "| Missingness Type | Strategy |\n",
    "|------------------|----------|\n",
    "| MCAR (rare) | Complete case analysis acceptable (inefficient) |\n",
    "| MAR | Include variables predicting missingness as controls |\n",
    "| MNAR | Selection models (Chapter 17) or bound analysis |\n",
    "| Large missingness | Report with/without imputation; document assumptions |\n",
    "\n",
    "**Handling Outliers:**\n",
    "\n",
    "```\n",
    "Decision Tree:\n",
    "1. Are outliers data errors?\n",
    "   -> YES: Correct or delete (document correction)\n",
    "   -> NO: Proceed to step 2\n",
    "\n",
    "2. Are they from the target population?\n",
    "   -> NO: Consider separate analysis or exclusion (justify)\n",
    "   -> YES: Proceed to step 3\n",
    "\n",
    "3. Do they change substantive conclusions?\n",
    "   -> NO: Report main results, note robustness\n",
    "   -> YES: Proceed to step 4\n",
    "\n",
    "4. Reporting strategy:\n",
    "   - Main results with outliers (full sample)\n",
    "   - Robustness with LAD or winsorization\n",
    "   - Sensitivity analysis excluding outliers\n",
    "   - Explain which you prefer and why\n",
    "```\n",
    "\n",
    "### 9.5 Connections to Other Chapters\n",
    "\n",
    "**To Chapter 6 (Further Issues in MRA):**\n",
    "- Builds on functional form discussion (logs, interactions from Ch6)\n",
    "- Extends scaling and measurement concerns\n",
    "- Connects proxy variables to omitted variable bias (Ch6.3)\n",
    "\n",
    "**To Chapter 8 (Heteroskedasticity):**\n",
    "- Random coefficients (Section 9.7) naturally create heteroskedasticity\n",
    "- Robust SEs essential when coefficient variation suspected\n",
    "- LAD as alternative to WLS for dealing with non-constant variance\n",
    "\n",
    "**To Chapter 13-14 (Panel Data):**\n",
    "- Fixed effects eliminate time-invariant measurement error\n",
    "- Panel data allows differencing out individual-specific unobservables (proxy alternative)\n",
    "- Random effects are special case of random coefficients\n",
    "\n",
    "**To Chapter 15 (Instrumental Variables):**\n",
    "- IV solution to measurement error in $x$ more general than proxies\n",
    "- Both address endogeneity but with different assumptions\n",
    "- IV requires excluded instruments; proxies require inclusion restrictions\n",
    "\n",
    "**To Chapter 17 (Limited Dependent Variables):**\n",
    "- Sample selection is special case of MNAR missing data\n",
    "- Heckman correction formally models selection process\n",
    "- Tobit model handles censored data (special missingness case)\n",
    "\n",
    "### 9.6 Mathematical Summary\n",
    "\n",
    "**Functional Form:**\n",
    "\n",
    "True model: $y = g(x_1, \\ldots, x_k, u)$\n",
    "\n",
    "Approximation: $y \\approx \\beta_0 + \\beta_1 h_1(x) + \\cdots + \\beta_p h_p(x) + \\text{error}$\n",
    "\n",
    "where $h_j(x)$ are transformations (logs, polynomials, interactions).\n",
    "\n",
    "**RESET statistic:**\n",
    "\n",
    "$$\n",
    "F = \\frac{(\\text{SSR}_{\\text{restricted}} - \\text{SSR}_{\\text{unrestricted}})/q}{\\text{SSR}_{\\text{unrestricted}}/(n - k_{\\text{unrestricted}})}\n",
    "$$\n",
    "\n",
    "**Measurement error in $x$:**\n",
    "\n",
    "True model: $y = \\beta_0 + \\beta_1 x^* + u$\n",
    "\n",
    "Observed: $x = x^* + e$ where $\\text{Cov}(x^*, e) = 0$\n",
    "\n",
    "Attenuation bias:\n",
    "\n",
    "$$\n",
    "\\text{plim} \\, \\hat{\\beta}_1 = \\beta_1 \\cdot \\frac{\\text{Var}(x^*)}{\\text{Var}(x^*) + \\text{Var}(e)} = \\beta_1 \\cdot \\lambda\n",
    "$$\n",
    "\n",
    "where $0 < \\lambda < 1$ (attenuation factor).\n",
    "\n",
    "**Proxy variables:**\n",
    "\n",
    "Unobserved: $y = \\beta_0 + \\beta_1 x + \\beta_2 a^* + u$\n",
    "\n",
    "Proxy relation: $a^* = \\delta_0 + \\delta_1 a + v$ where $\\text{Cov}(v, x) = 0$\n",
    "\n",
    "Estimated model: $y = \\alpha_0 + \\alpha_1 x + \\alpha_2 a + \\text{error}$\n",
    "\n",
    "Under proxy assumptions: $\\hat{\\alpha}_1 \\xrightarrow{p} \\beta_1$ (consistent for effect of $x$)\n",
    "\n",
    "**Random coefficients:**\n",
    "\n",
    "$$\n",
    "y_i = \\beta_{0i} + \\beta_{1i} x_i + u_i = (\\beta_0 + a_{0i}) + (\\beta_1 + a_{1i}) x_i + u_i\n",
    "$$\n",
    "\n",
    "Composite error: $v_i = a_{0i} + a_{1i} x_i + u_i$\n",
    "\n",
    "Heteroskedasticity:\n",
    "\n",
    "$$\n",
    "\\text{Var}(v_i | x_i) = \\sigma_{a_0}^2 + x_i^2 \\sigma_{a_1}^2 + 2x_i \\sigma_{a_0 a_1} + \\sigma_u^2\n",
    "$$\n",
    "\n",
    "**LAD estimator:**\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{LAD}} = \\arg\\min_{\\beta} \\sum_{i=1}^n |y_i - x_i' \\beta|\n",
    "$$\n",
    "\n",
    "Estimates: $\\text{Median}(y | x)$ instead of $E(y | x)$\n",
    "\n",
    "### 9.7 Learning Objectives Recap\n",
    "\n",
    "This chapter covered all 11 learning objectives:\n",
    "\n",
    "**9.1** - Explain why choosing the wrong functional form can bias coefficient estimates and describe methods to test for and correct functional form misspecification\n",
    "\n",
    "**9.2** - Understand the consequences of using proxy variables for unobserved explanatory variables and the assumptions required for valid inference\n",
    "\n",
    "**9.3** - Analyze the implications of models with random slopes (random coefficients) and explain why heteroskedasticity-robust standard errors are necessary\n",
    "\n",
    "**9.4** - Apply the RESET test to detect functional form misspecification and interpret results\n",
    "\n",
    "**9.5** - Conduct Davidson-MacKinnon J-tests to compare non-nested model specifications\n",
    "\n",
    "**9.6** - Demonstrate how measurement error in the dependent variable affects OLS estimates differently than measurement error in explanatory variables\n",
    "\n",
    "**9.7** - Explain the attenuation bias that results from classical measurement error in explanatory variables and calculate the bias analytically\n",
    "\n",
    "**9.8** - Distinguish between missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR), and select appropriate methods for each\n",
    "\n",
    "**9.9** - Identify outliers and influential observations using residual analysis, leverage statistics, and Cook's distance\n",
    "\n",
    "**9.10** - Implement Least Absolute Deviations (LAD) estimation as a robust alternative to OLS when outliers are present\n",
    "\n",
    "**9.11** - Conduct comprehensive specification and data quality diagnostics for empirical projects, including functional form tests, outlier detection, and missing data analysis\n",
    "\n",
    "### 9.8 Further Reading and Extensions\n",
    "\n",
    "**Advanced topics not covered:**\n",
    "\n",
    "- **Imputation methods:** Multiple imputation, EM algorithm for handling missing data\n",
    "- **Robust regression:** M-estimators, Huber regression, bounded influence estimators\n",
    "- **Non-parametric regression:** Kernel regression, local polynomial regression, splines\n",
    "- **Model averaging:** Combining predictions from multiple specifications\n",
    "- **Diagnostic testing:** Leverage plots, partial regression plots, added variable plots\n",
    "\n",
    "**Recommended resources:**\n",
    "\n",
    "- Wooldridge (2020), Chapters 9 and 17 for theoretical foundations\n",
    "- Cameron & Trivedi (2005), \"Microeconometrics\" for advanced treatment of missing data and outliers\n",
    "- Angrist & Pischke (2009), \"Mostly Harmless Econometrics\" for practical specification advice\n",
    "- Fox (2016), \"Applied Regression Analysis and GLMs in R\" for extensive diagnostic methods\n",
    "- Heiss (2020), \"Using Python for Introductory Econometrics\" for Python implementations\n",
    "\n",
    "This chapter equipped you with essential tools for **data cleaning**, **model specification**, and **robust analysis**. Applying these methods systematically will improve the credibility and replicability of empirical work in economics and related fields."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "notebooks//ipynb,markdown//md,scripts//py"
  },
  "kernelspec": {
   "display_name": "merino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
