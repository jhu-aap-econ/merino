{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Multiple Regression Analysis: OLS Asymptotics\n",
    "\n",
    "This notebook explores the asymptotic properties of Ordinary Least Squares (OLS) estimators in multiple regression analysis. Asymptotic theory is crucial because it describes the behavior of estimators as the sample size grows infinitely large ($n \\to \\infty$). In practice, we often rely on these asymptotic properties to make inferences when sample sizes are reasonably large.\n",
    "\n",
    "## Asymptotic Properties of OLS\n",
    "\n",
    "Under the Gauss-Markov assumptions **MLR.1-MLR.4** (from Chapter 3: linearity, random sampling, no perfect collinearity, and zero conditional mean), OLS estimators have the following asymptotic properties:\n",
    "\n",
    "**1. Consistency:** $\\hat{\\beta}_j \\xrightarrow{p} \\beta_j$ as $n \\to \\infty$ for all $j = 0, 1, \\ldots, k$\n",
    "\n",
    "The OLS estimators converge in probability to the true parameter values as the sample size grows large. Consistency is a weaker property than unbiasedness--it only requires that the estimator approaches the true value asymptotically. Notably, consistency does **not** require normality (MLR.6) or homoscedasticity (MLR.5).\n",
    "\n",
    "**2. Asymptotic Normality:** $\\sqrt{n}(\\hat{\\beta}_j - \\beta_j) \\xrightarrow{d} N(0, \\sigma^2_{\\beta_j})$ as $n \\to \\infty$\n",
    "\n",
    "By the **Central Limit Theorem (CLT)**, the sampling distribution of $\\hat{\\beta}_j$ approaches a normal distribution as $n \\to \\infty$, even when the errors are **not** normally distributed. This requires:\n",
    "- MLR.1-MLR.4 (especially zero conditional mean)\n",
    "- Finite fourth moments of the errors: $E(u^4) < \\infty$ (mild regularity condition)\n",
    "- **No normality assumption required**\n",
    "\n",
    "**Practical Implications:**\n",
    "- With large samples ($n \\geq 30$ typically), t-tests and F-tests are approximately valid even without normal errors (MLR.6)\n",
    "- Consistency means OLS remains reliable in large samples even under violations of MLR.5 (heteroskedasticity)\n",
    "- Asymptotic inference requires only MLR.1-MLR.4 plus regularity conditions, making it more robust than finite-sample inference\n",
    "\n",
    "We will use simulations to visualize these concepts and then apply the Lagrange Multiplier (LM) test to a real-world example.\n",
    "\n",
    "**Connection to Previous Chapters:**\n",
    "- Chapters 2-3 established finite-sample properties under Gauss-Markov assumptions\n",
    "- Chapter 4 showed exact inference requires normality (MLR.6) for finite samples\n",
    "- This chapter demonstrates that normality is **not necessary** for large-sample inference\n",
    "- These results justify the robustness claims made in Chapter 4 about t-tests with $n \\geq 30$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:52:10.978338Z",
     "iopub.status.busy": "2025-10-19T23:52:10.978205Z",
     "iopub.status.idle": "2025-10-19T23:52:12.103378Z",
     "shell.execute_reply": "2025-10-19T23:52:12.103079Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import wooldridge as woo\n",
    "from scipy import stats\n",
    "\n",
    "# Configure matplotlib to avoid font parsing issues\n",
    "plt.rcParams['mathtext.fontset'] = 'dejavusans'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Simulation Exercises\n",
    "\n",
    "In this section, we will conduct simulation exercises to illustrate the asymptotic properties of the OLS estimator, particularly focusing on its distribution as the sample size increases under different scenarios.\n",
    "\n",
    "### 5.1.1 Normally Distributed Error Terms\n",
    "\n",
    "This simulation demonstrates the behavior of the OLS estimator when the error terms are normally distributed.  Under the classical linear model assumptions, including normally distributed errors, the OLS estimators are not only BLUE (Best Linear Unbiased Estimator) but also have desirable properties even in small samples. Asymptotically, the OLS estimator is consistent and normally distributed. We will visualize how the distribution of the estimated coefficient $\\hat{\\beta}_1$ approaches a normal distribution as the sample size $n$ increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:52:12.105247Z",
     "iopub.status.busy": "2025-10-19T23:52:12.105084Z",
     "iopub.status.idle": "2025-10-19T23:52:12.490417Z",
     "shell.execute_reply": "2025-10-19T23:52:12.490143Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Generating Process</td>\n",
       "      <td>y = 1.0 + 0.5*x + u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X Distribution</td>\n",
       "      <td>X ~ N(4.0, 1.0^2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Error Distribution</td>\n",
       "      <td>u ~ N(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Replications</td>\n",
       "      <td>10,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Parameter                Value\n",
       "0  Data Generating Process  y = 1.0 + 0.5*x + u\n",
       "1           X Distribution    X ~ N(4.0, 1.0^2)\n",
       "2       Error Distribution          u ~ N(0, 1)\n",
       "3             Replications               10,000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rg/vmn_nq41613gkxt0_9spwzx80000gp/T/ipykernel_46779/1391603992.py:122: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()  # Display the plot\n"
     ]
    }
   ],
   "source": [
    "# Monte Carlo Simulation Setup: OLS with Normal Errors\n",
    "# Demonstrates convergence to normality as sample size increases\n",
    "\n",
    "# Set random seed for reproducible results\n",
    "np.random.seed(1234567)\n",
    "\n",
    "# Configure simulation parameters\n",
    "sample_sizes = [5, 10, 100, 1000]  # n: Small to large samples\n",
    "num_replications = 10000  # r: Number of Monte Carlo iterations\n",
    "\n",
    "# Define true population parameters (Data Generating Process)\n",
    "# True model: y = beta_0 + beta_1x + u, where u ~ N(0, 1)\n",
    "true_intercept = 1.0  # beta_0 = 1\n",
    "true_slope = 0.5  # beta_1 = 0.5\n",
    "x_std_dev = 1.0  # sigma_x: Standard deviation of x\n",
    "x_mean = 4.0  # mu_x: Mean of x\n",
    "\n",
    "# SIMULATION 1: OLS WITH NORMAL ERRORS\n",
    "sim_info = pd.DataFrame(\n",
    "    {\n",
    "        \"Parameter\": [\n",
    "            \"Data Generating Process\",\n",
    "            \"X Distribution\",\n",
    "            \"Error Distribution\",\n",
    "            \"Replications\",\n",
    "        ],\n",
    "        \"Value\": [\n",
    "            f\"y = {true_intercept} + {true_slope}*x + u\",\n",
    "            f\"X ~ N({x_mean}, {x_std_dev}^2)\",\n",
    "            \"u ~ N(0, 1)\",\n",
    "            f\"{num_replications:,}\",\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "display(sim_info)\n",
    "\n",
    "# Create visualization grid for results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axes = axes.ravel()  # Flatten for easier iteration\n",
    "\n",
    "# Run simulation for each sample size\n",
    "for idx, n in enumerate(sample_sizes):\n",
    "    # Step 1: Generate fixed x values for this sample size\n",
    "    # X is held constant across replications to isolate error term effects\n",
    "    x_values = stats.norm.rvs(x_mean, x_std_dev, size=n)\n",
    "\n",
    "    # Step 2: Generate error terms for all replications\n",
    "    # Shape: (num_replications, n) - each row is one replication\n",
    "    error_terms = stats.norm.rvs(0, 1, size=(num_replications, n))\n",
    "\n",
    "    # Step 3: Generate y values using true DGP\n",
    "    # y = beta_0 + beta_1*x + u for each replication\n",
    "    y_values = true_intercept + true_slope * x_values + error_terms\n",
    "\n",
    "    # Step 4: Construct design matrix X (same for all replications)\n",
    "    # X = [1, x] where first column is for intercept\n",
    "    X_matrix = np.column_stack((np.ones(n), x_values))\n",
    "\n",
    "    # Step 5: Pre-compute matrix operations for efficiency\n",
    "    # (X'X)^(-1)X' is constant across replications since X is fixed\n",
    "    XtX_inv = np.linalg.inv(X_matrix.T @ X_matrix)\n",
    "    XtX_inv_Xt = XtX_inv @ X_matrix.T\n",
    "\n",
    "    # Step 6: Estimate coefficients for all replications at once\n",
    "    # beta_hat = (X'X)^(-1)X'y for each replication\n",
    "    # Result: 2 x num_replications matrix (each column = one replication)\n",
    "    all_coefficients = XtX_inv_Xt @ y_values.T\n",
    "    slope_estimates = all_coefficients[1, :]  # Extract beta_hat_1 (second row)\n",
    "\n",
    "    # Step 7: Calculate theoretical standard error for comparison\n",
    "    # Under CLM assumptions: Var(beta_hat) = sigma^2(X'X)^(-1), where sigma^2 = 1\n",
    "    variance_matrix = XtX_inv  # Since sigma^2 = 1\n",
    "    theoretical_se = np.sqrt(variance_matrix[1, 1])  # SE(beta_hat_1)\n",
    "\n",
    "    # Step 8: Estimate empirical density using kernel density estimation\n",
    "    kde = sm.nonparametric.KDEUnivariate(slope_estimates)\n",
    "    kde.fit()\n",
    "\n",
    "    # Step 9: Generate theoretical normal distribution for comparison\n",
    "    x_range = np.linspace(min(slope_estimates), max(slope_estimates), 1000)\n",
    "    theoretical_density = stats.norm.pdf(x_range, true_slope, theoretical_se)\n",
    "\n",
    "    # Step 10: Plot empirical vs theoretical distributions\n",
    "    axes[idx].plot(\n",
    "        kde.support,\n",
    "        kde.density,\n",
    "        color=\"black\",\n",
    "        linewidth=2,\n",
    "        label=\"Empirical Density (KDE)\",\n",
    "    )\n",
    "    axes[idx].plot(\n",
    "        x_range,\n",
    "        theoretical_density,\n",
    "        linestyle=\"--\",\n",
    "        color=\"red\",\n",
    "        linewidth=1.5,\n",
    "        label=\"Theoretical Normal\",\n",
    "    )\n",
    "\n",
    "    # Add visualization details\n",
    "    axes[idx].set_ylabel(\"Density\")\n",
    "    axes[idx].set_xlabel(r\"$\\hat{\\beta}_1$ (Slope Estimate)\")\n",
    "    axes[idx].legend(loc=\"best\", fontsize=9)\n",
    "    axes[idx].set_title(f\"Sample Size n = {n}\")\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "    # Add statistics annotation\n",
    "    mean_est = np.mean(slope_estimates)\n",
    "    std_est = np.std(slope_estimates)\n",
    "    axes[idx].text(\n",
    "        0.05,\n",
    "        0.95,\n",
    "        f\"Mean: {mean_est:.4f}\\nStd: {std_est:.4f}\",\n",
    "        transform=axes[idx].transAxes,\n",
    "        verticalalignment=\"top\",\n",
    "        fontsize=8,\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.5),\n",
    "    )\n",
    "\n",
    "plt.suptitle(\"Convergence to Normality: OLS with Normal Errors\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of 5.1.1:**\n",
    "\n",
    "The plots above show the simulated density of the OLS estimator $\\hat{\\beta}_1$ for different sample sizes ($n = 5, 10, 100, 1000$) when the error term is normally distributed.  We compare this simulated density to the theoretical normal distribution that $\\hat{\\beta}_1$ should asymptotically follow.\n",
    "\n",
    "- **Small Sample Sizes (n=5, 10):** For very small sample sizes, the simulated density of $\\hat{\\beta}_1$ is somewhat close to the normal distribution, but there are noticeable deviations. This is because while OLS is BLUE under these conditions, the asymptotic normality is approached as $n \\rightarrow \\infty$.\n",
    "\n",
    "- **Larger Sample Sizes (n=100, 1000):** As the sample size increases to $n=100$ and $n=1000$, the simulated density of $\\hat{\\beta}_1$ gets increasingly closer to the theoretical normal distribution.  For $n=1000$, the simulated density is almost indistinguishable from the normal distribution.\n",
    "\n",
    "This simulation visually confirms that when the errors are normally distributed, the distribution of the OLS estimator $\\hat{\\beta}_1$ approaches a normal distribution as the sample size $n$ increases, consistent with asymptotic theory and even showing reasonable approximation for smaller sample sizes in this ideal scenario.\n",
    "\n",
    "### 5.1.2 Non-Normal Error Terms\n",
    "\n",
    "In this simulation, we investigate what happens when one of the classical linear model assumptions is violated - specifically, the assumption of normally distributed errors. We will use error terms that follow a standardized Chi-squared distribution with 1 degree of freedom.  Even when the error term is not normally distributed, under the Gauss-Markov conditions and assuming homoskedasticity and no autocorrelation, OLS is still BLUE. More importantly, even with non-normal errors, the OLS estimator is still consistent and asymptotically normally distributed under weaker conditions (CLT for sample averages). This simulation will demonstrate the asymptotic normality even with non-normal errors.\n",
    "\n",
    "First, let's visualize the shape of the standardized Chi-squared distribution compared to the standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:52:12.491757Z",
     "iopub.status.busy": "2025-10-19T23:52:12.491675Z",
     "iopub.status.idle": "2025-10-19T23:52:12.496197Z",
     "shell.execute_reply": "2025-10-19T23:52:12.495884Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rg/vmn_nq41613gkxt0_9spwzx80000gp/T/ipykernel_46779/369378280.py:29: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# support of normal density:\n",
    "x_range = np.linspace(-4, 4, num=100)\n",
    "\n",
    "# pdf for standard normal distribution:\n",
    "pdf_n = stats.norm.pdf(x_range)\n",
    "# pdf for standardized chi-squared distribution with 1 degree of freedom.\n",
    "# We subtract the mean (which is 1 for chi2(1)) and divide by the standard deviation (which is sqrt(2) for chi2(1)) to standardize it.\n",
    "pdf_c = stats.chi2.pdf(x_range * np.sqrt(2) + 1, 1)\n",
    "\n",
    "# plot:\n",
    "plt.plot(\n",
    "    x_range,\n",
    "    pdf_n,\n",
    "    linestyle=\"-\",\n",
    "    color=\"black\",\n",
    "    label=\"Standard Normal Distribution\",\n",
    ")\n",
    "plt.plot(\n",
    "    x_range,\n",
    "    pdf_c,\n",
    "    linestyle=\"--\",\n",
    "    color=\"black\",\n",
    "    label=\"Standardized Chi-squared[1] Distribution\",\n",
    ")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.title(\"Comparison of Standard Normal and Standardized Chi-squared(1) Distributions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4dce92",
   "metadata": {},
   "source": [
    "The plot above shows that the standardized Chi-squared distribution is skewed to the right and has a different shape compared to the standard normal distribution. Now, let's perform the simulation with these non-normal errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:52:12.497557Z",
     "iopub.status.busy": "2025-10-19T23:52:12.497461Z",
     "iopub.status.idle": "2025-10-19T23:52:13.161578Z",
     "shell.execute_reply": "2025-10-19T23:52:13.161276Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rg/vmn_nq41613gkxt0_9spwzx80000gp/T/ipykernel_46779/2972247688.py:71: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# set the random seed for reproducibility:\n",
    "np.random.seed(1234567)\n",
    "\n",
    "# set sample sizes to be investigated:\n",
    "n = [5, 10, 100, 1000]\n",
    "# set number of simulations (replications):\n",
    "r = 10000\n",
    "\n",
    "# set true population parameters:\n",
    "beta0 = 1  # true intercept\n",
    "beta1 = 0.5  # true slope coefficient\n",
    "sx = 1  # standard deviation of x\n",
    "ex = 4  # expected value of x\n",
    "\n",
    "# Create a 2x2 subplot to display density plots for each sample size\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axs = axs.ravel()  # Flatten the 2x2 array of axes for easier indexing\n",
    "\n",
    "# Loop through each sample size in the list 'n'\n",
    "for idx, j in enumerate(n):\n",
    "    # draw a sample of x, fixed over replications:\n",
    "    x = stats.norm.rvs(ex, sx, size=j)\n",
    "\n",
    "    # Create the design matrix X. For each replication, X is the same as 'x' is fixed.\n",
    "    X = np.column_stack((np.ones(j), x))\n",
    "\n",
    "    # Compute (X'X)^(-1)X' once as X is fixed for each n.\n",
    "    XX_inv = np.linalg.inv(X.T @ X)\n",
    "    XTX_inv_XT = XX_inv @ X.T\n",
    "\n",
    "    # Generate error terms 'u' from a standardized Chi-squared distribution with 1 degree of freedom for all replications at once.\n",
    "    u = (stats.chi2.rvs(1, size=(r, j)) - 1) / np.sqrt(2)\n",
    "\n",
    "    # Compute the dependent variable 'y' for all replications at once using the true model: y = beta0 + beta1*x + u\n",
    "    y = beta0 + beta1 * x + u\n",
    "\n",
    "    # Estimate beta (including beta0 and beta1) for all 'r' replications at once.\n",
    "    b = XTX_inv_XT @ y.T\n",
    "    b1 = b[1, :]  # Extract all estimated beta1 coefficients.\n",
    "\n",
    "    # Estimate the PDF of the simulated b1 estimates using Kernel Density Estimation (KDE).\n",
    "    kde = sm.nonparametric.KDEUnivariate(b1)\n",
    "    kde.fit()\n",
    "\n",
    "    # Theoretical normal density, calculated the same way as in the normal error case.\n",
    "    Vbhat = XX_inv  # Variance-covariance matrix\n",
    "    se = np.sqrt(np.diagonal(Vbhat))\n",
    "    x_range = np.linspace(min(b1), max(b1), 1000)\n",
    "    y = stats.norm.pdf(x_range, beta1, se[1])\n",
    "\n",
    "    # plotting:\n",
    "    axs[idx].plot(\n",
    "        kde.support,\n",
    "        kde.density,\n",
    "        color=\"black\",\n",
    "        label=\"Simulated Density of b1\",\n",
    "    )\n",
    "    axs[idx].plot(\n",
    "        x_range,\n",
    "        y,\n",
    "        linestyle=\"--\",\n",
    "        color=\"black\",\n",
    "        label=\"Theoretical Normal Distribution\",\n",
    "    )\n",
    "    axs[idx].set_ylabel(\"Density\")\n",
    "    axs[idx].set_xlabel(r\"$\\hat{\\beta}_1$\")\n",
    "    axs[idx].legend()\n",
    "    axs[idx].set_title(f\"Sample Size n = {j}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of 5.1.2:**\n",
    "\n",
    "These plots illustrate the distribution of $\\hat{\\beta}_1$ when the error terms are from a standardized Chi-squared distribution, which is non-normal.\n",
    "\n",
    "- **Small Sample Sizes (n=5, 10):** For small sample sizes, the simulated density of $\\hat{\\beta}_1$ is noticeably skewed and deviates from the normal distribution, reflecting the non-normality of the error term.\n",
    "\n",
    "- **Larger Sample Sizes (n=100, 1000):** As the sample size increases, the simulated density of $\\hat{\\beta}_1$ becomes progressively more symmetric and approaches the theoretical normal distribution. By $n=1000$, the simulated distribution is very close to normal, even though the underlying errors are non-normal.\n",
    "\n",
    "This simulation demonstrates the power of the Central Limit Theorem in action. Even when the errors are not normally distributed, the OLS estimator $\\hat{\\beta}_1$, which is a function of the sample average of the error terms (indirectly), becomes approximately normally distributed as the sample size grows large. This is a key result in asymptotic theory, justifying the use of normal distribution based inference (t-tests, confidence intervals) in OLS regression with large samples, even if we suspect the errors are not normally distributed.\n",
    "\n",
    "### 5.1.3 (Not) Conditioning on the Regressors\n",
    "\n",
    "In previous simulations (5.1.1 and 5.1.2), we fixed the regressors $x$ across replications for each sample size $n$. This is akin to *conditioning on the regressors*. In econometric theory, we often derive properties of OLS estimators *conditional* on the observed values of the regressors. However, in reality, regressors are also random variables. This simulation explores the implications of *not conditioning* on the regressors by drawing new samples of $x$ in each replication, along with new error terms. We will see if the asymptotic normality of $\\hat{\\beta}_1$ still holds when both $x$ and $u$ are randomly drawn in each simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:52:13.163180Z",
     "iopub.status.busy": "2025-10-19T23:52:13.163066Z",
     "iopub.status.idle": "2025-10-19T23:52:14.072475Z",
     "shell.execute_reply": "2025-10-19T23:52:14.072205Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rg/vmn_nq41613gkxt0_9spwzx80000gp/T/ipykernel_46779/245936939.py:74: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# set the random seed for reproducibility:\n",
    "np.random.seed(1234567)\n",
    "\n",
    "# set sample sizes to be investigated:\n",
    "n = [5, 10, 100, 1000]\n",
    "# set number of simulations (replications):\n",
    "r = 10000\n",
    "\n",
    "# set true population parameters:\n",
    "beta0 = 1  # true intercept\n",
    "beta1 = 0.5  # true slope coefficient\n",
    "sx = 1  # standard deviation of x\n",
    "ex = 4  # expected value of x\n",
    "\n",
    "# Create a 2x2 subplot to display density plots for each sample size\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axs = axs.ravel()  # Flatten the 2x2 array of axes for easier indexing\n",
    "\n",
    "# Loop through each sample size in the list 'n'\n",
    "for idx, j in enumerate(n):\n",
    "    # initialize b1 to store results later:\n",
    "    b1 = np.empty(r)\n",
    "    XX_inv_list = []  # To store (X'X)^-1 for each replication\n",
    "\n",
    "    # draw a sample of x, varying over replications:\n",
    "    x = stats.norm.rvs(ex, sx, size=(r, j))\n",
    "    # draw a sample of u (std. normal):\n",
    "    u = stats.norm.rvs(0, 1, size=(r, j))\n",
    "    y = beta0 + beta1 * x + u\n",
    "    # repeat r times:\n",
    "    for i in range(r):\n",
    "        # Create design matrix X\n",
    "        X = np.column_stack((np.ones(j), x[i]))\n",
    "\n",
    "        # Compute (X'X)^(-1)\n",
    "        XX_inv = np.linalg.inv(X.T @ X)\n",
    "        XX_inv_list.append(XX_inv)  # Store (X'X)^-1 for averaging\n",
    "        XTX_inv_XT = XX_inv @ X.T  # Unused in variance calc, but used for beta\n",
    "\n",
    "        # Estimate beta for all replications at once\n",
    "        b = XTX_inv_XT @ y[i].T\n",
    "        b1[i] = b[1]\n",
    "\n",
    "    # simulated density:\n",
    "    kde = sm.nonparametric.KDEUnivariate(b1)\n",
    "    kde.fit()\n",
    "    # normal density/ compute mu and se\n",
    "    # Average (X'X)^-1 over replications\n",
    "    avg_XX_inv = np.mean(np.array(XX_inv_list), axis=0)\n",
    "    Vbhat = sx * avg_XX_inv  # Use averaged (X'X)^-1\n",
    "    se = np.sqrt(np.diagonal(Vbhat))\n",
    "    x_range = np.linspace(min(b1), max(b1))\n",
    "    y = stats.norm.pdf(x_range, beta1, se[1])\n",
    "    # plotting:\n",
    "    axs[idx].plot(\n",
    "        kde.support,\n",
    "        kde.density,\n",
    "        color=\"black\",\n",
    "        label=\"Simulated Density of b1\",\n",
    "    )\n",
    "    axs[idx].plot(\n",
    "        x_range,\n",
    "        y,\n",
    "        linestyle=\"--\",\n",
    "        color=\"black\",\n",
    "        label=\"Theoretical Normal Distribution\",\n",
    "    )\n",
    "    axs[idx].set_ylabel(\"Density\")\n",
    "    axs[idx].set_xlabel(r\"$\\hat{\\beta}_1$\")\n",
    "    axs[idx].legend()\n",
    "    axs[idx].set_title(f\"Sample Size n = {j}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of 5.1.3:**\n",
    "\n",
    "In this simulation, both the regressors $x$ and the error terms $u$ are randomly drawn in each replication.\n",
    "\n",
    "- **Small Sample Sizes (n=5, 10):** Similar to the previous simulations, with very small sample sizes, the simulated distribution of $\\hat{\\beta}_1$ shows some deviation from the normal distribution.\n",
    "\n",
    "- **Larger Sample Sizes (n=100, 1000):** As the sample size increases, even when we are not conditioning on the regressors, the distribution of $\\hat{\\beta}_1$ still converges to a normal distribution. By $n=1000$, the convergence is quite evident.\n",
    "\n",
    "This simulation reinforces the asymptotic normality of the OLS estimator even when we consider the randomness of the regressors. The key conditions for asymptotic normality are related to the properties of the population and the law of large numbers and central limit theorem applying to sample averages, which hold true whether we condition on regressors or not, as long as certain regularity conditions are met (like finite variance of $x$ and $u$, and exogeneity). This is fundamental because in most econometric applications, regressors are indeed random variables.\n",
    "\n",
    "## 5.2 LM Test\n",
    "\n",
    "The Lagrange Multiplier (LM) test, also known as the score test, is a statistical test used to test hypotheses in the context of constrained optimization. In econometrics, it is often used to test for omitted variables or other forms of model misspecification. It is particularly useful because it only requires estimation of the *restricted* model (the model under the null hypothesis).\n",
    "\n",
    "For testing restrictions in a linear regression model, the LM test statistic is often computationally simpler than the Wald or Likelihood Ratio tests, especially when the null hypothesis involves restrictions on coefficients.\n",
    "\n",
    "For testing $q$ restrictions of the form $H_0: R\\beta = r$ in a linear regression, where $R$ is a $q \\times (k+1)$ matrix and $r$ is a $q \\times 1$ vector, the LM test statistic can be calculated as:\n",
    "\n",
    "$$ \\text{LM} = n \\cdot R^2_{\\tilde{u}} \\sim \\chi^2_q \\text{ asymptotically under } H_0$$\n",
    "\n",
    "where:\n",
    "- $n$ is the sample size.\n",
    "- $R^2_{\\tilde{u}}$ is the R-squared from a regression of the residuals from the restricted model ($\\tilde{u}$) on all the independent variables in the *unrestricted* model.\n",
    "- $q$ is the number of restrictions being tested (degrees of freedom).\n",
    "- $\\chi^2_q$ denotes a Chi-squared distribution with $q$ degrees of freedom.\n",
    "\n",
    "The steps to perform an LM test are typically as follows:\n",
    "\n",
    "1. **Estimate the Restricted Model:** Estimate the regression model under the null hypothesis. Obtain the residuals from this restricted model ($\\tilde{u}$).\n",
    "2. **Auxiliary Regression:** Regress the residuals $\\tilde{u}$ from the restricted model on all the independent variables from the *unrestricted* model. Calculate the $R^2$ from this auxiliary regression ($R^2_{\\tilde{u}}$).\n",
    "3. **Calculate the LM Statistic:** Compute the LM test statistic as $LM = n \\cdot R^2_{\\tilde{u}}$.\n",
    "4. **Determine the p-value:** Compare the LM statistic to a $\\chi^2_q$ distribution, where $q$ is the number of restrictions. Calculate the p-value or compare the LM statistic to a critical value from the $\\chi^2_q$ distribution to make a decision.\n",
    "\n",
    "### Example 5.3: Economic Model of Crime\n",
    "\n",
    "We will use the `crime1` dataset from the `wooldridge` package to illustrate the LM test. The example considers an economic model of crime where the number of arrests (`narr86`) is modeled as a function of several factors.\n",
    "\n",
    "The unrestricted model is:\n",
    "\n",
    "$$\\text{narr86} = \\beta_0 + \\beta_1 \\cdot \\text{pcnv} + \\beta_2 \\cdot \\text{avgsen} + \\beta_3 \\cdot \\text{tottime} + \\beta_4 \\cdot \\text{ptime86} + \\beta_5 \\cdot \\text{qemp86} + u$$\n",
    "\n",
    "We want to test the null hypothesis that `avgsen` (average sentence length) and `tottime` (total time served) have no effect on `narr86`, i.e., $H_0: \\beta_2 = 0 \\text{ and } \\beta_3 = 0$.  Thus, we have $q=2$ restrictions.\n",
    "\n",
    "The restricted model under $H_0$ is:\n",
    "\n",
    "$$\\text{narr86} = \\beta_0 + \\beta_1 \\cdot \\text{pcnv} + \\beta_4 \\cdot \\text{ptime86} + \\beta_5 \\cdot \\text{qemp86} + u$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:52:14.073768Z",
     "iopub.status.busy": "2025-10-19T23:52:14.073685Z",
     "iopub.status.idle": "2025-10-19T23:52:14.087258Z",
     "shell.execute_reply": "2025-10-19T23:52:14.086772Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>R-squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Restricted</td>\n",
       "      <td>0.0413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model R-squared\n",
       "0  Restricted    0.0413"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crime1 = woo.dataWoo(\"crime1\")\n",
    "\n",
    "# 1. Estimate the restricted model under H0: beta_avgsen = 0 and beta_tottime = 0\n",
    "reg_r = smf.ols(formula=\"narr86 ~ pcnv + ptime86 + qemp86\", data=crime1)\n",
    "fit_r = reg_r.fit()\n",
    "r2_r = fit_r.rsquared\n",
    "# Display R-squared of Restricted Model\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": [\"Restricted\"],\n",
    "        \"R-squared\": [f\"{r2_r:.4f}\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:52:14.088549Z",
     "iopub.status.busy": "2025-10-19T23:52:14.088459Z",
     "iopub.status.idle": "2025-10-19T23:52:14.097333Z",
     "shell.execute_reply": "2025-10-19T23:52:14.097043Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>R-squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LM Auxiliary</td>\n",
       "      <td>0.0015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model R-squared\n",
       "0  LM Auxiliary    0.0015"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Obtain residuals from the restricted model and add them to the DataFrame\n",
    "crime1[\"utilde\"] = fit_r.resid\n",
    "\n",
    "# 3. Run auxiliary regression: regress residuals (utilde) on ALL variables from the UNRESTRICTED model\n",
    "reg_LM = smf.ols(\n",
    "    formula=\"utilde ~ pcnv + ptime86 + qemp86 + avgsen + tottime\",\n",
    "    data=crime1,\n",
    ")\n",
    "fit_LM = reg_LM.fit()\n",
    "r2_LM = fit_LM.rsquared\n",
    "# Display R-squared of LM Regression\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": [\"LM Auxiliary\"],\n",
    "        \"R-squared\": [f\"{r2_LM:.4f}\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:52:14.098562Z",
     "iopub.status.busy": "2025-10-19T23:52:14.098474Z",
     "iopub.status.idle": "2025-10-19T23:52:14.101725Z",
     "shell.execute_reply": "2025-10-19T23:52:14.101496Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statistic</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LM Test</td>\n",
       "      <td>4.071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Statistic  Value\n",
       "0   LM Test  4.071"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Calculate the LM test statistic: LM = n * R^2_utilde\n",
    "LM = r2_LM * fit_LM.nobs\n",
    "# Display LM Test Statistic\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Statistic\": [\"LM Test\"],\n",
    "        \"Value\": [f\"{LM:.3f}\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:52:14.102901Z",
     "iopub.status.busy": "2025-10-19T23:52:14.102824Z",
     "iopub.status.idle": "2025-10-19T23:52:14.106968Z",
     "shell.execute_reply": "2025-10-19T23:52:14.106704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test</th>\n",
       "      <th>df</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chi-squared critical value</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Test  df  Alpha  Value\n",
       "0  Chi-squared critical value   2    0.1  4.605"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Determine the critical value from the chi-squared distribution with q=2 degrees of freedom at alpha=10% significance level\n",
    "# For a test at 10% significance level, alpha = 0.10.\n",
    "# We want to find the chi-squared value such that the area to the right is 0.10.\n",
    "cv = stats.chi2.ppf(1 - 0.10, 2)  # ppf is the percent point function (inverse of CDF)\n",
    "# Display Critical Value\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Test\": [\"Chi-squared critical value\"],\n",
    "        \"df\": [2],\n",
    "        \"Alpha\": [0.10],\n",
    "        \"Value\": [f\"{cv:.3f}\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:52:14.108273Z",
     "iopub.status.busy": "2025-10-19T23:52:14.108189Z",
     "iopub.status.idle": "2025-10-19T23:52:14.111439Z",
     "shell.execute_reply": "2025-10-19T23:52:14.111201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LM Test</td>\n",
       "      <td>0.1306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Test p-value\n",
       "0  LM Test  0.1306"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Calculate the p-value for the LM test\n",
    "# The p-value is the probability of observing a test statistic as extreme as, or more extreme than, the one calculated, under the null hypothesis.\n",
    "pval = 1 - stats.chi2.cdf(LM, 2)  # cdf is the cumulative distribution function\n",
    "# Display P-value for LM Test\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Test\": [\"LM Test\"],\n",
    "        \"p-value\": [f\"{pval:.4f}\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T23:52:14.112791Z",
     "iopub.status.busy": "2025-10-19T23:52:14.112701Z",
     "iopub.status.idle": "2025-10-19T23:52:14.121155Z",
     "shell.execute_reply": "2025-10-19T23:52:14.120878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statistic</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F-statistic</td>\n",
       "      <td>2.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p-value</td>\n",
       "      <td>0.1310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Statistic   Value\n",
       "0  F-statistic   2.034\n",
       "1      p-value  0.1310"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Compare the LM test to the F-test for the same hypothesis using the unrestricted model directly.\n",
    "reg = smf.ols(\n",
    "    formula=\"narr86 ~ pcnv + avgsen + tottime + ptime86 + qemp86\",\n",
    "    data=crime1,\n",
    ")\n",
    "results = reg.fit()\n",
    "# Define the hypotheses to be tested: beta_avgsen = 0 and beta_tottime = 0\n",
    "hypotheses = [\"avgsen = 0\", \"tottime = 0\"]\n",
    "# Perform the F-test\n",
    "ftest = results.f_test(hypotheses)\n",
    "fstat = ftest.statistic\n",
    "fpval = ftest.pvalue\n",
    "# Display F-test results\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Statistic\": [\"F-statistic\", \"p-value\"],\n",
    "        \"Value\": [f\"{fstat:.3f}\", f\"{fpval:.4f}\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87970fe3",
   "metadata": {},
   "source": [
    "**Interpretation of Example 5.3:**\n",
    "\n",
    "- **LM Test Statistic:** The calculated LM test statistic is 4.071.\n",
    "- **Critical Value:** The critical value from the $\\chi^2_2$ distribution at the 10% significance level is 4.605.\n",
    "- **P-value:** The p-value for the LM test is 0.1306.\n",
    "\n",
    "Since the LM test statistic (4.071) is less than the critical value (4.605), or equivalently, the p-value (0.1306) is greater than the significance level (0.10), we fail to reject the null hypothesis $H_0: \\beta_2 = 0 \\text{ and } \\beta_3 = 0$.  This suggests that `avgsen` and `tottime` are jointly statistically insignificant in explaining `narr86`, given the other variables in the model.\n",
    "\n",
    "- **Comparison with F-test:** The F-test directly tests the same hypothesis using the unrestricted model. The F-statistic is 2.034 and the p-value is 0.1310.\n",
    "\n",
    "The results from the LM test and the F-test are very similar in this case, leading to the same conclusion: we fail to reject the null hypothesis.  In linear regression models, under homoskedasticity, the LM test, Wald test, and F-test are asymptotically equivalent for testing linear restrictions on the coefficients. In practice, especially with reasonably large samples, these tests often provide similar conclusions. The LM test is advantageous when estimating the unrestricted model is more complex or computationally intensive, as it only requires estimating the restricted model.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "This notebook has explored the asymptotic properties of OLS estimators through simulations and demonstrated the application of the Lagrange Multiplier (LM) test. The simulations visually confirmed the asymptotic normality of OLS estimators even under non-normal errors and when regressors are random. The LM test example provided a practical application of hypothesis testing within the framework of linear regression, showing its usefulness as an alternative to the F-test, particularly in situations where estimating only the restricted model is beneficial. Understanding these asymptotic properties and testing procedures is crucial for conducting sound econometric analysis and making valid inferences from regression models, especially when dealing with large datasets where asymptotic theory becomes increasingly relevant."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "notebooks//ipynb,markdown//md,scripts//py"
  },
  "kernelspec": {
   "display_name": "merino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
