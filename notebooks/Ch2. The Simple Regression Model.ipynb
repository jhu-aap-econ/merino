{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Simple Regression Model\n",
    "\n",
    ":::{important} Learning Objectives\n",
    ":class: dropdown\n",
    "By the end of this chapter, you should be able to:\n",
    "\n",
    "**2.1** List the assumptions of the simple linear regression model and interpret the parameters in the model.\n",
    "\n",
    "**2.2** Apply the formulas for the slope and intercept estimates using ordinary least squares estimation.\n",
    "\n",
    "**2.3** Use the formulas to calculate the fitted values and residuals, and list the properties of these statistics on any sample of data.\n",
    "\n",
    "**2.4** Compute and interpret the R-squared of a simple regression.\n",
    "\n",
    "**2.5** List and interpret the assumptions used to obtain unbiasedness of the ordinary least squares (OLS) estimator.\n",
    "\n",
    "**2.6** Explain the meaning of the homoskedasticity (constant variance) assumption, and discuss its role in calculating the sampling variance of the OLS estimators.\n",
    "\n",
    "**2.7** Discuss the limitations of the simple regression model for uncovering causal effects.\n",
    "\n",
    "**2.8** Interpret the simple regression estimates in the case of a binary (dummy) explanatory variable.\n",
    "\n",
    "**2.9** Describe the potential outcomes framework for causal inference and explain how experimental interventions (randomized controlled trials) can be analyzed using simple regression.\n",
    ":::\n",
    "\n",
    "The Simple Linear Regression model provides a cornerstone for econometric and statistical analysis, establishing foundational methods for understanding relationships between two variables. This chapter explores the mechanics of Ordinary Least Squares (OLS) regression, develops intuition for interpreting results, and examines the crucial assumptions that underpin validity of inference.\n",
    "\n",
    "The presentation follows a hierarchical development from basic concepts to advanced applications. We demonstrate theoretical results using real-world datasets from the Wooldridge package, illustrating how abstract econometric principles manifest in practical examples. The chapter proceeds through derivation of OLS estimators (Section 2.1-2.2), analysis of their statistical properties (Section 2.3-2.4), and examination of units of measurement and functional forms (Section 2.5). We conclude with expected values, variance decomposition, and an introduction to causal inference through randomized experiments (Section 2.6-2.9).\n",
    "\n",
    "Throughout this chapter, we implement concepts using Python's scientific computing stack, building intuition through numerical examples and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:16:42.275880Z",
     "iopub.status.busy": "2025-10-20T00:16:42.275802Z",
     "iopub.status.idle": "2025-10-20T00:16:43.688743Z",
     "shell.execute_reply": "2025-10-20T00:16:43.688467Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import wooldridge as wool\n",
    "from IPython.display import display\n",
    "from scipy import stats\n",
    "\n",
    "# Set plotting style for enhanced visualizations\n",
    "sns.set_style(\"whitegrid\")  # Clean seaborn style with grid\n",
    "sns.set_palette(\"husl\")  # Attractive color palette\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 6]  # Default figure size\n",
    "plt.rcParams[\"font.size\"] = 11  # Slightly larger font size\n",
    "plt.rcParams[\"axes.titlesize\"] = 14  # Larger title font\n",
    "plt.rcParams[\"axes.labelsize\"] = 12  # Larger axis label font"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23794f42",
   "metadata": {},
   "source": [
    "## 2.1 Simple OLS Regression\n",
    "\n",
    "The Simple Linear Regression model aims to explain the variation in a dependent variable, $y$, using a single independent variable, $x$. It assumes a linear relationship between $x$ and the expected value of $y$. The model is mathematically represented as:\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 x + u $$\n",
    "\n",
    "Let's break down each component:\n",
    "\n",
    "- **$y$ (Dependent Variable)**: This is the variable we are trying to explain or predict. It's often called the explained variable, regressand, or outcome variable.\n",
    "- **$x$ (Independent Variable)**: This variable is used to explain the variations in $y$. It's also known as the explanatory variable, regressor, or control variable.\n",
    "- **$\\beta_0$ (Intercept)**: This is the value of $y$ when $x$ is zero. It's the point where the regression line crosses the y-axis.\n",
    "- **$\\beta_1$ (Slope Coefficient)**: This represents the change in $y$ for a one-unit increase in $x$. It quantifies the effect of $x$ on $y$.\n",
    "- **$u$ (Error Term)**: Also known as the disturbance term, it represents all other factors, besides $x$, that affect $y$. It captures the unexplained variation in $y$. We assume that the error term has an expected value of zero conditional on $x$: $E(u|x) = 0$, which implies $E(u) = 0$ and that $u$ is uncorrelated with $x$.\n",
    "\n",
    "Our goal in OLS regression is to estimate the unknown parameters $\\beta_0$ and $\\beta_1$. The Ordinary Least Squares (OLS) method achieves this by minimizing the sum of the squared residuals. The OLS estimators for $\\beta_0$ and $\\beta_1$ are given by the following formulas:\n",
    "\n",
    "$$\\hat{\\beta}_1 = \\frac{\\widehat{\\text{Cov}}(x,y)}{\\widehat{\\text{Var}}(x)} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}$$\n",
    "\n",
    "This formula shows that $\\hat{\\beta}_1$ is the ratio of the **sample covariance** between $x$ and $y$ to the **sample variance** of $x$. It essentially captures the linear association between $x$ and $y$. The hat notation ($\\widehat{\\text{Cov}}$ and $\\widehat{\\text{Var}}$) emphasizes these are sample estimates of the population covariance and variance.\n",
    "\n",
    "$$\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}$$\n",
    "\n",
    "Once we have $\\hat{\\beta}_1$, we can easily calculate $\\hat{\\beta}_0$ using this formula. It ensures that the regression line passes through the sample mean point $(\\bar{x}, \\bar{y})$.\n",
    "\n",
    "After estimating $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, we can compute the fitted values, which are the predicted values of $y$ for each observation based on our regression model:\n",
    "\n",
    "$$\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x$$\n",
    "\n",
    "These fitted values represent the points on the regression line.\n",
    "\n",
    "Let's put these formulas into practice with some real-world examples.\n",
    "\n",
    "### Example 2.3: CEO Salary and Return on Equity\n",
    "\n",
    "In this example, we investigate the relationship between CEO salaries and the return on equity (ROE) of their firms. We want to see if firms with better performance (higher ROE) tend to pay their CEOs more. Our simple regression model is:\n",
    "\n",
    "$$ \\text{salary} = \\beta_0 + \\beta_1 \\text{roe} + u $$\n",
    "\n",
    "Here, `salary` is the dependent variable (CEO's annual salary in thousands of dollars), and `roe` is the independent variable (return on equity, in percentage). We hypothesize that $\\beta_1 > 0$, meaning that a higher ROE is associated with a higher CEO salary. Let's calculate the OLS coefficients manually first to understand the underlying computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:16:43.690527Z",
     "iopub.status.busy": "2025-10-20T00:16:43.690374Z",
     "iopub.status.idle": "2025-10-20T00:16:43.702641Z",
     "shell.execute_reply": "2025-10-20T00:16:43.702375Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Calculation</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sample covariance</td>\n",
       "      <td>1342.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sample variance of ROE</td>\n",
       "      <td>72.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Slope calculation</td>\n",
       "      <td>1342.54 / 72.56 = 18.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Calculation                    Value\n",
       "0       Sample covariance                  1342.54\n",
       "1  Sample variance of ROE                    72.56\n",
       "2       Slope calculation  1342.54 / 72.56 = 18.50"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Formatted</th>\n",
       "      <th>Interpretation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept ($\\hat{\\beta}_0$)</td>\n",
       "      <td>963.19</td>\n",
       "      <td>Expected salary when ROE=0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Slope ($\\hat{\\beta}_1$)</td>\n",
       "      <td>18.50</td>\n",
       "      <td>Salary increase per 1% ROE increase</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Parameter Formatted                       Interpretation\n",
       "0  Intercept ($\\hat{\\beta}_0$)    963.19           Expected salary when ROE=0\n",
       "1      Slope ($\\hat{\\beta}_1$)     18.50  Salary increase per 1% ROE increase"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "ceosal1 = wool.data(\"ceosal1\")  # Load the ceosal1 dataset from wooldridge package\n",
    "roe_values = ceosal1[\n",
    "    \"roe\"\n",
    "]  # Extract 'roe' (return on equity, %) as independent variable\n",
    "salary_values = ceosal1[\"salary\"]  # Extract 'salary' (in $1000s) as dependent variable\n",
    "\n",
    "# Calculate OLS coefficients manually using the formula:\n",
    "# beta_hat_1 = Cov(x,y) / Var(x) and beta_hat_0 = y_bar - beta_hat_1x_bar\n",
    "\n",
    "# Step 1: Calculate sample statistics\n",
    "covariance_roe_salary = np.cov(roe_values, salary_values)[1, 0]  # Sample covariance\n",
    "variance_roe = np.var(roe_values, ddof=1)  # Sample variance (n-1 denominator)\n",
    "mean_roe = np.mean(roe_values)  # Sample mean of ROE\n",
    "mean_salary = np.mean(salary_values)  # Sample mean of salary\n",
    "\n",
    "# Step 2: Apply OLS formulas\n",
    "slope_estimate = covariance_roe_salary / variance_roe  # beta_hat_1 = Cov(x,y)/Var(x)\n",
    "intercept_estimate = mean_salary - slope_estimate * mean_roe  # beta_hat_0 = y_bar - beta_hat_1x_bar\n",
    "\n",
    "# Display results with clear formatting\n",
    "manual_results = pd.DataFrame(\n",
    "    {\n",
    "        \"Parameter\": [\"Intercept ($\\\\hat{\\\\beta}_0$)\", \"Slope ($\\\\hat{\\\\beta}_1$)\"],\n",
    "        \"Estimate\": [intercept_estimate, slope_estimate],\n",
    "        \"Formatted\": [f\"{intercept_estimate:.2f}\", f\"{slope_estimate:.2f}\"],\n",
    "        \"Interpretation\": [\n",
    "            \"Expected salary when ROE=0\",\n",
    "            \"Salary increase per 1% ROE increase\",\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "# Display calculation details\n",
    "calc_details = pd.DataFrame(\n",
    "    {\n",
    "        \"Calculation\": [\n",
    "            \"Sample covariance\",\n",
    "            \"Sample variance of ROE\",\n",
    "            \"Slope calculation\",\n",
    "        ],\n",
    "        \"Value\": [\n",
    "            f\"{covariance_roe_salary:.2f}\",\n",
    "            f\"{variance_roe:.2f}\",\n",
    "            f\"{covariance_roe_salary:.2f} / {variance_roe:.2f} = {slope_estimate:.2f}\",\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "\n",
    "display(calc_details)\n",
    "manual_results[[\"Parameter\", \"Formatted\", \"Interpretation\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code first loads the `ceosal1` dataset and extracts the 'roe' and 'salary' columns as our $x$ and $y$ variables, respectively. Then, it calculates the covariance between `roe` and `salary`, the variance of `roe`, and the means of both variables. Finally, it applies the formulas to compute $\\hat{\\beta}_1$ and $\\hat{\\beta}_0$ and displays them.\n",
    "\n",
    "Now, let's use the `statsmodels` library, which provides a more convenient and comprehensive way to perform OLS regression. This will also serve as a verification of our manual calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:16:43.703986Z",
     "iopub.status.busy": "2025-10-20T00:16:43.703892Z",
     "iopub.status.idle": "2025-10-20T00:16:43.714161Z",
     "shell.execute_reply": "2025-10-20T00:16:43.713887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R-squared</td>\n",
       "      <td>0.0132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Number of observations</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Metric   Value\n",
       "0               R-squared  0.0132\n",
       "1  Number of observations     209"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Estimate</th>\n",
       "      <th>Std Error</th>\n",
       "      <th>t-statistic</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept ($\\hat{\\beta}_0$)</td>\n",
       "      <td>963.1913</td>\n",
       "      <td>213.2403</td>\n",
       "      <td>4.5169</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Slope ($\\hat{\\beta}_1$)</td>\n",
       "      <td>18.5012</td>\n",
       "      <td>11.1233</td>\n",
       "      <td>1.6633</td>\n",
       "      <td>0.0978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Parameter  Estimate  Std Error  t-statistic  p-value\n",
       "0  Intercept ($\\hat{\\beta}_0$)  963.1913   213.2403       4.5169   0.0000\n",
       "1      Slope ($\\hat{\\beta}_1$)   18.5012    11.1233       1.6633   0.0978"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit regression model using statsmodels for comparison and validation\n",
    "regression_model = smf.ols(\n",
    "    formula=\"salary ~ roe\",  # y ~ x notation: salary depends on roe\n",
    "    data=ceosal1,\n",
    ")\n",
    "fitted_results = regression_model.fit()  # Estimate parameters via OLS\n",
    "coefficient_estimates = fitted_results.params  # Extract beta_hat estimates\n",
    "\n",
    "# Display statsmodels results with additional statistics\n",
    "statsmodels_results = pd.DataFrame(\n",
    "    {\n",
    "        \"Parameter\": [\"Intercept ($\\\\hat{\\\\beta}_0$)\", \"Slope ($\\\\hat{\\\\beta}_1$)\"],\n",
    "        \"Estimate\": [coefficient_estimates.iloc[0], coefficient_estimates.iloc[1]],\n",
    "        \"Std Error\": [fitted_results.bse.iloc[0], fitted_results.bse.iloc[1]],\n",
    "        \"t-statistic\": [fitted_results.tvalues.iloc[0], fitted_results.tvalues.iloc[1]],\n",
    "        \"p-value\": [fitted_results.pvalues.iloc[0], fitted_results.pvalues.iloc[1]],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Verify manual calculations match statsmodels\n",
    "model_stats = pd.DataFrame(\n",
    "    {\n",
    "        \"Metric\": [\"R-squared\", \"Number of observations\"],\n",
    "        \"Value\": [f\"{fitted_results.rsquared:.4f}\", f\"{fitted_results.nobs:.0f}\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "display(model_stats)\n",
    "statsmodels_results.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet uses `statsmodels.formula.api` to define and fit the same regression model. The `smf.ols` function takes a formula string (`\"salary ~ roe\"`) specifying the model and the dataframe (`ceosal1`) as input. `results.fit()` performs the OLS estimation, and `results.params` extracts the estimated coefficients. We can see that the coefficients obtained from `statsmodels` match our manual calculations, which is reassuring.\n",
    "\n",
    "To better visualize the regression results and the relationship between CEO salary and ROE, let's create an enhanced regression plot. We'll define a reusable function for this purpose, which includes the regression line, scatter plot of the data, confidence intervals, and annotations for the regression equation and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:16:43.715552Z",
     "iopub.status.busy": "2025-10-20T00:16:43.715463Z",
     "iopub.status.idle": "2025-10-20T00:16:43.719220Z",
     "shell.execute_reply": "2025-10-20T00:16:43.718893Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_regression(\n",
    "    x: str,\n",
    "    y: str,\n",
    "    data: pd.DataFrame,\n",
    "    results,\n",
    "    title: str,\n",
    "    add_ci: bool = True,\n",
    "):\n",
    "    \"\"\"Create an enhanced regression plot with confidence intervals and statistics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : str\n",
    "        Column name of independent variable in data\n",
    "    y : str\n",
    "        Column name of dependent variable in data\n",
    "    data : pandas.DataFrame\n",
    "        Dataset containing both variables\n",
    "    results : statsmodels.regression.linear_model.RegressionResults\n",
    "        Fitted OLS regression results object\n",
    "    title : str\n",
    "        Main title for the plot\n",
    "    add_ci : bool, default=True\n",
    "        Whether to display 95% confidence interval band\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The created figure object\n",
    "\n",
    "    \"\"\"\n",
    "    # Create figure with professional styling\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Plot data points and regression line with confidence band\n",
    "    sns.regplot(\n",
    "        data=data,\n",
    "        x=x,\n",
    "        y=y,\n",
    "        ci=95 if add_ci else None,  # 95% confidence interval for mean prediction\n",
    "        ax=ax,\n",
    "        scatter_kws={\"alpha\": 0.6, \"edgecolor\": \"white\", \"linewidths\": 0.5},\n",
    "        line_kws={\"linewidth\": 2},\n",
    "    )\n",
    "\n",
    "    # Construct regression equation and statistics text\n",
    "    intercept = results.params.iloc[0]\n",
    "    slope = results.params.iloc[1]\n",
    "    equation = f\"$\\\\hat{{y}}$ = {intercept:.2f} + {slope:.2f}x\"\n",
    "    r_squared = f\"$R^2$ = {results.rsquared:.3f}\"\n",
    "    n_obs = f\"n = {int(results.nobs)}\"\n",
    "\n",
    "    # Add formatted text box with regression statistics\n",
    "    textstr = f\"{equation}\\n{r_squared}\\n{n_obs}\"\n",
    "    ax.text(\n",
    "        0.05,\n",
    "        0.95,\n",
    "        textstr,\n",
    "        transform=ax.transAxes,\n",
    "        verticalalignment=\"top\",\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8),\n",
    "    )\n",
    "\n",
    "    # Clean title and labels\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(x.replace(\"_\", \" \").title())\n",
    "    ax.set_ylabel(y.replace(\"_\", \" \").title())\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b89879",
   "metadata": {},
   "source": [
    "This function, `plot_regression`, takes the variable names, data, regression results, and plot title as input. It generates a scatter plot of the data points, plots the regression line, and optionally adds 95% confidence intervals around the regression line. It also annotates the plot with the regression equation and the R-squared value. This function makes it easy to visualize and interpret simple regression results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:16:43.720465Z",
     "iopub.status.busy": "2025-10-20T00:16:43.720387Z",
     "iopub.status.idle": "2025-10-20T00:16:43.845480Z",
     "shell.execute_reply": "2025-10-20T00:16:43.845176Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create enhanced regression plot for CEO Salary vs ROE\n",
    "plot_regression(\n",
    "    \"roe\",\n",
    "    \"salary\",\n",
    "    ceosal1,\n",
    "    fitted_results,\n",
    "    \"CEO Salary vs Return on Equity\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9315eb",
   "metadata": {},
   "source": [
    "Running this code will generate a scatter plot with 'roe' on the x-axis and 'salary' on the y-axis, along with the OLS regression line and its 95% confidence interval. The plot also displays the estimated regression equation and the R-squared value.\n",
    "\n",
    ":::{note} Interpretation of Example 2.3\n",
    ":class: dropdown\n",
    "\n",
    "Looking at the output and the plot, we can interpret the results. The estimated regression equation (visible on the plot) will be something like:\n",
    "\n",
    "$$ \\widehat{\\text{salary}} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\text{roe} $$\n",
    "\n",
    "We find $\\hat{\\beta}_1 = 18.50$. This means that, on average, for every one percentage point increase in ROE, CEO salary is predicted to increase by approximately \\$18,500 (since salary is in thousands of dollars). The intercept, $\\hat{\\beta}_0 = 963.19$, represents the predicted salary when ROE is zero.  The R-squared value (also on the plot) is 0.013, indicating that only about 1.3% of the variation in CEO salaries is explained by ROE in this simple linear model. This suggests that ROE alone is not a strong predictor of CEO salary, and other factors are likely more important.\n",
    ":::\n",
    "\n",
    "### Example 2.4: Wage and Education\n",
    "\n",
    "Let's consider another example, examining the relationship between hourly wages and years of education. We use the `wage1` dataset and the following model:\n",
    "\n",
    "$$\\text{wage} = \\beta_0 + \\beta_1 \\text{educ} + u$$\n",
    "\n",
    "Here, `wage` is the hourly wage (in dollars), and `educ` is years of education. We expect a positive relationship, i.e., $\\beta_1 > 0$, as more education is generally believed to lead to higher wages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68a273ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:16:43.846911Z",
     "iopub.status.busy": "2025-10-20T00:16:43.846823Z",
     "iopub.status.idle": "2025-10-20T00:16:43.855853Z",
     "shell.execute_reply": "2025-10-20T00:16:43.855614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Formatted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept ($\\beta_0$)</td>\n",
       "      <td>-0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Education coefficient ($\\beta_1$)</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R-squared</td>\n",
       "      <td>0.165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Parameter Formatted\n",
       "0              Intercept ($\\beta_0$)     -0.90\n",
       "1  Education coefficient ($\\beta_1$)      0.54\n",
       "2                          R-squared     0.165"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and analyze wage data\n",
    "wage1 = wool.data(\"wage1\")  # Load the wage1 dataset\n",
    "\n",
    "# Fit regression model\n",
    "reg = smf.ols(formula=\"wage ~ educ\", data=wage1)  # Define and fit the OLS model\n",
    "results = reg.fit()  # Fit the model\n",
    "\n",
    "# Display regression results using DataFrame\n",
    "wage_results = pd.DataFrame(\n",
    "    {\n",
    "        \"Parameter\": [\n",
    "            \"Intercept ($\\\\beta_0$)\",\n",
    "            \"Education coefficient ($\\\\beta_1$)\",\n",
    "            \"R-squared\",\n",
    "        ],\n",
    "        \"Value\": [results.params.iloc[0], results.params.iloc[1], results.rsquared],\n",
    "        \"Formatted\": [\n",
    "            f\"{results.params.iloc[0]:.2f}\",\n",
    "            f\"{results.params.iloc[1]:.2f}\",\n",
    "            f\"{results.rsquared:.3f}\",\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "wage_results[[\"Parameter\", \"Formatted\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c06a58",
   "metadata": {},
   "source": [
    "This code loads the `wage1` dataset, fits the regression model of `wage` on `educ` using `statsmodels`, and then shows the estimated coefficients and R-squared.\n",
    "\n",
    ":::{note} Interpretation of Example 2.4\n",
    ":class: dropdown\n",
    "\n",
    "We find $\\hat{\\beta}_1 = 0.54$. This implies that, on average, each additional year of education is associated with an increase in hourly wage of approximately \\$0.54. The intercept, $\\hat{\\beta}_0 = -0.90$, represents the predicted wage for someone with zero years of education. The R-squared is 0.165, meaning that about 16.5% of the variation in hourly wages is explained by years of education in this simple model. Education appears to be a somewhat more important factor in explaining wages than ROE was for CEO salaries, but still, a large portion of wage variation remains unexplained by education alone.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:16:43.857030Z",
     "iopub.status.busy": "2025-10-20T00:16:43.856955Z",
     "iopub.status.idle": "2025-10-20T00:16:43.928986Z",
     "shell.execute_reply": "2025-10-20T00:16:43.928690Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "plot_regression(\n",
    "    \"educ\",\n",
    "    \"wage\",\n",
    "    wage1,\n",
    "    results,\n",
    "    \"Wage vs Years of Education\",\n",
    ")  # Generate regression plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression plot visualizes the relationship between years of education and hourly wage, showing the fitted regression line along with the data points and confidence interval.\n",
    "\n",
    "### Example 2.5: Voting Outcomes and Campaign Expenditures\n",
    "\n",
    "In this example, we explore the relationship between campaign spending and voting outcomes. We use the `vote1` dataset and the model:\n",
    "\n",
    "$$ \\text{voteA} = \\beta_0 + \\beta_1 \\text{shareA} + u $$\n",
    "\n",
    "Here, `voteA` is the percentage of votes received by candidate A, and `shareA` is the percentage of campaign spending by candidate A out of the total spending by both candidates. We expect that higher campaign spending share for candidate A will lead to a higher vote share, so we anticipate $\\beta_1 > 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d74355c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:16:43.930498Z",
     "iopub.status.busy": "2025-10-20T00:16:43.930401Z",
     "iopub.status.idle": "2025-10-20T00:16:43.941787Z",
     "shell.execute_reply": "2025-10-20T00:16:43.941531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Formatted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept ($\\beta_0$)</td>\n",
       "      <td>26.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Share coefficient ($\\beta_1$)</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R-squared</td>\n",
       "      <td>0.856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Parameter Formatted\n",
       "0          Intercept ($\\beta_0$)     26.81\n",
       "1  Share coefficient ($\\beta_1$)      0.46\n",
       "2                      R-squared     0.856"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and analyze voting data\n",
    "vote1 = wool.data(\"vote1\")  # Load the vote1 dataset\n",
    "\n",
    "# Fit regression model\n",
    "reg = smf.ols(formula=\"voteA ~ shareA\", data=vote1)  # Define and fit the OLS model\n",
    "results = reg.fit()  # Fit the model\n",
    "\n",
    "# Display regression results using DataFrame\n",
    "vote_results = pd.DataFrame(\n",
    "    {\n",
    "        \"Parameter\": [\n",
    "            \"Intercept ($\\\\beta_0$)\",\n",
    "            \"Share coefficient ($\\\\beta_1$)\",\n",
    "            \"R-squared\",\n",
    "        ],\n",
    "        \"Value\": [results.params.iloc[0], results.params.iloc[1], results.rsquared],\n",
    "        \"Formatted\": [\n",
    "            f\"{results.params.iloc[0]:.2f}\",\n",
    "            f\"{results.params.iloc[1]:.2f}\",\n",
    "            f\"{results.rsquared:.3f}\",\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "vote_results[[\"Parameter\", \"Formatted\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6226fec",
   "metadata": {},
   "source": [
    "This code loads the `vote1` dataset, fits the regression model using `statsmodels`, and shows the estimated coefficients and R-squared.\n",
    "\n",
    ":::{note} Interpretation of Example 2.5\n",
    ":class: dropdown\n",
    "\n",
    "We find $\\hat{\\beta}_1 = 0.46$. This suggests that for every one percentage point increase in candidate A's share of campaign spending, candidate A's vote share is predicted to increase by approximately 0.46 percentage points. The intercept, $\\hat{\\beta}_0 = 26.81$, represents the predicted vote share for candidate A if their campaign spending share is zero. The R-squared is 0.856, which is quite high! It indicates that about 85.6% of the variation in candidate A's vote share is explained by their share of campaign spending in this simple model. This suggests that campaign spending share is a very strong predictor of voting outcomes, at least in this dataset.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:16:43.943039Z",
     "iopub.status.busy": "2025-10-20T00:16:43.942954Z",
     "iopub.status.idle": "2025-10-20T00:16:43.998972Z",
     "shell.execute_reply": "2025-10-20T00:16:43.998685Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "plot_regression(\n",
    "    \"shareA\",\n",
    "    \"voteA\",\n",
    "    vote1,\n",
    "    results,\n",
    "    \"Vote Share vs Campaign Spending Share\",\n",
    ")  # Generate regression plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression plot demonstrates the strong linear relationship between campaign spending share and vote share, with most data points closely following the fitted line.\n",
    "\n",
    "## 2.2. Coefficients, Fitted Values, and Residuals\n",
    "\n",
    "As we discussed earlier, after estimating the OLS regression, we obtain fitted values ($\\hat{y}_i$) and residuals ($\\hat{u}_i$). Let's formally define them again:\n",
    "\n",
    "**Fitted Values**: These are the predicted values of $y$ for each observation $i$, calculated using the estimated regression equation:\n",
    "\n",
    "$$\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$$\n",
    "\n",
    "Fitted values lie on the OLS regression line. For each $x_i$, $\\hat{y}_i$ is the point on the line directly above or below $x_i$.\n",
    "\n",
    "**Residuals**: These are the differences between the actual values of $y_i$ and the fitted values $\\hat{y}_i$. They represent the unexplained part of $y_i$ for each observation:\n",
    "\n",
    "$$\\hat{u}_i = y_i - \\hat{y}_i$$\n",
    "\n",
    "Residuals are estimates of the unobservable error terms $u_i$. In OLS regression, we aim to minimize the sum of squared residuals.\n",
    "\n",
    "**Important Properties of OLS Residuals:**\n",
    "1. **Zero mean:** $\\bar{\\hat{u}} = \\frac{1}{n}\\sum_{i=1}^n \\hat{u}_i = 0$ (always true by construction)\n",
    "2. **Orthogonality:** $\\sum_{i=1}^n x_i \\hat{u}_i = 0$ (residuals are uncorrelated with regressors)\n",
    "3. **Regression line passes through mean:** The point $(\\bar{x}, \\bar{y})$ always lies on the fitted regression line\n",
    "\n",
    "### Example 2.6: CEO Salary and Return on Equity\n",
    "\n",
    "Let's go back to the CEO salary and ROE example and examine the fitted values and residuals. We will calculate these and present the first 15 observations in a table. We will also create a residual plot to visualize the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f5af6c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:16:44.000461Z",
     "iopub.status.busy": "2025-10-20T00:16:44.000373Z",
     "iopub.status.idle": "2025-10-20T00:16:44.008248Z",
     "shell.execute_reply": "2025-10-20T00:16:44.007984Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROE</th>\n",
       "      <th>Actual Salary</th>\n",
       "      <th>Predicted Salary</th>\n",
       "      <th>Residual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.10</td>\n",
       "      <td>1095</td>\n",
       "      <td>1224.06</td>\n",
       "      <td>-129.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.90</td>\n",
       "      <td>1001</td>\n",
       "      <td>1164.85</td>\n",
       "      <td>-163.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.50</td>\n",
       "      <td>1122</td>\n",
       "      <td>1397.97</td>\n",
       "      <td>-275.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.90</td>\n",
       "      <td>578</td>\n",
       "      <td>1072.35</td>\n",
       "      <td>-494.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.80</td>\n",
       "      <td>1368</td>\n",
       "      <td>1218.51</td>\n",
       "      <td>149.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20.00</td>\n",
       "      <td>1145</td>\n",
       "      <td>1333.22</td>\n",
       "      <td>-188.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16.40</td>\n",
       "      <td>1078</td>\n",
       "      <td>1266.61</td>\n",
       "      <td>-188.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16.30</td>\n",
       "      <td>1094</td>\n",
       "      <td>1264.76</td>\n",
       "      <td>-170.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.50</td>\n",
       "      <td>1237</td>\n",
       "      <td>1157.45</td>\n",
       "      <td>79.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>26.30</td>\n",
       "      <td>833</td>\n",
       "      <td>1449.77</td>\n",
       "      <td>-616.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25.90</td>\n",
       "      <td>567</td>\n",
       "      <td>1442.37</td>\n",
       "      <td>-875.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>26.80</td>\n",
       "      <td>933</td>\n",
       "      <td>1459.02</td>\n",
       "      <td>-526.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14.80</td>\n",
       "      <td>1339</td>\n",
       "      <td>1237.01</td>\n",
       "      <td>101.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>22.30</td>\n",
       "      <td>937</td>\n",
       "      <td>1375.77</td>\n",
       "      <td>-438.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>56.30</td>\n",
       "      <td>2011</td>\n",
       "      <td>2004.81</td>\n",
       "      <td>6.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ROE  Actual Salary  Predicted Salary  Residual\n",
       "0  14.10           1095           1224.06   -129.06\n",
       "1  10.90           1001           1164.85   -163.85\n",
       "2  23.50           1122           1397.97   -275.97\n",
       "3   5.90            578           1072.35   -494.35\n",
       "4  13.80           1368           1218.51    149.49\n",
       "5  20.00           1145           1333.22   -188.22\n",
       "6  16.40           1078           1266.61   -188.61\n",
       "7  16.30           1094           1264.76   -170.76\n",
       "8  10.50           1237           1157.45     79.55\n",
       "9  26.30            833           1449.77   -616.77\n",
       "10 25.90            567           1442.37   -875.37\n",
       "11 26.80            933           1459.02   -526.02\n",
       "12 14.80           1339           1237.01    101.99\n",
       "13 22.30            937           1375.77   -438.77\n",
       "14 56.30           2011           2004.81      6.19"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare regression results - Re-run the regression for ceosal1 dataset\n",
    "ceosal1 = wool.data(\"ceosal1\")  # Load data again (if needed)\n",
    "reg = smf.ols(formula=\"salary ~ roe\", data=ceosal1)  # Define the regression model\n",
    "results = reg.fit()  # Fit the model\n",
    "\n",
    "# Calculate fitted values and residuals\n",
    "salary_hat = results.fittedvalues  # Get fitted values from results object\n",
    "u_hat = results.resid  # Get residuals from results object\n",
    "\n",
    "# Create summary table\n",
    "table = pd.DataFrame(  # Create a Pandas DataFrame\n",
    "    {\n",
    "        \"ROE\": ceosal1[\"roe\"],  # Include ROE values\n",
    "        \"Actual Salary\": ceosal1[\"salary\"],  # Include actual salary values\n",
    "        \"Predicted Salary\": salary_hat,  # Include fitted salary values\n",
    "        \"Residual\": u_hat,  # Include residual values\n",
    "    },\n",
    ")\n",
    "\n",
    "# Format and display the first 15 rows\n",
    "pd.set_option(\n",
    "    \"display.float_format\",\n",
    "    lambda x: \"%.2f\" % x,\n",
    ")  # Set float format for display\n",
    "table.head(15)  # Display the first 15 rows of the table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3a4c5d",
   "metadata": {},
   "source": [
    "This code calculates the fitted values and residuals, then creates and displays a summary table showing ROE, actual salary, predicted salary, and residuals for the first 15 observations.\n",
    "\n",
    ":::{note} Interpretation of Example 2.6\n",
    ":class: dropdown\n",
    "\n",
    "By examining the table, you can see for each company the actual CEO salary, the salary predicted by the regression model based on ROE, and the residual, which is the difference between the actual and predicted salary. A positive residual means the actual salary is higher than predicted, and a negative residual means it's lower.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:16:44.009495Z",
     "iopub.status.busy": "2025-10-20T00:16:44.009419Z",
     "iopub.status.idle": "2025-10-20T00:16:44.036912Z",
     "shell.execute_reply": "2025-10-20T00:16:44.036597Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create residual plot with seaborn defaults\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Simple, clean scatter plot\n",
    "sns.scatterplot(x=salary_hat, y=u_hat, ax=ax)\n",
    "\n",
    "# Add reference line\n",
    "ax.axhline(y=0, linestyle=\"--\", label=\"Zero Line\")\n",
    "\n",
    "# Clean titles and labels\n",
    "ax.set_title(\"Residual Plot\")\n",
    "ax.set_xlabel(\"Fitted Values\")\n",
    "ax.set_ylabel(\"Residuals\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0554da91",
   "metadata": {},
   "source": [
    "The residual plot is useful for checking assumptions of OLS regression, particularly homoscedasticity (constant variance of errors). Ideally, residuals should be randomly scattered around zero with no systematic pattern. Look for patterns where the spread of residuals changes with fitted values, which could indicate heteroscedasticity.\n",
    "\n",
    "### Example 2.7: Wage and Education\n",
    "\n",
    "Let's verify some important properties of OLS residuals using the wage and education example. These properties are mathematical consequences of the OLS minimization process:\n",
    "\n",
    "1. **The sum of residuals is zero**: $\\sum_{i=1}^n \\hat{u}_i = 0$. This implies that the mean of residuals is also zero: $\\frac{1}{n}\\sum_{i=1}^n \\hat{u}_i = 0$.\n",
    "2. **The sample covariance between regressors and residuals is zero**: $\\sum_{i=1}^n x_i \\hat{u}_i = 0$. This means that the residuals are uncorrelated with the independent variable $x$.\n",
    "3. **The point $(\\bar{x}, \\bar{y})$ lies on the regression line**: This is ensured by the formula for $\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}$.\n",
    "\n",
    "Let's check these properties using the `wage1` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:16:44.038231Z",
     "iopub.status.busy": "2025-10-20T00:16:44.038142Z",
     "iopub.status.idle": "2025-10-20T00:16:44.047605Z",
     "shell.execute_reply": "2025-10-20T00:16:44.047357Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Property</th>\n",
       "      <th>Formatted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mean of residuals</td>\n",
       "      <td>-0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Covariance between education and residuals</td>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mean wage</td>\n",
       "      <td>5.896103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Predicted wage at mean education</td>\n",
       "      <td>5.896103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Property      Formatted\n",
       "0                           Mean of residuals  -0.0000000000\n",
       "1  Covariance between education and residuals   0.0000000000\n",
       "2                                   Mean wage       5.896103\n",
       "3            Predicted wage at mean education       5.896103"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and prepare data - Re-run the regression for wage1 dataset\n",
    "wage1 = wool.data(\"wage1\")  # Load wage1 data\n",
    "reg = smf.ols(formula=\"wage ~ educ\", data=wage1)  # Define regression model\n",
    "results = reg.fit()  # Fit the model\n",
    "\n",
    "# Get coefficients, fitted values and residuals\n",
    "b = results.params  # Extract coefficients\n",
    "wage_hat = results.fittedvalues  # Extract fitted values\n",
    "u_hat = results.resid  # Extract residuals\n",
    "\n",
    "# Property 1: Mean of residuals should be zero\n",
    "u_hat_mean = np.mean(u_hat)  # Calculate the mean of residuals\n",
    "\n",
    "# Property 2: Covariance between education and residuals should be zero\n",
    "educ_u_cov = np.cov(wage1[\"educ\"], u_hat)[\n",
    "    1,\n",
    "    0,\n",
    "]  # Calculate covariance between educ and residuals\n",
    "\n",
    "# Property 3: Point (x_mean, y_mean) lies on regression line\n",
    "educ_mean = np.mean(wage1[\"educ\"])  # Calculate mean of education\n",
    "wage_mean = np.mean(wage1[\"wage\"])  # Calculate mean of wage\n",
    "wage_pred = (\n",
    "    b.iloc[0] + b.iloc[1] * educ_mean\n",
    ")  # Predict wage at mean education using regression line\n",
    "\n",
    "# Display regression properties\n",
    "properties_data = pd.DataFrame(\n",
    "    {\n",
    "        \"Property\": [\n",
    "            \"Mean of residuals\",\n",
    "            \"Covariance between education and residuals\",\n",
    "            \"Mean wage\",\n",
    "            \"Predicted wage at mean education\",\n",
    "        ],\n",
    "        \"Value\": [u_hat_mean, educ_u_cov, wage_mean, wage_pred],\n",
    "        \"Formatted\": [\n",
    "            f\"{u_hat_mean:.10f}\",\n",
    "            f\"{educ_u_cov:.10f}\",\n",
    "            f\"{wage_mean:.6f}\",\n",
    "            f\"{wage_pred:.6f}\",\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "properties_data[[\"Property\", \"Formatted\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code calculates the mean of the residuals, the covariance between education and residuals, and verifies that the predicted wage at the mean level of education is equal to the mean wage.\n",
    "\n",
    ":::{note} Interpretation of Example 2.7\n",
    ":class: dropdown\n",
    "\n",
    "The output should show that the mean of residuals is very close to zero (practically zero, given potential floating-point inaccuracies). Similarly, the covariance between education and residuals should be very close to zero. Finally, the predicted wage at the average level of education should be very close to the average wage. These results confirm the mathematical properties of OLS residuals. These properties are not assumptions, but rather outcomes of the OLS estimation procedure.\n",
    ":::\n",
    "\n",
    "## 2.3. Goodness of Fit\n",
    "\n",
    "After fitting a regression model, it's important to assess how well the model fits the data. A key measure of goodness of fit in simple linear regression is the R-squared ($R^2$) statistic. R-squared measures the proportion of the total variation in the dependent variable ($y$) that is explained by the independent variable ($x$) in our model.\n",
    "\n",
    "To understand R-squared, we first need to define three sums of squares:\n",
    "\n",
    "**Total Sum of Squares (SST)**: This measures the total sample variation in $y$. It is the sum of squared deviations of $y_i$ from its mean $\\bar{y}$:\n",
    "\n",
    "$$\\text{SST} = \\sum_{i=1}^n (y_i - \\bar{y})^2 = (n-1) \\widehat{\\text{Var}}(y)$$\n",
    "\n",
    "where $\\widehat{\\text{Var}}(y) = \\frac{1}{n-1}\\sum_{i=1}^n (y_i - \\bar{y})^2$ is the sample variance of $y$. SST represents the total variability in the dependent variable that we want to explain.\n",
    "\n",
    "**Explained Sum of Squares (SSE)**: This measures the variation in $\\hat{y}$ predicted by our model. It is the sum of squared deviations of the fitted values $\\hat{y}_i$ from the mean of $y$, $\\bar{y}$:\n",
    "\n",
    "$$\\text{SSE} = \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2 = (n-1) \\widehat{\\text{Var}}(\\hat{y})$$\n",
    "\n",
    "where $\\widehat{\\text{Var}}(\\hat{y}) = \\frac{1}{n-1}\\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2$ is the sample variance of the fitted values. SSE represents the variability in $y$ that is explained by our model.\n",
    "\n",
    "**Residual Sum of Squares (SSR)**: This measures the variation in the residuals $\\hat{u}_i$, which is the unexplained variation in $y$. It is the sum of squared residuals:\n",
    "\n",
    "$$\\text{SSR} = \\sum_{i=1}^n \\hat{u}_i^2 = (n-1) \\widehat{\\text{Var}}(\\hat{u})$$\n",
    "\n",
    "where $\\widehat{\\text{Var}}(\\hat{u}) = \\frac{1}{n-1}\\sum_{i=1}^n \\hat{u}_i^2$ is the sample variance of residuals. Note that the mean of OLS residuals is always zero ($\\bar{\\hat{u}} = 0$), so we are summing squared deviations from zero. SSR represents the variability in $y$ that is *not* explained by our model.\n",
    "\n",
    "These three sums of squares are related by the following identity:\n",
    "\n",
    "$$\\text{SST} = \\text{SSE} + \\text{SSR}$$\n",
    "\n",
    "This equation states that the total variation in $y$ can be decomposed into the variation explained by the model (SSE) and the unexplained variation (SSR).\n",
    "\n",
    "Now we can define R-squared:\n",
    "\n",
    "$$R^2 = \\frac{\\text{SSE}}{\\text{SST}} = 1 - \\frac{\\text{SSR}}{\\text{SST}} = \\frac{\\widehat{\\text{Var}}(\\hat{y})}{\\widehat{\\text{Var}}(y)} = 1 - \\frac{\\widehat{\\text{Var}}(\\hat{u})}{\\widehat{\\text{Var}}(y)}$$\n",
    "\n",
    "R-squared is the ratio of the explained variation to the total variation. It ranges from 0 to 1 (or 0% to 100%).\n",
    "\n",
    "- $R^2 = 0$ means the model explains none of the variation in $y$. In this case, SSE = 0 and SSR = SST.\n",
    "- $R^2 = 1$ means the model explains all of the variation in $y$. In this case, SSR = 0 and SSE = SST.\n",
    "\n",
    "A higher R-squared generally indicates a better fit, but it's important to remember that a high R-squared does not necessarily mean that the model is good or that there is a causal relationship. R-squared only measures the strength of the linear relationship and the proportion of variance explained.\n",
    "\n",
    "### Example 2.8: CEO Salary and Return on Equity\n",
    "\n",
    "Let's calculate and compare R-squared for the CEO salary and ROE example using different formulas. We will also create visualizations to understand the concept of goodness of fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "013b0dd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:16:44.048845Z",
     "iopub.status.busy": "2025-10-20T00:16:44.048758Z",
     "iopub.status.idle": "2025-10-20T00:16:44.056975Z",
     "shell.execute_reply": "2025-10-20T00:16:44.056735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Formatted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Using var($\\hat{y}$)/var(y)</td>\n",
       "      <td>0.0132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Using 1 - var($\\hat{u}$)/var(y)</td>\n",
       "      <td>0.0132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Using correlation coefficient</td>\n",
       "      <td>0.0132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Method Formatted\n",
       "0      Using var($\\hat{y}$)/var(y)    0.0132\n",
       "1  Using 1 - var($\\hat{u}$)/var(y)    0.0132\n",
       "2    Using correlation coefficient    0.0132"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and prepare data - Re-run regression for ceosal1\n",
    "ceosal1 = wool.data(\"ceosal1\")  # Load data\n",
    "reg = smf.ols(formula=\"salary ~ roe\", data=ceosal1)  # Define regression model\n",
    "results = reg.fit()  # Fit the model\n",
    "\n",
    "# Calculate predicted values & residuals\n",
    "sal_hat = results.fittedvalues  # Get fitted values\n",
    "u_hat = results.resid  # Get residuals\n",
    "sal = ceosal1[\"salary\"]  # Get actual salary values\n",
    "\n",
    "# Calculate $R^2$ in three different ways - Using different formulas for R-squared\n",
    "R2_a = np.var(sal_hat, ddof=1) / np.var(sal, ddof=1)  # $R^2$ = var(y_hat)/var(y)\n",
    "R2_b = 1 - np.var(u_hat, ddof=1) / np.var(sal, ddof=1)  # $R^2$ = 1 - var(u_hat)/var(y)\n",
    "R2_c = np.corrcoef(sal, sal_hat)[1, 0] ** 2  # $R^2$ = correlation(y, y_hat)^2\n",
    "\n",
    "# Display R-squared calculations\n",
    "r_squared_data = pd.DataFrame(\n",
    "    {\n",
    "        \"Method\": [\n",
    "            \"Using var($\\\\hat{y}$)/var(y)\",\n",
    "            \"Using 1 - var($\\\\hat{u}$)/var(y)\",\n",
    "            \"Using correlation coefficient\",\n",
    "        ],\n",
    "        \"Formula\": [\n",
    "            \"var($\\\\hat{y}$)/var(y)\",\n",
    "            \"1 - var($\\\\hat{u}$)/var(y)\",\n",
    "            \"corr(y, $\\\\hat{y}$)^2\",\n",
    "        ],\n",
    "        \"R-squared\": [R2_a, R2_b, R2_c],\n",
    "        \"Formatted\": [f\"{R2_a:.4f}\", f\"{R2_b:.4f}\", f\"{R2_c:.4f}\"],\n",
    "    },\n",
    ")\n",
    "r_squared_data[[\"Method\", \"Formatted\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9848d286",
   "metadata": {},
   "source": [
    "This code calculates R-squared using three different formulas to demonstrate their equivalence. All three methods should yield the same R-squared value (within rounding errors).\n",
    "\n",
    ":::{note} Interpretation of Example 2.8\n",
    ":class: dropdown\n",
    "\n",
    "The R-squared value calculated (around 0.013 in our example) will be the same regardless of which formula is used, confirming their equivalence. This low R-squared indicates that ROE explains very little of the variation in CEO salaries.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:16:44.058207Z",
     "iopub.status.busy": "2025-10-20T00:16:44.058121Z",
     "iopub.status.idle": "2025-10-20T00:16:44.109600Z",
     "shell.execute_reply": "2025-10-20T00:16:44.109303Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create model fit visualization with seaborn defaults\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Actual vs Predicted plot\n",
    "sns.scatterplot(x=sal, y=sal_hat, ax=axes[0])\n",
    "axes[0].plot([sal.min(), sal.max()], [sal.min(), sal.max()], \"--\", label=\"Perfect Fit\")\n",
    "axes[0].set_title(\"Actual vs Predicted Salary\")\n",
    "axes[0].set_xlabel(\"Actual Salary\")\n",
    "axes[0].set_ylabel(\"Predicted Salary\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Residuals vs Fitted plot\n",
    "sns.scatterplot(x=sal_hat, y=u_hat, ax=axes[1])\n",
    "axes[1].axhline(y=0, linestyle=\"--\", label=\"Zero Line\")\n",
    "axes[1].set_title(\"Residuals vs Fitted Values\")\n",
    "axes[1].set_xlabel(\"Fitted Values\")\n",
    "axes[1].set_ylabel(\"Residuals\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two plots provide visual assessments of model fit:\n",
    "\n",
    "1. **Actual vs Predicted Plot**: Shows how well predicted salaries align with actual salaries. If the model fit were perfect ($R^2$ = 1), all points would lie on the 45-degree dashed red line.\n",
    "2. **Residuals vs Fitted Values Plot**: Helps assess homoscedasticity and model adequacy. Ideally, residuals should be randomly scattered around zero with no discernible pattern.\n",
    "\n",
    "### Example 2.9: Voting Outcomes and Campaign Expenditures\n",
    "\n",
    "Let's examine the complete regression summary for the voting outcomes and campaign expenditures example, including R-squared and other statistical measures. We will also create an enhanced visualization with a 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:16:44.111222Z",
     "iopub.status.busy": "2025-10-20T00:16:44.111130Z",
     "iopub.status.idle": "2025-10-20T00:16:44.177488Z",
     "shell.execute_reply": "2025-10-20T00:16:44.177171Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Formatted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R-squared</td>\n",
       "      <td>0.8561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adjusted R-squared</td>\n",
       "      <td>0.8553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F-statistic</td>\n",
       "      <td>1017.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Number of observations</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Metric Formatted\n",
       "0               R-squared    0.8561\n",
       "1      Adjusted R-squared    0.8553\n",
       "2             F-statistic   1017.66\n",
       "3  Number of observations       173"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and analyze voting data - Re-run regression for vote1 dataset\n",
    "vote1 = wool.data(\"vote1\")  # Load data\n",
    "\n",
    "# Fit regression model\n",
    "reg = smf.ols(formula=\"voteA ~ shareA\", data=vote1)  # Define model\n",
    "results = reg.fit()  # Fit model\n",
    "\n",
    "# Create a clean summary table - Extract key statistics from results object\n",
    "summary_stats = pd.DataFrame(  # Create a DataFrame for summary statistics\n",
    "    {\n",
    "        \"Coefficient\": results.params,  # Estimated coefficients\n",
    "        \"Std. Error\": results.bse,  # Standard errors of coefficients\n",
    "        \"t-value\": results.tvalues,  # t-statistics for coefficients\n",
    "        \"p-value\": results.pvalues,  # p-values for coefficients\n",
    "    },\n",
    ")\n",
    "\n",
    "# Display regression summary\n",
    "summary_metrics = pd.DataFrame(\n",
    "    {\n",
    "        \"Metric\": [\n",
    "            \"R-squared\",\n",
    "            \"Adjusted R-squared\",\n",
    "            \"F-statistic\",\n",
    "            \"Number of observations\",\n",
    "        ],\n",
    "        \"Value\": [\n",
    "            results.rsquared,\n",
    "            results.rsquared_adj,\n",
    "            results.fvalue,\n",
    "            int(results.nobs),\n",
    "        ],\n",
    "        \"Formatted\": [\n",
    "            f\"{results.rsquared:.4f}\",\n",
    "            f\"{results.rsquared_adj:.4f}\",\n",
    "            f\"{results.fvalue:.2f}\",\n",
    "            f\"{int(results.nobs)}\",\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "display(summary_metrics[[\"Metric\", \"Formatted\"]])\n",
    "\n",
    "summary_stats.round(4)  # Display summary statistics table, rounded to 4 decimal places\n",
    "\n",
    "# Create enhanced visualization - Regression plot with confidence interval\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_regression(\n",
    "    \"shareA\",\n",
    "    \"voteA\",\n",
    "    vote1,\n",
    "    results,\n",
    "    \"Vote Share vs Campaign Spending Share\\nwith 95% Confidence Interval\",  # Title with CI mention\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code fits the regression model for voting outcomes and campaign spending share. It then creates a summary table containing coefficient estimates, standard errors, t-values, and p-values. It shows this summary, along with R-squared, adjusted R-squared, F-statistic, and the number of observations. Finally, it generates an enhanced regression plot, including a 95% confidence interval, using our `plot_regression` function.\n",
    "\n",
    ":::{note} Interpretation of Example 2.9\n",
    ":class: dropdown\n",
    "\n",
    "The output will provide a comprehensive summary of the regression results. You can see the estimated coefficients for the intercept and `shareA`, their standard errors, t-values, and p-values. The p-values are used for hypothesis testing (we will discuss this in detail in later chapters). The R-squared and Adjusted R-squared values indicate the goodness of fit. Adjusted R-squared is a modified version of R-squared that adjusts for the number of regressors in the model (it is more relevant in multiple regression). The F-statistic is used for testing the overall significance of the regression model.\n",
    "\n",
    "The enhanced regression plot visually represents the relationship between `shareA` and `voteA`, along with the 95% confidence interval around the regression line, providing a visual sense of the uncertainty in our predictions.\n",
    ":::\n",
    "\n",
    "## 2.4 Nonlinearities\n",
    "\n",
    "So far, we have focused on linear relationships between variables. However, in many cases, the relationship between the dependent and independent variables might be nonlinear. We can still use linear regression techniques to model certain types of nonlinear relationships by transforming the variables. Common transformations include using logarithms of variables. Let's consider three common models involving logarithms:\n",
    "\n",
    "1. **Log-Level Model**: In this model, the dependent variable is in logarithm form, while the independent variable is in level form:\n",
    "\n",
    "   $$\\log(y) = \\beta_0 + \\beta_1 x + u$$\n",
    "\n",
    "   In this model, $\\beta_1$ represents the approximate percentage change in $y$ for a one-unit change in $x$. Specifically, a one-unit increase in $x$ is associated with approximately a $100 \\cdot \\beta_1$ percent change in $y$. This approximation is most accurate when $|\\beta_1|$ is small (typically $|\\beta_1| < 0.1$). The exact percentage change is $100 \\cdot [\\exp(\\beta_1) - 1]$ percent.\n",
    "\n",
    "2. **Level-Log Model**: Here, the dependent variable is in level form, and the independent variable is in logarithm form:\n",
    "\n",
    "   $$y = \\beta_0 + \\beta_1 \\log(x) + u$$\n",
    "\n",
    "   In this model, $\\beta_1$ represents the approximate change in $y$ for a one-percent increase in $x$. Specifically, a one-percent increase in $x$ (i.e., from $x$ to $1.01x$) is associated with approximately a change in $y$ of $\\beta_1/100$ units. Equivalently, a 100 percent increase in $x$ (i.e., doubling from $x$ to $2x$) is associated with a change in $y$ of approximately $\\beta_1 \\cdot \\log(2) \\approx 0.69\\beta_1$ units.\n",
    "\n",
    "3. **Log-Log Model**: In this model, both the dependent and independent variables are in logarithm form:\n",
    "\n",
    "   $$\\log(y) = \\beta_0 + \\beta_1 \\log(x) + u$$\n",
    "\n",
    "   In the log-log model, $\\beta_1$ is interpreted as the **elasticity** of $y$ with respect to $x$. That is, a one-percent increase in $x$ is associated with approximately a $\\beta_1$ percent change in $y$. This interpretation is exact (not an approximation) because $\\frac{d \\log(y)}{d \\log(x)} = \\frac{dy/y}{dx/x} = \\beta_1$, which is the definition of elasticity.\n",
    "\n",
    "### Example 2.10: Wage and Education (Log-Level Model)\n",
    "\n",
    "Let's revisit the wage and education example and estimate a log-level model:\n",
    "\n",
    "$$\\log(\\text{wage}) = \\beta_0 + \\beta_1 \\text{educ} + u$$\n",
    "\n",
    "In this model, we are interested in the percentage increase in wage for each additional year of education."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3622864",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:16:44.178955Z",
     "iopub.status.busy": "2025-10-20T00:16:44.178870Z",
     "iopub.status.idle": "2025-10-20T00:16:44.187496Z",
     "shell.execute_reply": "2025-10-20T00:16:44.187245Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Formatted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept ($\\beta_0$)</td>\n",
       "      <td>0.5838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Education coefficient ($\\beta_1$)</td>\n",
       "      <td>0.0827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R-squared</td>\n",
       "      <td>0.1858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Parameter Formatted\n",
       "0              Intercept ($\\beta_0$)    0.5838\n",
       "1  Education coefficient ($\\beta_1$)    0.0827\n",
       "2                          R-squared    0.1858"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and prepare data - Re-use wage1 dataset\n",
    "wage1 = wool.data(\"wage1\")  # Load data (if needed)\n",
    "\n",
    "# Estimate log-level model - Define and fit the model with log(wage) as dependent variable\n",
    "reg = smf.ols(\n",
    "    formula=\"np.log(wage) ~ educ\",\n",
    "    data=wage1,\n",
    ")  # Use np.log() to take logarithm of wage\n",
    "results = reg.fit()  # Fit the model\n",
    "\n",
    "# Display Log-Level Model results\n",
    "log_level_results = pd.DataFrame(\n",
    "    {\n",
    "        \"Parameter\": [\n",
    "            \"Intercept ($\\\\beta_0$)\",\n",
    "            \"Education coefficient ($\\\\beta_1$)\",\n",
    "            \"R-squared\",\n",
    "        ],\n",
    "        \"Value\": [results.params.iloc[0], results.params.iloc[1], results.rsquared],\n",
    "        \"Formatted\": [\n",
    "            f\"{results.params.iloc[0]:.4f}\",\n",
    "            f\"{results.params.iloc[1]:.4f}\",\n",
    "            f\"{results.rsquared:.4f}\",\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "log_level_results[[\"Parameter\", \"Formatted\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be21fed",
   "metadata": {},
   "source": [
    "This code estimates the log-level model using `statsmodels` with `np.log(wage)` as the dependent variable and shows the estimated coefficients and R-squared.\n",
    "\n",
    ":::{note} Interpretation of Example 2.10\n",
    ":class: dropdown\n",
    "\n",
    "We find $\\hat{\\beta}_1 = 0.0827$. In the log-level model, this coefficient can be interpreted as the approximate percentage change in wage for a one-unit increase in education. So, approximately, each additional year of education is associated with an 8.27% increase in hourly wage. The intercept $\\hat{\\beta}_0 = 0.5838$ represents the predicted log(wage) when education is zero. The R-squared value is 0.1858, indicating the proportion of variation in $\\log(\\text{wage})$ explained by education.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:16:44.188692Z",
     "iopub.status.busy": "2025-10-20T00:16:44.188616Z",
     "iopub.status.idle": "2025-10-20T00:16:44.259745Z",
     "shell.execute_reply": "2025-10-20T00:16:44.259455Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create log-level visualization with seaborn defaults\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Simple, elegant regression plot\n",
    "sns.regplot(\n",
    "    data=wage1,\n",
    "    x=\"educ\",\n",
    "    y=np.log(wage1[\"wage\"]),\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# Clean titles and labels\n",
    "ax.set_title(\"Log-Level Model: Log(Wage) vs Years of Education\")\n",
    "ax.set_xlabel(\"Years of Education\")\n",
    "ax.set_ylabel(\"Log(Hourly Wage)\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot visualizes the log-level relationship, showing how log(wage) varies with years of education, along with the fitted regression line.\n",
    "\n",
    "### Example 2.11: CEO Salary and Firm Sales (Log-Log Model)\n",
    "\n",
    "Let's consider the CEO salary and firm sales relationship and estimate a log-log model:\n",
    "\n",
    "$$\\log(\\text{salary}) = \\beta_0 + \\beta_1 \\log(\\text{sales}) + u$$\n",
    "\n",
    "In this model, $\\beta_1$ represents the elasticity of CEO salary with respect to firm sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4345d950",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:16:44.261247Z",
     "iopub.status.busy": "2025-10-20T00:16:44.261162Z",
     "iopub.status.idle": "2025-10-20T00:16:44.268829Z",
     "shell.execute_reply": "2025-10-20T00:16:44.268566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Formatted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept ($\\beta_0$)</td>\n",
       "      <td>4.8220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sales elasticity ($\\beta_1$)</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R-squared</td>\n",
       "      <td>0.2108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Parameter Formatted\n",
       "0         Intercept ($\\beta_0$)    4.8220\n",
       "1  Sales elasticity ($\\beta_1$)    0.2567\n",
       "2                     R-squared    0.2108"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and prepare data - Re-use ceosal1 dataset\n",
    "ceosal1 = wool.data(\"ceosal1\")  # Load data (if needed)\n",
    "\n",
    "# Estimate log-log model - Define and fit model with log(salary) and log(sales)\n",
    "reg = smf.ols(\n",
    "    formula=\"np.log(salary) ~ np.log(sales)\",\n",
    "    data=ceosal1,\n",
    ")  # Use np.log() for both salary and sales\n",
    "results = reg.fit()  # Fit the model\n",
    "\n",
    "# Display Log-Log Model results\n",
    "log_log_results = pd.DataFrame(\n",
    "    {\n",
    "        \"Parameter\": [\n",
    "            \"Intercept ($\\\\beta_0$)\",\n",
    "            \"Sales elasticity ($\\\\beta_1$)\",\n",
    "            \"R-squared\",\n",
    "        ],\n",
    "        \"Value\": [results.params.iloc[0], results.params.iloc[1], results.rsquared],\n",
    "        \"Formatted\": [\n",
    "            f\"{results.params.iloc[0]:.4f}\",\n",
    "            f\"{results.params.iloc[1]:.4f}\",\n",
    "            f\"{results.rsquared:.4f}\",\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "log_log_results[[\"Parameter\", \"Formatted\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0260320b",
   "metadata": {},
   "source": [
    "This code estimates the log-log model using `statsmodels` with both variables in logarithmic form and shows the estimated coefficients and R-squared.\n",
    "\n",
    ":::{note} Interpretation of Example 2.11\n",
    ":class: dropdown\n",
    "\n",
    "We find $\\hat{\\beta}_1 = 0.2567$. In the log-log model, this coefficient is the elasticity of salary with respect to sales. It means that a 1% increase in firm sales is associated with approximately a 0.2567% increase in CEO salary. The intercept $\\hat{\\beta}_0 = 4.8220$ does not have a direct practical interpretation in terms of original variables in this model, but it is needed for the regression equation. The R-squared value is 0.2108, indicating the proportion of variation in $\\log(\\text{salary})$ explained by $\\log(\\text{sales})$.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:16:44.270002Z",
     "iopub.status.busy": "2025-10-20T00:16:44.269928Z",
     "iopub.status.idle": "2025-10-20T00:16:44.321792Z",
     "shell.execute_reply": "2025-10-20T00:16:44.321513Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create log-log visualization with seaborn defaults\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Simple, elegant regression plot\n",
    "sns.regplot(\n",
    "    data=ceosal1,\n",
    "    x=np.log(ceosal1[\"sales\"]),\n",
    "    y=np.log(ceosal1[\"salary\"]),\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# Clean titles and labels\n",
    "ax.set_title(\"Log-Log Model: CEO Salary Elasticity\")\n",
    "ax.set_xlabel(\"Log(Firm Sales)\")\n",
    "ax.set_ylabel(\"Log(CEO Salary)\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d57832",
   "metadata": {},
   "source": [
    "The scatter plot visualizes the log-log relationship between firm sales and CEO salary, demonstrating the elasticity concept where both variables are on logarithmic scales.\n",
    "\n",
    "## 2.5. Regression through the Origin and Regression on a Constant\n",
    "\n",
    "In standard simple linear regression, we include an intercept term ($\\beta_0$). However, in some cases, it might be appropriate to omit the intercept, forcing the regression line to pass through the origin (0,0). This is called **regression through the origin**. The model becomes:\n",
    "\n",
    "$$y = \\beta_1 x + u$$\n",
    "\n",
    "In `statsmodels`, you can perform regression through the origin by including `0 +` or `- 1 +` in the formula, like `\"salary ~ 0 + roe\"` or `\"salary ~ roe - 1\"`.\n",
    "\n",
    "Another special case is **regression on a constant only**, where we only estimate the intercept and do not include any independent variable. The model is:\n",
    "\n",
    "$$y = \\beta_0 + u$$\n",
    "\n",
    "In this case, $\\hat{\\beta}_0$ is simply the sample mean of $y$, $\\bar{y}$. This model essentially predicts the same value ($\\bar{y}$) for all observations, regardless of any other factors. In `statsmodels`, you can specify this as `\"salary ~ 1\"` or `\"salary ~ constant\"`.\n",
    "\n",
    "Let's compare these three regression specifications using the CEO salary and ROE example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:16:44.323263Z",
     "iopub.status.busy": "2025-10-20T00:16:44.323175Z",
     "iopub.status.idle": "2025-10-20T00:16:44.367887Z",
     "shell.execute_reply": "2025-10-20T00:16:44.367587Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load and prepare data - Re-use ceosal1 dataset\n",
    "ceosal1 = wool.data(\"ceosal1\")  # Load data (if needed)\n",
    "\n",
    "# 1. Regular OLS regression - Regression with intercept\n",
    "reg1 = smf.ols(formula=\"salary ~ roe\", data=ceosal1)  # Standard regression model\n",
    "results1 = reg1.fit()  # Fit model 1\n",
    "\n",
    "# 2. Regression through origin - Regression without intercept\n",
    "reg2 = smf.ols(\n",
    "    formula=\"salary ~ 0 + roe\",\n",
    "    data=ceosal1,\n",
    ")  # Regression through origin (no intercept)\n",
    "results2 = reg2.fit()  # Fit model 2\n",
    "\n",
    "# 3. Regression on constant only - Regression with only intercept (mean model)\n",
    "reg3 = smf.ols(formula=\"salary ~ 1\", data=ceosal1)  # Regression on constant only\n",
    "results3 = reg3.fit()  # Fit model 3\n",
    "\n",
    "# Compare results across all three models\n",
    "comparison_data = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": [\n",
    "            \"1. Regular regression\",\n",
    "            \"2. Regression through origin\",\n",
    "            \"3. Regression on constant\",\n",
    "        ],\n",
    "        \"Intercept ($\\\\beta_0$)\": [\n",
    "            f\"{results1.params.iloc[0]:.2f}\",\n",
    "            \"N/A\",\n",
    "            f\"{results3.params.iloc[0]:.2f}\",\n",
    "        ],\n",
    "        \"Slope ($\\\\beta_1$)\": [\n",
    "            f\"{results1.params.iloc[1]:.2f}\",\n",
    "            f\"{results2.params.iloc[0]:.2f}\",\n",
    "            \"N/A\",\n",
    "        ],\n",
    "        \"R-squared\": [\n",
    "            f\"{results1.rsquared:.4f}\",\n",
    "            f\"{results2.rsquared:.4f}\",\n",
    "            f\"{results3.rsquared:.4f}\",\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "comparison_data\n",
    "\n",
    "# Create regression comparison visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Enhanced scatter plot with seaborn\n",
    "sns.scatterplot(\n",
    "    data=ceosal1,\n",
    "    x=\"roe\",\n",
    "    y=\"salary\",\n",
    "    alpha=0.7,\n",
    "    s=80,\n",
    "    edgecolors=\"white\",\n",
    "    linewidths=0.5,\n",
    "    label=\"Data Points\",\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# Generate smooth x range for regression lines\n",
    "x_range = np.linspace(ceosal1[\"roe\"].min(), ceosal1[\"roe\"].max(), 100)\n",
    "\n",
    "# Plot regression lines with distinct, attractive colors\n",
    "colors = [\"#e74c3c\", \"#27ae60\", \"#3498db\"]  # Red, green, blue\n",
    "linestyles = [\"-\", \"--\", \"-.\"]  # Different line styles\n",
    "labels = [\"Regular Regression\", \"Through Origin\", \"Constant Only\"]\n",
    "linewidths = [2.5, 2.5, 2.5]\n",
    "\n",
    "# Regular regression line\n",
    "ax.plot(\n",
    "    x_range,\n",
    "    results1.params.iloc[0] + results1.params.iloc[1] * x_range,\n",
    "    color=colors[0],\n",
    "    linestyle=linestyles[0],\n",
    "    linewidth=linewidths[0],\n",
    "    label=labels[0],\n",
    "    alpha=0.9,\n",
    ")\n",
    "\n",
    "# Through origin line\n",
    "ax.plot(\n",
    "    x_range,\n",
    "    results2.params.iloc[0] * x_range,\n",
    "    color=colors[1],\n",
    "    linestyle=linestyles[1],\n",
    "    linewidth=linewidths[1],\n",
    "    label=labels[1],\n",
    "    alpha=0.9,\n",
    ")\n",
    "\n",
    "# Constant only line (horizontal)\n",
    "ax.axhline(\n",
    "    y=results3.params.iloc[0],\n",
    "    color=colors[2],\n",
    "    linestyle=linestyles[2],\n",
    "    linewidth=linewidths[2],\n",
    "    label=labels[2],\n",
    "    alpha=0.9,\n",
    ")\n",
    "\n",
    "# Enhanced styling\n",
    "ax.set_title(\n",
    "    \"Comparison of Regression Specifications\",\n",
    "    fontsize=16,\n",
    "    fontweight=\"bold\",\n",
    "    pad=20,\n",
    ")\n",
    "ax.set_xlabel(\"Return on Equity (ROE)\", fontsize=13, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"CEO Salary (thousands $)\", fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "# Enhanced legend\n",
    "ax.legend(\n",
    "    loc=\"upper right\",\n",
    "    frameon=True,\n",
    "    fancybox=True,\n",
    "    shadow=True,\n",
    "    fontsize=11,\n",
    ")\n",
    "\n",
    "# Enhanced grid\n",
    "ax.grid(True, alpha=0.3, linestyle=\"-\", linewidth=0.5)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization compares three different regression specifications with distinct styling for each model type.\n",
    "\n",
    ":::{note} Interpretation of Example 2.12\n",
    ":class: dropdown\n",
    "\n",
    "By comparing the results, you can see how the estimated coefficients and R-squared values differ across these specifications. Regression through the origin forces the line to go through (0,0), which may or may not be appropriate depending on the context. Regression on a constant simply gives you the mean of the dependent variable as the prediction and will always have an R-squared of 0 (unless variance of y is also zero, which is trivial case).\n",
    "\n",
    "In the visualization, you can visually compare the fit of these different regression lines to the data. Notice that R-squared for regression through the origin can sometimes be higher or lower than regular OLS, and it should be interpreted cautiously as the total sum of squares is calculated differently in regression through the origin. Regression on a constant will be a horizontal line at the mean of salary.\n",
    ":::\n",
    "\n",
    "## 2.6. Expected Values, Variances, and Standard Errors\n",
    "\n",
    "To understand the statistical properties of OLS estimators, we need to make certain assumptions about the population regression model and the data. These are known as the **Classical Linear Model (CLM) assumptions** for simple linear regression. The first five are often called **SLR assumptions**.\n",
    "\n",
    "Here are the five key assumptions for Simple Linear Regression:\n",
    "\n",
    "1. **SLR.1: Linear Population Regression Function**: The relationship between $y$ and $x$ in the population is linear:\n",
    "   $$y = \\beta_0 + \\beta_1 x + u$$\n",
    "   This assumes that the model is correctly specified in terms of linearity. This is a maintained assumption throughout the analysis.\n",
    "\n",
    "2. **SLR.2: Random Sampling**: We have a random sample of size $n$, $\\{(x_i, y_i): i=1, 2, ..., n\\}$, from the population model.\n",
    "   This assumption ensures that our sample is representative of the population we want to study and that observations are independent across $i$.\n",
    "\n",
    "3. **SLR.3: Sample Variation in x**: There is sample variation in the independent variable $x$, i.e., $\\widehat{\\text{Var}}(x) > 0$, meaning not all $x_i$ values are identical.\n",
    "   If there is no variation in $x$, we cannot estimate the relationship between $x$ and $y$ (the slope $\\beta_1$ is not identified).\n",
    "\n",
    "4. **SLR.4: Zero Conditional Mean**: The error term $u$ has an expected value of zero given any value of $x$:\n",
    "   $$E(u|x) = 0$$\n",
    "   This is the most crucial assumption for **unbiasedness** of OLS. It implies that the unobserved factors represented by $u$ are, on average, unrelated to $x$ at all values of $x$. This assumption implies $E(u) = 0$ and $\\text{Cov}(x, u) = 0$. If $x$ and $u$ are correlated, OLS estimators will be **biased** and **inconsistent**.\n",
    "\n",
    "5. **SLR.5: Homoscedasticity**: The error term $u$ has the same variance given any value of $x$:\n",
    "   $$\\text{Var}(u|x) = \\sigma^2$$\n",
    "   This assumption means that the spread of the errors is constant across all values of $x$. This assumption is required for **efficiency** (BLUE property) and for the standard formulas for OLS standard errors to be valid. If this assumption is violated (heteroscedasticity), OLS estimators remain **unbiased** and **consistent** under SLR.1-SLR.4, but they are no longer the Best Linear Unbiased Estimators (BLUE), and the usual standard errors and test statistics will be incorrect.\n",
    "\n",
    "Under these assumptions, we have important theoretical results about the OLS estimators:\n",
    "\n",
    "- **Theorem 2.1: Unbiasedness of OLS Estimators**: Under assumptions **SLR.1-SLR.4** (linearity, random sampling, sample variation, and zero conditional mean), the OLS estimators $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are unbiased estimators of $\\beta_0$ and $\\beta_1$, respectively. That is,\n",
    "  $$E(\\hat{\\beta}_0) = \\beta_0 \\quad \\text{and} \\quad E(\\hat{\\beta}_1) = \\beta_1$$\n",
    "  Unbiasedness means that on average, across many random samples from the same population, the OLS estimates will equal the true population parameters. Note that **SLR.5 (homoscedasticity) is not required** for unbiasedness.\n",
    "\n",
    "- **Theorem 2.2: Variances of OLS Estimators**: Under assumptions **SLR.1-SLR.5** (including homoscedasticity), the variances of the OLS estimators conditional on the sample values of $x$ are given by:\n",
    "  $$\\text{Var}(\\hat{\\beta}_0|x_1, \\ldots, x_n) = \\frac{\\sigma^2 \\sum_{i=1}^n x_i^2}{n \\sum_{i=1}^n (x_i - \\bar{x})^2} = \\sigma^2 \\frac{\\frac{1}{n}\\sum_{i=1}^n x_i^2}{\\text{SST}_x}$$\n",
    "  $$\\text{Var}(\\hat{\\beta}_1|x_1, \\ldots, x_n) = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{\\sigma^2}{\\text{SST}_x}$$\n",
    "  where $\\text{SST}_x = \\sum_{i=1}^n (x_i - \\bar{x})^2$ is the total sum of squares for $x$, and $\\sigma^2 = \\text{Var}(u|x)$ is the (constant) conditional variance of the error term. These formulas show that the variance of $\\hat{\\beta}_1$ decreases as the sample size $n$ increases and as the variation in $x$ (measured by $\\text{SST}_x$) increases.\n",
    "\n",
    "To make these variance formulas practically useful, we need to estimate the error variance $\\sigma^2$. An unbiased estimator of $\\sigma^2$ is the **standard error of regression (SER)**, denoted as $\\hat{\\sigma}^2$:\n",
    "\n",
    "$$\\hat{\\sigma}^2 = \\frac{1}{n-2} \\cdot \\sum_{i=1}^n \\hat{u}_i^2 = \\frac{\\text{SSR}}{n-2}$$\n",
    "\n",
    "The denominator $n-2$ reflects the degrees of freedom, as we lose two degrees of freedom when estimating $\\beta_0$ and $\\beta_1$. Notice that $\\hat{\\sigma}^2 = \\frac{n-1}{n-2} \\cdot \\text{var}(\\hat{u}_i)$, which is a slight adjustment to the sample variance of residuals to get an unbiased estimator of $\\sigma^2$.\n",
    "\n",
    "Using $\\hat{\\sigma}^2$, we can estimate the standard errors of $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$:\n",
    "\n",
    "$$\\text{se}(\\hat{\\beta}_0) = \\sqrt{\\widehat{\\text{var}}(\\hat{\\beta}_0)} = \\sqrt{\\frac{\\hat{\\sigma}^2 \\sum_{i=1}^n x_i^2}{n \\sum_{i=1}^n (x_i - \\bar{x})^2}} = \\sqrt{\\frac{\\hat{\\sigma}^2 [\\frac{1}{n}\\sum_{i=1}^n x_i^2]}{\\text{SST}_x}}$$\n",
    "\n",
    "$$\\text{se}(\\hat{\\beta}_1) = \\sqrt{\\widehat{\\text{var}}(\\hat{\\beta}_1)} = \\sqrt{\\frac{\\hat{\\sigma}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}} = \\sqrt{\\frac{\\hat{\\sigma}^2}{\\text{SST}_x}}$$\n",
    "\n",
    "Standard errors measure the precision of our coefficient estimates. Smaller standard errors indicate more precise estimates. They are crucial for hypothesis testing and constructing confidence intervals (which we will cover in subsequent chapters).\n",
    "\n",
    "### Example 2.12: Student Math Performance and the School Lunch Program\n",
    "\n",
    "Let's consider an example using the `meap93` dataset, examining the relationship between student math performance and the percentage of students eligible for the school lunch program. The model is:\n",
    "\n",
    "$$\\text{math10} = \\beta_0 + \\beta_1 \\text{lnchprg} + u$$\n",
    "\n",
    "where `math10` is the percentage of students passing a math test, and `lnchprg` is the percentage of students eligible for the lunch program. We expect a negative relationship, i.e., $\\beta_1 < 0$, as higher lunch program participation (indicating more poverty) might be associated with lower math scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:16:44.369570Z",
     "iopub.status.busy": "2025-10-20T00:16:44.369478Z",
     "iopub.status.idle": "2025-10-20T00:16:44.446391Z",
     "shell.execute_reply": "2025-10-20T00:16:44.446079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statistic</th>\n",
       "      <th>Formatted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SER</td>\n",
       "      <td>9.5659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SE($\\beta_0$)</td>\n",
       "      <td>0.9976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SE($\\beta_1$)</td>\n",
       "      <td>0.0348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Statistic Formatted\n",
       "0            SER    9.5659\n",
       "1  SE($\\beta_0$)    0.9976\n",
       "2  SE($\\beta_1$)    0.0348"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and analyze data - Use meap93 dataset\n",
    "meap93 = wool.data(\"meap93\")  # Load data\n",
    "\n",
    "# Estimate model - Regular OLS regression\n",
    "reg = smf.ols(formula=\"math10 ~ lnchprg\", data=meap93)  # Define model\n",
    "results = reg.fit()  # Fit model\n",
    "\n",
    "# Calculate SER manually - Calculate Standard Error of Regression manually\n",
    "n = results.nobs  # Number of observations\n",
    "u_hat_var = np.var(results.resid, ddof=1)  # Sample variance of residuals\n",
    "SER = np.sqrt(u_hat_var * (n - 1) / (n - 2))  # Calculate SER using formula\n",
    "\n",
    "# Calculate standard errors manually - Calculate standard errors of beta_0 and beta_1 manually\n",
    "lnchprg_var = np.var(meap93[\"lnchprg\"], ddof=1)  # Sample variance of lnchprg\n",
    "lnchprg_mean = np.mean(meap93[\"lnchprg\"])  # Mean of lnchprg\n",
    "lnchprg_sq_mean = np.mean(meap93[\"lnchprg\"] ** 2)  # Mean of squared lnchprg\n",
    "\n",
    "se_b1 = SER / (np.sqrt(lnchprg_var) * np.sqrt(n - 1))  # Standard error of beta_1\n",
    "se_b0 = se_b1 * np.sqrt(lnchprg_sq_mean)  # Standard error of beta_0\n",
    "\n",
    "# Display manual calculations\n",
    "manual_calculations = pd.DataFrame(\n",
    "    {\n",
    "        \"Statistic\": [\"SER\", \"SE($\\\\beta_0$)\", \"SE($\\\\beta_1$)\"],\n",
    "        \"Manual Calculation\": [SER, se_b0, se_b1],\n",
    "        \"Formatted\": [f\"{SER:.4f}\", f\"{se_b0:.4f}\", f\"{se_b1:.4f}\"],\n",
    "    },\n",
    ")\n",
    "display(manual_calculations[[\"Statistic\", \"Formatted\"]])\n",
    "\n",
    "results.summary().tables[1]  # Display statsmodels summary table\n",
    "\n",
    "# Create visualization with confidence intervals - Regression plot with CI\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_regression(\n",
    "    \"lnchprg\",\n",
    "    \"math10\",\n",
    "    meap93,\n",
    "    results,\n",
    "    \"Math Scores vs Lunch Program Participation\\nwith 95% Confidence Interval\",  # Title with CI mention\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code estimates the regression model, calculates the SER and standard errors of coefficients manually using the formulas, and then shows these manual calculations. It also displays the `statsmodels` regression summary table, which includes the standard errors calculated by `statsmodels` (which should match our manual calculations). Finally, it generates a regression plot with a 95% confidence interval.\n",
    "\n",
    ":::{note} Interpretation of Example 2.12 (Standard Errors)\n",
    ":class: dropdown\n",
    "\n",
    "By comparing the manually calculated SER and standard errors with those reported in the `statsmodels` summary table, you can verify that they are consistent. The standard errors provide a measure of the uncertainty associated with our coefficient estimates. Smaller standard errors mean our estimates are more precise. The regression plot with confidence intervals visually shows the range of plausible regression lines, given the uncertainty in our estimates.\n",
    ":::\n",
    "\n",
    "## 2.7. Causal Inference and Limitations\n",
    "\n",
    "While simple regression is powerful for describing relationships between variables, we must be careful when making **causal** interpretations. The slope coefficient $\\beta_1$ tells us the association between $x$ and $y$, but correlation does not imply causation. To interpret $\\beta_1$ as a causal effect requires strong assumptions that often don't hold with observational data.\n",
    "\n",
    "### 2.7.1. The Ceteris Paribus Interpretation\n",
    "\n",
    "The regression coefficient $\\beta_1$ represents the change in $y$ associated with a one-unit change in $x$, **holding all other factors constant** (ceteris paribus). However, in reality, when $x$ changes, other factors may also change, confounding our ability to isolate the true effect of $x$ on $y$.\n",
    "\n",
    "For example, in our wage-education regression:\n",
    "- More education (higher $x$) is associated with higher wages (higher $y$)\n",
    "- But education is correlated with ability, family background, motivation, etc.\n",
    "- These omitted factors affect both education and wages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e484fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrate the omitted variable problem\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "# Generate data with an omitted variable (ability)\n",
    "ability = stats.norm.rvs(0, 1, size=n)  # Unmeasured ability\n",
    "education = 12 + 2 * ability + stats.norm.rvs(0, 2, size=n)  # Ability affects education\n",
    "wage = 5 + 1.5 * education + 3 * ability + stats.norm.rvs(0, 5, size=n)  # Ability affects wages\n",
    "\n",
    "# Create DataFrame\n",
    "df_omitted = pd.DataFrame({\"wage\": wage, \"education\": education, \"ability\": ability})\n",
    "\n",
    "# Regression WITHOUT controlling for ability (omitted variable bias)\n",
    "model_biased = smf.ols(\"wage ~ education\", data=df_omitted).fit()\n",
    "\n",
    "# Regression WITH controlling for ability (closer to true effect)\n",
    "model_unbiased = smf.ols(\"wage ~ education + ability\", data=df_omitted).fit()\n",
    "\n",
    "# Compare results\n",
    "comparison = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": [\"Without Ability (Biased)\", \"With Ability (Unbiased)\", \"True Value\"],\n",
    "        \"Education Coefficient\": [\n",
    "            model_biased.params[\"education\"],\n",
    "            model_unbiased.params[\"education\"],\n",
    "            1.5,\n",
    "        ],\n",
    "        \"Interpretation\": [\n",
    "            \"Overestimates effect (includes ability)\",\n",
    "            \"Closer to true causal effect\",\n",
    "            \"True causal effect of education\",\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "display(comparison.round(3))\n",
    "\n",
    "# Visualize the bias\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Biased regression (omitting ability)\n",
    "ax1.scatter(education, wage, alpha=0.3, s=20)\n",
    "ax1.plot(\n",
    "    education,\n",
    "    model_biased.predict(),\n",
    "    \"r-\",\n",
    "    linewidth=2,\n",
    "    label=f\"Slope = {model_biased.params['education']:.2f}\",\n",
    ")\n",
    "ax1.set_xlabel(\"Education (years)\")\n",
    "ax1.set_ylabel(\"Wage\")\n",
    "ax1.set_title(\"Omitted Variable Bias\\n(Ability not controlled)\")\n",
    "ax1.legend()\n",
    "\n",
    "# Color by ability to show the confounding\n",
    "scatter = ax2.scatter(education, wage, c=ability, cmap=\"viridis\", alpha=0.5, s=20)\n",
    "ax2.plot(\n",
    "    education,\n",
    "    model_biased.predict(),\n",
    "    \"r--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Biased: {model_biased.params['education']:.2f}\",\n",
    ")\n",
    "# Note: For true line, we'd need to fix ability at its mean\n",
    "ax2.set_xlabel(\"Education (years)\")\n",
    "ax2.set_ylabel(\"Wage\")\n",
    "ax2.set_title(\"Wage colored by Ability\\n(showing confounding)\")\n",
    "ax2.legend()\n",
    "plt.colorbar(scatter, ax=ax2, label=\"Ability\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504159a4",
   "metadata": {},
   "source": [
    ":::{warning} Key Limitation\n",
    ":class: dropdown\n",
    "\n",
    "**Omitted variable bias** occurs when a variable that affects both $x$ and $y$ is left out of the regression. This causes the estimated coefficient to capture not just the effect of $x$, but also the indirect effect through the omitted variable. Multiple regression (Chapter 3) helps address this by including additional control variables, but we can never be sure we've controlled for everything.\n",
    ":::\n",
    "\n",
    "### 2.7.2. Requirements for Causal Interpretation\n",
    "\n",
    "For $\\beta_1$ to represent a **causal effect** of $x$ on $y$, we need:\n",
    "\n",
    "1. **Exogeneity**: $E(u|x) = 0$ - The error term must be uncorrelated with $x$\n",
    "   - All other factors affecting $y$ must be independent of $x$\n",
    "   - Difficult to achieve with observational data\n",
    "\n",
    "2. **No reverse causality**: $x$ causes $y$, not the other way around\n",
    "   - Example: Does income cause health, or does health cause income?\n",
    "\n",
    "3. **No measurement error**: Both $x$ and $y$ are measured accurately\n",
    "   - Measurement error in $x$ causes attenuation bias\n",
    "\n",
    "4. **Correct functional form**: The linear model is appropriate\n",
    "   - Relationship may be nonlinear in reality\n",
    "\n",
    "These conditions are rarely satisfied with observational data, which is why economists often seek **natural experiments** or use more advanced techniques (instrumental variables, difference-in-differences, regression discontinuity) to credibly estimate causal effects.\n",
    "\n",
    "## 2.8. Potential Outcomes and Randomized Experiments\n",
    "\n",
    "Modern causal inference uses the **potential outcomes framework**, which provides a rigorous way to think about causality and connects directly to regression analysis.\n",
    "\n",
    "### 2.8.1. The Potential Outcomes Framework\n",
    "\n",
    "Suppose we want to estimate the effect of a treatment (e.g., job training program) on an outcome (e.g., earnings). Each individual $i$ has two **potential outcomes**:\n",
    "\n",
    "- $y_i(1)$: Outcome if individual $i$ receives treatment ($x_i = 1$)\n",
    "- $y_i(0)$: Outcome if individual $i$ does not receive treatment ($x_i = 0$)\n",
    "\n",
    "The **individual treatment effect** is: $y_i(1) - y_i(0)$\n",
    "\n",
    "The fundamental problem: We can only observe ONE of these potential outcomes for each person!\n",
    "- If person $i$ receives treatment, we observe $y_i(1)$ but not $y_i(0)$\n",
    "- If person $i$ doesn't receive treatment, we observe $y_i(0)$ but not $y_i(1)$\n",
    "\n",
    "We can, however, estimate the **Average Treatment Effect (ATE)**:\n",
    "\n",
    "$$\\text{ATE} = E[y_i(1) - y_i(0)] = E[y_i(1)] - E[y_i(0)]$$\n",
    "\n",
    "### 2.8.2. Randomized Controlled Trials (RCTs)\n",
    "\n",
    "**Randomization** solves the fundamental problem by ensuring that treatment and control groups are comparable on average:\n",
    "\n",
    "$$E[y_i(1)|x_i=1] = E[y_i(1)] \\quad \\text{and} \\quad E[y_i(0)|x_i=0] = E[y_i(0)]$$\n",
    "\n",
    "With randomization, the ATE can be estimated by comparing average outcomes:\n",
    "\n",
    "$$\\widehat{\\text{ATE}} = \\bar{y}_{\\text{treated}} - \\bar{y}_{\\text{control}}$$\n",
    "\n",
    "**Connection to regression**: When $x$ is binary (0/1), the regression coefficient equals the ATE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962069c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a randomized controlled trial\n",
    "np.random.seed(123)\n",
    "n = 500\n",
    "\n",
    "# Generate potential outcomes (both exist for everyone, but we only observe one)\n",
    "y0 = 50 + stats.norm.rvs(0, 10, size=n)  # Potential outcome without treatment\n",
    "treatment_effect = 15  # True ATE\n",
    "y1 = y0 + treatment_effect  # Potential outcome with treatment\n",
    "\n",
    "# RANDOM assignment to treatment\n",
    "treatment = stats.bernoulli.rvs(0.5, size=n)  # 50% get treatment\n",
    "\n",
    "# Observed outcome (fundamental problem: we only see one potential outcome)\n",
    "y_observed = treatment * y1 + (1 - treatment) * y0\n",
    "\n",
    "# Create DataFrame\n",
    "rct_data = pd.DataFrame(\n",
    "    {\n",
    "        \"outcome\": y_observed,\n",
    "        \"treatment\": treatment,\n",
    "        \"y0\": y0,  # Normally unobserved!\n",
    "        \"y1\": y1,  # Normally unobserved!\n",
    "    }\n",
    ")\n",
    "\n",
    "# Method 1: Difference in means (simple comparison)\n",
    "ate_diff_means = rct_data[rct_data[\"treatment\"] == 1][\"outcome\"].mean() - rct_data[\n",
    "    rct_data[\"treatment\"] == 0\n",
    "][\"outcome\"].mean()\n",
    "\n",
    "# Method 2: Regression (equivalent with binary treatment!)\n",
    "model_rct = smf.ols(\"outcome ~ treatment\", data=rct_data).fit()\n",
    "ate_regression = model_rct.params[\"treatment\"]\n",
    "\n",
    "# Display results\n",
    "ate_results = pd.DataFrame(\n",
    "    {\n",
    "        \"Method\": [\n",
    "            \"True ATE\",\n",
    "            \"Difference in Means\",\n",
    "            \"Regression Coefficient\",\n",
    "            \"Standard Error (Regression)\",\n",
    "        ],\n",
    "        \"Estimate\": [\n",
    "            treatment_effect,\n",
    "            ate_diff_means,\n",
    "            ate_regression,\n",
    "            model_rct.bse[\"treatment\"],\n",
    "        ],\n",
    "        \"95% CI\": [\n",
    "            \"N/A\",\n",
    "            \"N/A\",\n",
    "            f\"[{model_rct.conf_int().loc['treatment', 0]:.2f}, {model_rct.conf_int().loc['treatment', 1]:.2f}]\",\n",
    "            \"N/A\",\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "display(ate_results.round(3))\n",
    "\n",
    "# Visualize the RCT results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Box plot comparing treatment and control\n",
    "rct_data.boxplot(column=\"outcome\", by=\"treatment\", ax=ax1)\n",
    "ax1.set_xlabel(\"Treatment Status\")\n",
    "ax1.set_ylabel(\"Outcome\")\n",
    "ax1.set_title(\"RCT: Treatment vs Control\\n(Randomization ensures comparability)\")\n",
    "ax1.set_xticklabels([\"Control\", \"Treatment\"])\n",
    "plt.sca(ax1)\n",
    "plt.xticks([1, 2], [\"Control (x=0)\", \"Treatment (x=1)\"])\n",
    "\n",
    "# Regression visualization\n",
    "plot_regression(\n",
    "    rct_data,\n",
    "    \"treatment\",\n",
    "    \"outcome\",\n",
    "    model_rct,\n",
    "    \"Regression with Binary Treatment\\n(Slope = Average Treatment Effect)\",\n",
    "    ax=ax2,\n",
    ")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfebb4f6",
   "metadata": {},
   "source": [
    ":::{important} Key Insights\n",
    ":class: dropdown\n",
    "\n",
    "1. **With randomization**, the regression coefficient on a binary treatment variable equals the ATE\n",
    "2. **Randomization** makes treatment assignment independent of potential outcomes: $\\{y_i(0), y_i(1)\\} \\perp x_i$\n",
    "3. This is why RCTs are the \"gold standard\" for causal inference\n",
    "4. **Without randomization** (observational data), selection bias can occur if treatment is correlated with potential outcomes\n",
    ":::\n",
    "\n",
    "### 2.8.3. Limitations and Extensions\n",
    "\n",
    "Even with RCTs, challenges remain:\n",
    "- **External validity**: Results may not generalize beyond the study population\n",
    "- **Compliance**: Not everyone assigned to treatment actually receives it\n",
    "- **Attrition**: People drop out of the study\n",
    "- **Spillovers**: Treatment of some affects outcomes of others\n",
    "\n",
    "When RCTs are not feasible, econometricians use **quasi-experimental methods**:\n",
    "- Instrumental variables (Chapter 15)\n",
    "- Difference-in-differences  \n",
    "- Regression discontinuity\n",
    "- Matching methods\n",
    "\n",
    "These methods attempt to mimic randomization using observational data, but require strong assumptions.\n",
    "\n",
    "## 2.9 Monte Carlo Simulations\n",
    "\n",
    "Monte Carlo simulations are powerful tools for understanding the statistical properties of estimators, like the OLS estimators. They involve repeatedly generating random samples from a known population model, estimating the parameters using OLS in each sample, and then examining the distribution of these estimates. This helps us to empirically verify properties like unbiasedness and understand the sampling variability of estimators.\n",
    "\n",
    "### 2.9.1. One Sample\n",
    "\n",
    "Let's start by simulating a single sample from a population regression model. We will define a true population model, generate random data based on this model, estimate the model using OLS on this sample, and compare the estimated coefficients with the true population parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:16:44.447855Z",
     "iopub.status.busy": "2025-10-20T00:16:44.447761Z",
     "iopub.status.idle": "2025-10-20T00:16:44.555376Z",
     "shell.execute_reply": "2025-10-20T00:16:44.555096Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility - Ensure consistent random number generation\n",
    "np.random.seed(1234567)  # Set seed for random number generator\n",
    "\n",
    "# Set parameters - Define true population parameters and sample size\n",
    "n = 1000  # sample size\n",
    "beta0 = 1  # true intercept\n",
    "beta1 = 0.5  # true slope\n",
    "sigma_u = 2  # standard deviation of error term\n",
    "\n",
    "# Generate data - Simulate x, u, and y based on population model\n",
    "x = stats.norm.rvs(4, 1, size=n)  # x ~ N(4, 1) - Generate x from normal distribution\n",
    "u = stats.norm.rvs(\n",
    "    0,\n",
    "    sigma_u,\n",
    "    size=n,\n",
    ")  # u ~ N(0, 4) - Generate error term from normal distribution\n",
    "y = (\n",
    "    beta0 + beta1 * x + u\n",
    ")  # population regression function - Generate y based on true model\n",
    "df = pd.DataFrame({\"y\": y, \"x\": x})  # Create DataFrame\n",
    "\n",
    "# Estimate model - Perform OLS regression on simulated data\n",
    "reg = smf.ols(formula=\"y ~ x\", data=df)  # Define OLS model\n",
    "results = reg.fit()  # Fit model\n",
    "\n",
    "# Display true vs estimated parameters\n",
    "parameter_comparison = pd.DataFrame(\n",
    "    {\n",
    "        \"Parameter\": [\"Intercept ($\\\\beta_0$)\", \"Slope ($\\\\beta_1$)\", \"R-squared\"],\n",
    "        \"True Value\": [beta0, beta1, \"N/A\"],\n",
    "        \"Estimated Value\": [\n",
    "            results.params.iloc[0],\n",
    "            results.params.iloc[1],\n",
    "            results.rsquared,\n",
    "        ],\n",
    "        \"Comparison\": [\n",
    "            f\"{beta0:.4f} vs {results.params.iloc[0]:.4f}\",\n",
    "            f\"{beta1:.4f} vs {results.params.iloc[1]:.4f}\",\n",
    "            f\"{results.rsquared:.4f}\",\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "parameter_comparison[[\"Parameter\", \"Comparison\"]]\n",
    "\n",
    "# Create Monte Carlo visualization with seaborn defaults\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Data and regression lines plot\n",
    "sns.scatterplot(x=x, y=y, ax=axes[0])\n",
    "x_range = np.linspace(x.min(), x.max(), 100)\n",
    "axes[0].plot(x_range, beta0 + beta1 * x_range, label=\"True Population Line\")\n",
    "axes[0].plot(\n",
    "    x_range,\n",
    "    results.params.iloc[0] + results.params.iloc[1] * x_range,\n",
    "    \"--\",\n",
    "    label=\"Sample Estimate\",\n",
    ")\n",
    "axes[0].set_title(\"Simulated Data with Regression Lines\")\n",
    "axes[0].set_xlabel(\"x\")\n",
    "axes[0].set_ylabel(\"y\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Error distribution plot\n",
    "sns.histplot(u, bins=25, stat=\"density\", ax=axes[1])\n",
    "u_range = np.linspace(u.min(), u.max(), 100)\n",
    "axes[1].plot(u_range, stats.norm.pdf(u_range, 0, sigma_u), label=\"True Distribution\")\n",
    "axes[1].set_title(\"Error Term Distribution\")\n",
    "axes[1].set_xlabel(\"Error term (u)\")\n",
    "axes[1].set_ylabel(\"Density\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code simulates a dataset from a simple linear regression model with known parameters. It then estimates the model using OLS and compares the estimated coefficients to the true parameters. It also visualizes the simulated data with both the true population regression line and the estimated regression line, as well as the distribution of the error terms compared to the true error distribution.\n",
    "\n",
    ":::{note} Interpretation of Example 2.13.1\n",
    ":class: dropdown\n",
    "\n",
    "By running this code, you will see that the estimated coefficients $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are close to, but not exactly equal to, the true values $\\beta_0$ and $\\beta_1$. This is because we are using a single random sample, and there is sampling variability. The regression plot shows the scatter of data points around the true population regression line, and the estimated regression line is an approximation based on this sample. The error distribution histogram should resemble the true normal distribution of errors.\n",
    ":::\n",
    "\n",
    "### 2.9.2. Many Samples\n",
    "\n",
    "To better understand the sampling properties of OLS estimators, we need to repeat the simulation process many times. This will allow us to observe the distribution of the OLS estimates across different samples, which is known as the **sampling distribution**. We can then check if the estimators are unbiased by looking at the mean of these estimates and examine their variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:16:44.556850Z",
     "iopub.status.busy": "2025-10-20T00:16:44.556771Z",
     "iopub.status.idle": "2025-10-20T00:16:44.871139Z",
     "shell.execute_reply": "2025-10-20T00:16:44.870839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set parameters - Simulation parameters (number of replications increased)\n",
    "np.random.seed(1234567)  # Set seed\n",
    "n = 1000  # sample size\n",
    "r = 10000  # number of replications (vectorized for efficiency)\n",
    "beta0 = 1  # true intercept\n",
    "beta1 = 0.5  # true slope\n",
    "sigma_u = 2  # standard deviation of error term\n",
    "\n",
    "# Generate fixed x values - Keep x values constant across replications to isolate variability from error term\n",
    "x = stats.norm.rvs(4, 1, size=n)  # Fixed x values from normal distribution\n",
    "\n",
    "# Vectorized Monte Carlo simulation - Generate all error terms at once (r x n matrix)\n",
    "u = stats.norm.rvs(0, sigma_u, size=(r, n))  # All error terms: r replications x n observations\n",
    "\n",
    "# Generate all y values at once using broadcasting - Efficient vectorized computation\n",
    "y = beta0 + beta1 * x + u  # Shape: (r, n) - all samples simultaneously\n",
    "\n",
    "# Compute OLS coefficients using vectorized formulas - Direct calculation without loops\n",
    "x_centered = x - x.mean()  # Center x values\n",
    "y_centered = y - y.mean(axis=1, keepdims=True)  # Center y values (each replication)\n",
    "\n",
    "# Vectorized OLS formulas applied to all replications at once\n",
    "b1 = (x_centered * y_centered).sum(axis=1) / (x_centered**2).sum()  # Slope estimates\n",
    "b0 = y.mean(axis=1) - b1 * x.mean()  # Intercept estimates\n",
    "\n",
    "# Display Monte Carlo results\n",
    "monte_carlo_results = pd.DataFrame(\n",
    "    {\n",
    "        \"Parameter\": [\"Intercept ($\\\\beta_0$)\", \"Slope ($\\\\beta_1$)\"],\n",
    "        \"True Value\": [beta0, beta1],\n",
    "        \"Mean Estimate\": [np.mean(b0), np.mean(b1)],\n",
    "        \"Standard Deviation\": [np.std(b0, ddof=1), np.std(b1, ddof=1)],\n",
    "        \"Summary\": [\n",
    "            f\"True: {beta0:.4f}, Mean: {np.mean(b0):.4f}, SD: {np.std(b0, ddof=1):.4f}\",\n",
    "            f\"True: {beta1:.4f}, Mean: {np.mean(b1):.4f}, SD: {np.std(b1, ddof=1):.4f}\",\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "monte_carlo_results[[\"Parameter\", \"Summary\"]]\n",
    "\n",
    "# Create sampling distribution visualization with seaborn defaults\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Beta_0 distribution\n",
    "sns.histplot(b0, bins=25, stat=\"density\", ax=axes[0])\n",
    "axes[0].axvline(beta0, linestyle=\"-\", label=\"True Value\")\n",
    "axes[0].axvline(np.mean(b0), linestyle=\"--\", label=\"Sample Mean\")\n",
    "axes[0].set_title(\"Sampling Distribution of $\\\\beta_0$\")\n",
    "axes[0].set_xlabel(\"$\\\\beta_0$ Estimates\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Beta_1 distribution\n",
    "sns.histplot(b1, bins=25, stat=\"density\", ax=axes[1])\n",
    "axes[1].axvline(beta1, linestyle=\"-\", label=\"True Value\")\n",
    "axes[1].axvline(np.mean(b1), linestyle=\"--\", label=\"Sample Mean\")\n",
    "axes[1].set_title(\"Sampling Distribution of $\\\\beta_1$\")\n",
    "axes[1].set_xlabel(\"$\\\\beta_1$ Estimates\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf65ec",
   "metadata": {},
   "source": [
    "This vectorized code performs a Monte Carlo simulation efficiently with 10,000 replications. Instead of looping through each replication, we generate all error terms at once as an $(r \\times n)$ matrix and use broadcasting to compute all $y$ values simultaneously. The OLS formulas are then applied directly to the entire matrix using vectorized operations, which is much faster than iterating. After computing all coefficient estimates, we calculate summary statistics and visualize the sampling distributions.\n",
    "\n",
    ":::{note} Interpretation of Example 2.13.2\n",
    ":class: dropdown\n",
    "\n",
    "By running this code, you will observe the sampling distributions of $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$. The histograms should be roughly bell-shaped (approximating normal distributions, due to the Central Limit Theorem). Importantly, you should see that the mean of the estimated coefficients (vertical green dashed line in the histograms) is very close to the true population parameters (vertical red dashed line). This empirically demonstrates the unbiasedness of the OLS estimators under the SLR assumptions. The standard deviations of the estimated coefficients provide a measure of their sampling variability.\n",
    ":::\n",
    "\n",
    "### 2.9.3. Violation of SLR.4 (Zero Conditional Mean)\n",
    "\n",
    "Now, let's investigate what happens when one of the key assumptions is violated. Consider the violation of SLR.4, the zero conditional mean assumption, i.e., $\\text{E}(u|x) \\neq 0$. This means that the error term is correlated with $x$. In this case, we expect OLS estimators to be biased. Let's simulate a scenario where this assumption is violated and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:16:44.872635Z",
     "iopub.status.busy": "2025-10-20T00:16:44.872552Z",
     "iopub.status.idle": "2025-10-20T00:16:45.179751Z",
     "shell.execute_reply": "2025-10-20T00:16:45.179457Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set parameters - Simulation parameters (same as before)\n",
    "np.random.seed(1234567)  # Set seed\n",
    "n = 1000\n",
    "r = 10000  # full replications with vectorization\n",
    "beta0 = 1\n",
    "beta1 = 0.5\n",
    "sigma_u = 2\n",
    "\n",
    "# Generate fixed x values - Fixed x values\n",
    "x = stats.norm.rvs(4, 1, size=n)\n",
    "\n",
    "# Vectorized simulation with E(u|x) != 0 - Efficient violation of SLR.4\n",
    "u_mean = (x - 4) / 5  # E(u|x) = (x - 4)/5 - Conditional mean of error depends on x\n",
    "\n",
    "# Generate all error terms with non-zero conditional mean using broadcasting\n",
    "u = stats.norm.rvs(size=(r, n)) * sigma_u + u_mean  # Broadcasting u_mean across r replications\n",
    "\n",
    "# Generate all y values at once - Vectorized computation\n",
    "y = beta0 + beta1 * x + u  # Shape: (r, n)\n",
    "\n",
    "# Compute OLS coefficients using vectorized formulas\n",
    "x_centered = x - x.mean()\n",
    "y_centered = y - y.mean(axis=1, keepdims=True)\n",
    "\n",
    "b1 = (x_centered * y_centered).sum(axis=1) / (x_centered**2).sum()  # Slope estimates\n",
    "b0 = y.mean(axis=1) - b1 * x.mean()  # Intercept estimates\n",
    "\n",
    "# Display Monte Carlo results with bias analysis\n",
    "bias_results = pd.DataFrame(\n",
    "    {\n",
    "        \"Parameter\": [\"Intercept ($\\\\beta_0$)\", \"Slope ($\\\\beta_1$)\"],\n",
    "        \"True Value\": [beta0, beta1],\n",
    "        \"Mean Estimate\": [np.mean(b0), np.mean(b1)],\n",
    "        \"Bias\": [np.mean(b0) - beta0, np.mean(b1) - beta1],\n",
    "        \"Analysis\": [\n",
    "            f\"True: {beta0:.4f}, Estimate: {np.mean(b0):.4f}, Bias: {np.mean(b0) - beta0:.4f}\",\n",
    "            f\"True: {beta1:.4f}, Estimate: {np.mean(b1):.4f}, Bias: {np.mean(b1) - beta1:.4f}\",\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "bias_results[[\"Parameter\", \"Analysis\"]]\n",
    "\n",
    "# Create bias visualization with seaborn defaults\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Beta_0 distribution showing bias\n",
    "sns.histplot(b0, bins=25, stat=\"density\", ax=axes[0])\n",
    "axes[0].axvline(beta0, linestyle=\"-\", label=\"True Value\")\n",
    "axes[0].axvline(np.mean(b0), linestyle=\"--\", label=\"Biased Mean\")\n",
    "axes[0].set_title(\"Sampling Distribution of $\\\\beta_0$\\nwith E(u|x) $\\\\neq$ 0\")\n",
    "axes[0].set_xlabel(\"$\\\\beta_0$ Estimates\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Beta_1 distribution showing bias\n",
    "sns.histplot(b1, bins=25, stat=\"density\", ax=axes[1])\n",
    "axes[1].axvline(beta1, linestyle=\"-\", label=\"True Value\")\n",
    "axes[1].axvline(np.mean(b1), linestyle=\"--\", label=\"Biased Mean\")\n",
    "axes[1].set_title(\"Sampling Distribution of $\\\\beta_1$\\nwith E(u|x) $\\\\neq$ 0\")\n",
    "axes[1].set_xlabel(\"$\\\\beta_1$ Estimates\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this vectorized code, we intentionally violate the zero conditional mean assumption by setting the mean of the error term to depend on $x$: $\\text{E}(u|x) = (x - 4)/5$. Instead of looping, we use broadcasting to add the conditional mean to all replications at once, then perform the vectorized Monte Carlo simulation.\n",
    "\n",
    ":::{note} Interpretation of Example 2.13.3\n",
    ":class: dropdown\n",
    "\n",
    "By running this simulation, you will observe that the mean of the estimated coefficients $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are no longer close to the true values $\\beta_0$ and $\\beta_1$. The bias, calculated as the difference between the mean estimate and the true value, will be noticeably different from zero. This empirically demonstrates that when the zero conditional mean assumption (SLR.4) is violated, the OLS estimators become biased. The histograms of the sampling distributions will be centered around the biased mean estimates, not the true values.\n",
    ":::\n",
    "\n",
    "### 2.9.4. Violation of SLR.5 (Homoscedasticity)\n",
    "\n",
    "Finally, let's consider the violation of SLR.5, the homoscedasticity assumption, i.e., $\\text{var}(u|x) \\neq \\sigma^2$. This means that the variance of the error term is not constant across values of $x$ (heteroscedasticity). While heteroscedasticity does not cause bias in OLS estimators, it affects their efficiency and the validity of standard errors and inference. Let's simulate a scenario with heteroscedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T00:16:45.181271Z",
     "iopub.status.busy": "2025-10-20T00:16:45.181174Z",
     "iopub.status.idle": "2025-10-20T00:16:45.452722Z",
     "shell.execute_reply": "2025-10-20T00:16:45.452407Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set parameters - Simulation parameters (same as before)\n",
    "np.random.seed(1234567)  # Set seed\n",
    "n = 1000\n",
    "r = 10000  # full replications with vectorization\n",
    "beta0 = 1\n",
    "beta1 = 0.5\n",
    "\n",
    "# Generate fixed x values - Fixed x values\n",
    "x = stats.norm.rvs(4, 1, size=n)\n",
    "\n",
    "# Vectorized simulation with heteroscedasticity - Efficient violation of SLR.5\n",
    "u_std = np.sqrt(4 / np.exp(4.5) * np.exp(x))  # var(u|x) = 4e^(x-4.5) -> std(u|x)\n",
    "\n",
    "# Generate all error terms with variance depending on x using broadcasting\n",
    "u = stats.norm.rvs(size=(r, n)) * u_std  # Broadcasting u_std across r replications\n",
    "\n",
    "# Generate all y values at once - Vectorized computation\n",
    "y = beta0 + beta1 * x + u  # Shape: (r, n)\n",
    "\n",
    "# Compute OLS coefficients using vectorized formulas\n",
    "x_centered = x - x.mean()\n",
    "y_centered = y - y.mean(axis=1, keepdims=True)\n",
    "\n",
    "b1 = (x_centered * y_centered).sum(axis=1) / (x_centered**2).sum()  # Slope estimates\n",
    "b0 = y.mean(axis=1) - b1 * x.mean()  # Intercept estimates\n",
    "\n",
    "# Display Monte Carlo results with heteroscedasticity\n",
    "heteroscedasticity_results = pd.DataFrame(\n",
    "    {\n",
    "        \"Parameter\": [\"Intercept ($\\\\beta_0$)\", \"Slope ($\\\\beta_1$)\"],\n",
    "        \"True Value\": [beta0, beta1],\n",
    "        \"Mean Estimate\": [np.mean(b0), np.mean(b1)],\n",
    "        \"Standard Deviation\": [np.std(b0, ddof=1), np.std(b1, ddof=1)],\n",
    "        \"Summary\": [\n",
    "            f\"True: {beta0:.4f}, Mean: {np.mean(b0):.4f}, SD: {np.std(b0, ddof=1):.4f}\",\n",
    "            f\"True: {beta1:.4f}, Mean: {np.mean(b1):.4f}, SD: {np.std(b1, ddof=1):.4f}\",\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "heteroscedasticity_results[[\"Parameter\", \"Summary\"]]\n",
    "\n",
    "# Create heteroscedasticity visualization - Use last replication for visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Simple scatter plot with heteroscedasticity - Use last replication\n",
    "y_last = y[-1]  # Last replication for visualization\n",
    "u_last = u[-1]  # Last replication errors\n",
    "\n",
    "sns.scatterplot(x=x, y=y_last, ax=axes[0])\n",
    "x_range = np.linspace(x.min(), x.max(), 100)\n",
    "axes[0].plot(x_range, beta0 + beta1 * x_range, \"--\", label=\"True Regression Line\")\n",
    "axes[0].set_title(\"Sample Data with Heteroscedasticity\")\n",
    "axes[0].set_xlabel(\"x\")\n",
    "axes[0].set_ylabel(\"y\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Error term visualization\n",
    "sns.scatterplot(x=x, y=u_last, ax=axes[1])\n",
    "axes[1].axhline(y=0, linestyle=\"--\", label=\"E(u|x) = 0\")\n",
    "axes[1].set_title(\"Error Terms vs. x\")\n",
    "axes[1].set_xlabel(\"x\")\n",
    "axes[1].set_ylabel(\"Error term (u)\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7db6b4",
   "metadata": {},
   "source": [
    "In this vectorized code, we introduce heteroscedasticity by making the variance of the error term dependent on $x$: $\\text{var}(u|x) = 4e^{(x-4.5)}$. We compute the standard deviation and use broadcasting to scale the standard normal error terms across all replications simultaneously, making the simulation much more efficient.\n",
    "\n",
    ":::{note} Interpretation of Example 2.13.4\n",
    ":class: dropdown\n",
    "\n",
    "By running this simulation, you will observe that the mean of the estimated coefficients $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are still close to the true values $\\beta_0$ and $\\beta_1$. This confirms that OLS estimators remain unbiased even under heteroscedasticity (as long as SLR.1-SLR.4 hold). However, you might notice that the standard deviations of the estimated coefficients (sampling variability) could be different compared to the homoscedastic case (Example 2.7.2), although unbiasedness is maintained. The scatter plot of data with heteroscedasticity will show that the spread of data points around the regression line is not constant across the range of $x$. The plot of error terms vs. $x$ directly visualizes the heteroscedasticity, as you'll see the spread of error terms changing with $x$.\n",
    ":::\n",
    "\n",
    "Through these Monte Carlo simulations, we have empirically explored the properties of OLS estimators and the consequences of violating some of the key assumptions of the simple linear regression model.\n",
    "\n",
    "This concludes our exploration of the Simple Regression Model. We have covered the basics of OLS estimation, interpretation of results, goodness of fit, nonlinear transformations, special regression cases, and the importance of underlying assumptions. We also used Monte Carlo simulations to understand the statistical properties of OLS estimators. This foundation is crucial for understanding more advanced econometric techniques and models in subsequent chapters."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "notebooks//ipynb,markdown//md,scripts//py"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
