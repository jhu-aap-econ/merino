{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61e9b5cf",
   "metadata": {},
   "source": [
    "# 17. Limited Dependent Variable Models and Sample Selection Corrections\n",
    "\n",
    ":::{important} Learning Objectives\n",
    ":class: dropdown\n",
    "By the end of this chapter, you should be able to:\n",
    "\n",
    "**17.1** Estimate and interpret logit and probit models for binary response variables.\n",
    "\n",
    "**17.2** Apply fractional response models when the outcome is a proportion or percentage.\n",
    "\n",
    "**17.3** Use exponential mean models and Poisson regression for count data.\n",
    "\n",
    "**17.4** Estimate Tobit models for corner solution responses (e.g., hours worked, charitable contributions).\n",
    "\n",
    "**17.5** Distinguish between censored and truncated regression models and estimate them appropriately.\n",
    "\n",
    "**17.6** Apply Heckman's two-step procedure to correct for sample selection bias.\n",
    ":::\n",
    "\n",
    "Welcome to Chapter 17, where we study models for **limited dependent variables**—outcomes that are restricted in some way:\n",
    "\n",
    "- **Binary**: 0 or 1 (employed/unemployed, married/single)\n",
    "- **Fractional**: Proportions between 0 and 1 (savings rate, portfolio share)\n",
    "- **Count**: Non-negative integers (number of arrests, patent applications)\n",
    "- **Corner Solution**: Many zeros, positive continuous values (hours worked, charitable giving)\n",
    "- **Censored/Truncated**: Values cut off at some threshold\n",
    "\n",
    "The key insight: **OLS is inappropriate** for limited dependent variables because:\n",
    "1. Predictions can be outside the feasible range (e.g., negative probabilities)\n",
    "2. Errors are not normally distributed (heteroskedasticity is inherent)\n",
    "3. Effects are nonlinear (marginal effects depend on covariate values)\n",
    "\n",
    "We need **specialized models** that respect the nature of the outcome variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3e4c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install matplotlib numpy pandas statsmodels wooldridge scipy patsy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76031917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import patsy as pt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import wooldridge as wool\n",
    "from IPython.display import display\n",
    "from scipy import stats\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 6]\n",
    "plt.rcParams[\"font.size\"] = 11\n",
    "plt.rcParams[\"axes.titlesize\"] = 14\n",
    "plt.rcParams[\"axes.labelsize\"] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f7cef",
   "metadata": {},
   "source": [
    "## 17.1 Logit and Probit Models for Binary Response\n",
    "\n",
    "When the outcome is **binary** (0 or 1), we model the **probability** of the outcome being 1:\n",
    "\n",
    "$$ P(y = 1 | \\mathbf{x}) = G(\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k) = G(\\mathbf{x}'\\boldsymbol{\\beta}) $$\n",
    "\n",
    "where $G(\\cdot)$ is a **cumulative distribution function (CDF)** that ensures $0 \\leq P(y=1|\\mathbf{x}) \\leq 1$.\n",
    "\n",
    "### Three Approaches\n",
    "\n",
    "1. **Linear Probability Model (LPM)**: $G(z) = z$ (just use OLS)\n",
    "   - ✓ Simple, easy to interpret\n",
    "   - ✗ Can predict outside [0, 1]\n",
    "   - ✗ Heteroskedasticity by construction\n",
    "\n",
    "2. **Logit Model**: $G(z) = \\frac{e^z}{1 + e^z} = \\Lambda(z)$ (logistic CDF)\n",
    "   - ✓ Always predicts probabilities in [0, 1]\n",
    "   - ✓ Closed-form derivatives\n",
    "   - Assumes **logistic** error distribution\n",
    "\n",
    "3. **Probit Model**: $G(z) = \\Phi(z)$ (standard normal CDF)\n",
    "   - ✓ Always predicts probabilities in [0, 1]\n",
    "   - ✓ Based on **latent variable** interpretation\n",
    "   - Assumes **normal** error distribution\n",
    "\n",
    "### Latent Variable Interpretation\n",
    "\n",
    "Think of an **unobserved** continuous variable $y^*$ (e.g., \"propensity to work\"):\n",
    "\n",
    "$$ y^* = \\mathbf{x}'\\boldsymbol{\\beta} + u $$\n",
    "\n",
    "We observe:\n",
    "\n",
    "$$ y = \\begin{cases} 1 & \\text{if } y^* > 0 \\\\ 0 & \\text{if } y^* \\leq 0 \\end{cases} $$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$ P(y = 1 | \\mathbf{x}) = P(y^* > 0 | \\mathbf{x}) = P(u > -\\mathbf{x}'\\boldsymbol{\\beta} | \\mathbf{x}) = G(\\mathbf{x}'\\boldsymbol{\\beta}) $$\n",
    "\n",
    "- **Logit**: $u$ follows logistic distribution\n",
    "- **Probit**: $u \\sim N(0, 1)$\n",
    "\n",
    "### Estimation\n",
    "\n",
    "Both models are estimated by **maximum likelihood**:\n",
    "\n",
    "$$ \\mathcal{L}(\\boldsymbol{\\beta}) = \\prod_{i=1}^n [G(\\mathbf{x}_i'\\boldsymbol{\\beta})]^{y_i} [1 - G(\\mathbf{x}_i'\\boldsymbol{\\beta})]^{1-y_i} $$\n",
    "\n",
    "Log likelihood:\n",
    "\n",
    "$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left\\{ y_i \\log G(\\mathbf{x}_i'\\boldsymbol{\\beta}) + (1-y_i) \\log[1 - G(\\mathbf{x}_i'\\boldsymbol{\\beta})] \\right\\} $$\n",
    "\n",
    "### Example 17.1: Married Women's Labor Force Participation\n",
    "\n",
    "Let's estimate the probability that a married woman is in the labor force as a function of her characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e4e198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load married women's labor force participation data\n",
    "mroz = wool.data(\"mroz\")\n",
    "\n",
    "print(\"MARRIED WOMEN'S LABOR FORCE PARTICIPATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTotal observations: {len(mroz)}\")\n",
    "print(f\"Women in labor force (inlf=1): {mroz['inlf'].sum()}\")\n",
    "print(f\"Women not in labor force (inlf=0): {(1 - mroz['inlf']).sum()}\")\n",
    "print(f\"Participation rate: {mroz['inlf'].mean():.1%}\")\n",
    "\n",
    "print(\"\\nDEPENDENT VARIABLE:\")\n",
    "print(\"  inlf = 1 if woman worked for a wage in 1975, 0 otherwise\")\n",
    "\n",
    "print(\"\\nEXPLANATORY VARIABLES:\")\n",
    "print(\"  nwifeinc  = husband's income (thousands)\")\n",
    "print(\"  educ      = years of education\")\n",
    "print(\"  exper     = actual labor market experience (years)\")\n",
    "print(\"  age       = age in years\")\n",
    "print(\"  kidslt6   = number of children less than 6 years old\")\n",
    "print(\"  kidsge6   = number of children 6-18 years old\")\n",
    "\n",
    "# Summary statistics\n",
    "key_vars = [\"inlf\", \"nwifeinc\", \"educ\", \"exper\", \"age\", \"kidslt6\", \"kidsge6\"]\n",
    "display(mroz[key_vars].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c133ef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR PROBABILITY MODEL (LPM)\n",
    "print(\"\\n17.1.1 LINEAR PROBABILITY MODEL (OLS)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Model: P(inlf=1) = β₀ + β₁·nwifeinc + β₂·educ + ... + u\")\n",
    "print(\"Uses OLS but with heteroskedasticity-robust standard errors (HC3)\")\n",
    "\n",
    "lpm = smf.ols(\n",
    "    formula=\"inlf ~ nwifeinc + educ + exper + I(exper**2) + age + kidslt6 + kidsge6\",\n",
    "    data=mroz,\n",
    ").fit(cov_type=\"HC3\")\n",
    "\n",
    "table_lpm = pd.DataFrame(\n",
    "    {\n",
    "        \"Coefficient\": lpm.params,\n",
    "        \"Std. Error\": lpm.bse,\n",
    "        \"t-statistic\": lpm.tvalues,\n",
    "        \"p-value\": lpm.pvalues,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"\\nLINEAR PROBABILITY MODEL RESULTS:\")\n",
    "display(table_lpm.round(4))\n",
    "\n",
    "print(f\"\\nR-squared: {lpm.rsquared:.4f}\")\n",
    "print(f\"Observations: {lpm.nobs:.0f}\")\n",
    "\n",
    "print(\"\\nINTERPRETATION (coefficients are marginal effects):\")\n",
    "print(f\"  nwifeinc: {lpm.params['nwifeinc']:.4f}\")\n",
    "print(\n",
    "    f\"    → $1,000 increase in husband's income → {100 * lpm.params['nwifeinc']:.2f} percentage point decrease in participation\"\n",
    ")\n",
    "print(f\"  educ: {lpm.params['educ']:.4f}\")\n",
    "print(\n",
    "    f\"    → 1 more year of education → {100 * lpm.params['educ']:.2f} percentage point increase in participation\"\n",
    ")\n",
    "print(f\"  kidslt6: {lpm.params['kidslt6']:.4f}\")\n",
    "print(\n",
    "    f\"    → 1 more young child → {100 * lpm.params['kidslt6']:.2f} percentage point decrease in participation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70112f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check LPM predictions - some may be outside [0, 1]\n",
    "print(\"\\nPROBLEM WITH LPM: Predictions can be outside [0, 1]\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "lpm_predictions = lpm.predict(mroz)\n",
    "print(f\"Minimum prediction: {lpm_predictions.min():.4f}\")\n",
    "print(f\"Maximum prediction: {lpm_predictions.max():.4f}\")\n",
    "\n",
    "n_below_zero = (lpm_predictions < 0).sum()\n",
    "n_above_one = (lpm_predictions > 1).sum()\n",
    "\n",
    "print(f\"\\nPredictions < 0: {n_below_zero} ({100 * n_below_zero / len(mroz):.1f}%)\")\n",
    "print(f\"Predictions > 1: {n_above_one} ({100 * n_above_one / len(mroz):.1f}%)\")\n",
    "\n",
    "if n_below_zero > 0 or n_above_one > 0:\n",
    "    print(\"\\n✗ LPM produces invalid probabilities!\")\n",
    "    print(\"  → Need logit or probit to constrain predictions to [0, 1]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1717ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extreme predictions with LPM\n",
    "print(\"\\nEXTREME CASE PREDICTIONS (LPM)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Two \"extreme\" women\n",
    "X_extreme = pd.DataFrame(\n",
    "    {\n",
    "        \"nwifeinc\": [100, 0],  # High vs low husband income\n",
    "        \"educ\": [5, 17],  # Low vs high education\n",
    "        \"exper\": [0, 30],  # No experience vs lots\n",
    "        \"age\": [20, 52],  # Young vs older\n",
    "        \"kidslt6\": [2, 0],  # Young children vs none\n",
    "        \"kidsge6\": [0, 0],  # No older children\n",
    "    },\n",
    ")\n",
    "\n",
    "predictions_lpm = lpm.predict(X_extreme)\n",
    "\n",
    "print(\"Woman 1: Low education, high husband income, young children\")\n",
    "print(f\"  Predicted P(inlf=1) = {predictions_lpm.iloc[0]:.4f}\")\n",
    "\n",
    "print(\"\\nWoman 2: High education, low husband income, no young children\")\n",
    "print(f\"  Predicted P(inlf=1) = {predictions_lpm.iloc[1]:.4f}\")\n",
    "\n",
    "if predictions_lpm.iloc[0] < 0:\n",
    "    print(f\"\\n✗ Woman 1: Negative probability! ({predictions_lpm.iloc[0]:.4f})\")\n",
    "if predictions_lpm.iloc[1] > 1:\n",
    "    print(f\"✗ Woman 2: Probability > 1! ({predictions_lpm.iloc[1]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c8fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGIT MODEL\n",
    "print(\"\\n\\n17.1.3 LOGIT MODEL\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Model: P(inlf=1) = Λ(β₀ + β₁·nwifeinc + β₂·educ + ...)\")\n",
    "print(\"where Λ(z) = exp(z) / [1 + exp(z)] is the logistic CDF\")\n",
    "\n",
    "logit = smf.logit(\n",
    "    formula=\"inlf ~ nwifeinc + educ + exper + I(exper**2) + age + kidslt6 + kidsge6\",\n",
    "    data=mroz,\n",
    ").fit(disp=0)\n",
    "\n",
    "print(\"\\nLOGIT MODEL RESULTS:\")\n",
    "print(logit.summary())\n",
    "\n",
    "print(f\"\\nLog-Likelihood: {logit.llf:.4f}\")\n",
    "print(f\"McFadden's Pseudo R²: {logit.prsquared:.4f}\")\n",
    "print(f\"AIC: {logit.aic:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fa336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBIT MODEL\n",
    "print(\"\\n17.1.4 PROBIT MODEL\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Model: P(inlf=1) = Φ(β₀ + β₁·nwifeinc + β₂·educ + ...)\")\n",
    "print(\"where Φ(z) is the standard normal CDF\")\n",
    "\n",
    "probit = smf.probit(\n",
    "    formula=\"inlf ~ nwifeinc + educ + exper + I(exper**2) + age + kidslt6 + kidsge6\",\n",
    "    data=mroz,\n",
    ").fit(disp=0)\n",
    "\n",
    "print(\"\\nPROBIT MODEL RESULTS:\")\n",
    "print(probit.summary())\n",
    "\n",
    "print(f\"\\nLog-Likelihood: {probit.llf:.4f}\")\n",
    "print(f\"McFadden's Pseudo R²: {probit.prsquared:.4f}\")\n",
    "print(f\"AIC: {probit.aic:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adff254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three models\n",
    "print(\"\\n\\nCOMPARISON: LPM vs LOGIT vs PROBIT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "comparison = pd.DataFrame(\n",
    "    {\n",
    "        \"LPM\": lpm.params,\n",
    "        \"LPM SE\": lpm.bse,\n",
    "        \"Logit\": logit.params,\n",
    "        \"Logit SE\": logit.bse,\n",
    "        \"Probit\": probit.params,\n",
    "        \"Probit SE\": probit.bse,\n",
    "    },\n",
    ")\n",
    "\n",
    "display(comparison.round(4))\n",
    "\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(\"1. COEFFICIENT MAGNITUDES:\")\n",
    "print(\"   - LPM coefficients are on probability scale\")\n",
    "print(\"   - Logit coefficients are larger (logistic distribution has heavier tails)\")\n",
    "print(\"   - Probit coefficients are between LPM and Logit\")\n",
    "print(\"   - CANNOT directly compare coefficients across models!\")\n",
    "\n",
    "print(\"\\n2. STATISTICAL SIGNIFICANCE:\")\n",
    "print(\"   - Pattern of significance is similar across models\")\n",
    "print(\"   - Same variables are significant in all three models\")\n",
    "\n",
    "print(\"\\n3. PREDICTION VALIDITY:\")\n",
    "logit_pred = logit.predict(mroz)\n",
    "probit_pred = probit.predict(mroz)\n",
    "print(f\"   - LPM:    min={lpm_predictions.min():.4f}, max={lpm_predictions.max():.4f}\")\n",
    "print(f\"   - Logit:  min={logit_pred.min():.4f}, max={logit_pred.max():.4f}\")\n",
    "print(f\"   - Probit: min={probit_pred.min():.4f}, max={probit_pred.max():.4f}\")\n",
    "print(\"   ✓ Logit and Probit always give valid probabilities [0, 1]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc901fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the three CDFs\n",
    "print(\"\\nVISUALIZING THE RESPONSE FUNCTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create range of z values\n",
    "z = np.linspace(-3, 3, 300)\n",
    "\n",
    "# Three CDFs\n",
    "linear = z  # Linear probability (just z)\n",
    "logistic = 1 / (1 + np.exp(-z))  # Logit\n",
    "normal = stats.norm.cdf(z)  # Probit\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Panel A: Full range\n",
    "ax1.plot(z, linear, \"r--\", linewidth=2, label=\"Linear (LPM)\")\n",
    "ax1.plot(z, logistic, \"b-\", linewidth=2, label=\"Logistic (Logit)\")\n",
    "ax1.plot(z, normal, \"g-\", linewidth=2, label=\"Normal (Probit)\")\n",
    "ax1.axhline(y=0, color=\"black\", linestyle=\":\", linewidth=1)\n",
    "ax1.axhline(y=1, color=\"black\", linestyle=\":\", linewidth=1)\n",
    "ax1.set_xlabel(\"z = x'β (index function)\")\n",
    "ax1.set_ylabel(\"P(y=1|x) = G(z)\")\n",
    "ax1.set_title(\"Response Functions: LPM vs Logit vs Probit\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim([-0.2, 1.2])\n",
    "\n",
    "# Panel B: Zoomed to [0, 1]\n",
    "z_zoom = np.linspace(-2, 2, 300)\n",
    "logistic_zoom = 1 / (1 + np.exp(-z_zoom))\n",
    "normal_zoom = stats.norm.cdf(z_zoom)\n",
    "\n",
    "ax2.plot(z_zoom, logistic_zoom, \"b-\", linewidth=2, label=\"Logistic (Logit)\")\n",
    "ax2.plot(z_zoom, normal_zoom, \"g-\", linewidth=2, label=\"Normal (Probit)\")\n",
    "ax2.axhline(y=0, color=\"black\", linestyle=\":\", linewidth=1)\n",
    "ax2.axhline(y=1, color=\"black\", linestyle=\":\", linewidth=1)\n",
    "ax2.set_xlabel(\"z = x'β (index function)\")\n",
    "ax2.set_ylabel(\"P(y=1|x)\")\n",
    "ax2.set_title(\"Logit vs Probit (Very Similar!)\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY OBSERVATIONS:\")\n",
    "print(\"✓ LPM is linear - can exceed [0, 1]\")\n",
    "print(\"✓ Logit and Probit are S-shaped - always in [0, 1]\")\n",
    "print(\"✓ Logit has slightly heavier tails than Probit\")\n",
    "print(\"✓ Near the middle (z ≈ 0), all three are similar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9929a22",
   "metadata": {},
   "source": [
    "### Marginal Effects (Partial Effects)\n",
    "\n",
    "The **coefficients** in logit/probit are NOT marginal effects! We need to calculate:\n",
    "\n",
    "$$ \\frac{\\partial P(y=1|\\mathbf{x})}{\\partial x_j} = g(\\mathbf{x}'\\boldsymbol{\\beta}) \\cdot \\beta_j $$\n",
    "\n",
    "where $g(\\cdot) = G'(\\cdot)$ is the PDF:\n",
    "- **Logit**: $g(z) = \\frac{e^z}{(1 + e^z)^2} = \\Lambda(z)[1 - \\Lambda(z)]$\n",
    "- **Probit**: $g(z) = \\phi(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}$\n",
    "\n",
    "**Key insight**: Marginal effects **depend on** $\\mathbf{x}$ (evaluated at specific values)!\n",
    "\n",
    "**Common approaches**:\n",
    "1. **Marginal Effect at the Mean (MEM)**: Evaluate at $\\bar{\\mathbf{x}}$\n",
    "2. **Average Marginal Effect (AME)**: Average over all observations\n",
    "3. **Marginal Effect at Representative Values (MER)**: Choose interesting cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a1ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MARGINAL EFFECTS FOR BINARY MODELS\n",
    "print(\"\\n\\n17.1.5 MARGINAL EFFECTS (PARTIAL EFFECTS)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Coefficients in logit/probit are NOT marginal effects!\")\n",
    "print(\"Must calculate: ∂P(y=1|x)/∂xⱼ = g(x'β)·βⱼ\")\n",
    "\n",
    "# Average Marginal Effects (AME) for Logit\n",
    "print(\"\\nLOGIT: Average Marginal Effects (AME)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Get fitted values (predicted probabilities)\n",
    "logit_fitted = logit.predict(mroz)\n",
    "\n",
    "# Logit PDF: g(z) = Λ(z)[1 - Λ(z)]\n",
    "logit_pdf_values = logit_fitted * (1 - logit_fitted)\n",
    "\n",
    "# Calculate AME for each coefficient\n",
    "ame_logit = {}\n",
    "for var in logit.params.index:\n",
    "    if var == \"Intercept\":\n",
    "        continue\n",
    "    # For I(exper**2), need special handling\n",
    "    if var == \"I(exper ** 2)\":\n",
    "        ame_logit[var] = (logit_pdf_values * logit.params[var]).mean()\n",
    "    else:\n",
    "        ame_logit[var] = (logit_pdf_values * logit.params[var]).mean()\n",
    "\n",
    "ame_logit_df = pd.DataFrame(\n",
    "    {\"AME\": ame_logit, \"Logit Coef\": logit.params.drop(\"Intercept\")},\n",
    ")\n",
    "\n",
    "display(ame_logit_df.round(4))\n",
    "\n",
    "print(\"\\nINTERPRETATION of Average Marginal Effects:\")\n",
    "print(f\"  educ AME: {ame_logit['educ']:.4f}\")\n",
    "print(\n",
    "    f\"    → 1 more year of education increases participation probability by {100 * ame_logit['educ']:.2f} percentage points (on average)\"\n",
    ")\n",
    "print(f\"  kidslt6 AME: {ame_logit['kidslt6']:.4f}\")\n",
    "print(\n",
    "    f\"    → 1 more young child decreases participation probability by {abs(100 * ame_logit['kidslt6']):.2f} percentage points (on average)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b708d3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Marginal Effects (AME) for Probit\n",
    "print(\"\\nPROBIT: Average Marginal Effects (AME)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Get fitted index values\n",
    "probit_index = probit.predict(mroz, linear=True)\n",
    "\n",
    "# Probit PDF: g(z) = φ(z) = standard normal PDF\n",
    "probit_pdf_values = stats.norm.pdf(probit_index)\n",
    "\n",
    "# Calculate AME for each coefficient\n",
    "ame_probit = {}\n",
    "for var in probit.params.index:\n",
    "    if var == \"Intercept\":\n",
    "        continue\n",
    "    ame_probit[var] = (probit_pdf_values * probit.params[var]).mean()\n",
    "\n",
    "ame_probit_df = pd.DataFrame(\n",
    "    {\n",
    "        \"AME\": ame_probit,\n",
    "        \"Probit Coef\": probit.params.drop(\"Intercept\"),\n",
    "        \"Logit AME\": ame_logit,\n",
    "    },\n",
    ")\n",
    "\n",
    "display(ame_probit_df.round(4))\n",
    "\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(\"1. Logit and Probit AMEs are very similar\")\n",
    "print(\"2. AMEs are comparable to LPM coefficients\")\n",
    "print(\"3. AMEs have intuitive interpretation (percentage point changes)\")\n",
    "print(\"4. For small changes, AME ≈ LPM coefficient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ac0d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how marginal effects vary with x\n",
    "print(\"\\nMARGINAL EFFECTS DEPEND ON X VALUES!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Simulate data to show variation\n",
    "np.random.seed(42)\n",
    "y_sim = stats.binom.rvs(1, 0.5, size=100)\n",
    "x_sim = stats.norm.rvs(0, 1, size=100) + 2 * y_sim\n",
    "sim_data = pd.DataFrame({\"y\": y_sim, \"x\": x_sim})\n",
    "\n",
    "# Estimate three models\n",
    "reg_lin_sim = smf.ols(formula=\"y ~ x\", data=sim_data).fit()\n",
    "reg_logit_sim = smf.logit(formula=\"y ~ x\", data=sim_data).fit(disp=0)\n",
    "reg_probit_sim = smf.probit(formula=\"y ~ x\", data=sim_data).fit(disp=0)\n",
    "\n",
    "# Calculate marginal effects across range of x\n",
    "x_range = np.linspace(x_sim.min(), x_sim.max(), 100)\n",
    "\n",
    "# LPM: constant marginal effect\n",
    "me_lin = np.repeat(reg_lin_sim.params[\"x\"], 100)\n",
    "\n",
    "# Logit: g(xβ) * β\n",
    "xb_logit_range = reg_logit_sim.params[\"Intercept\"] + reg_logit_sim.params[\"x\"] * x_range\n",
    "prob_logit = 1 / (1 + np.exp(-xb_logit_range))\n",
    "me_logit = prob_logit * (1 - prob_logit) * reg_logit_sim.params[\"x\"]\n",
    "\n",
    "# Probit: φ(xβ) * β\n",
    "xb_probit_range = (\n",
    "    reg_probit_sim.params[\"Intercept\"] + reg_probit_sim.params[\"x\"] * x_range\n",
    ")\n",
    "me_probit = stats.norm.pdf(xb_probit_range) * reg_probit_sim.params[\"x\"]\n",
    "\n",
    "# Plot marginal effects\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(x_range, me_lin, \"r--\", linewidth=2, label=\"LPM (constant)\")\n",
    "ax.plot(x_range, me_logit, \"b-\", linewidth=2, label=\"Logit (varies)\")\n",
    "ax.plot(x_range, me_probit, \"g-\", linewidth=2, label=\"Probit (varies)\")\n",
    "ax.axhline(y=0, color=\"black\", linestyle=\":\", linewidth=1)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"∂P(y=1|x)/∂x\")\n",
    "ax.set_title(\"Marginal Effects: Constant (LPM) vs Varying (Logit/Probit)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY TAKEAWAY:\")\n",
    "print(\"✓ LPM: Marginal effect is constant (same for all x)\")\n",
    "print(\"✓ Logit/Probit: Marginal effect varies with x\")\n",
    "print(\"✓ Largest effects near middle, smaller at extremes\")\n",
    "print(\"✓ Must specify which x values when reporting marginal effects!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21c1f5f",
   "metadata": {},
   "source": [
    "## 17.2 Fractional Response Models\n",
    "\n",
    "When the dependent variable is a **fraction** or **proportion** (between 0 and 1), but not necessarily binary:\n",
    "\n",
    "Examples:\n",
    "- Savings rate (fraction of income saved)\n",
    "- Portfolio allocation (fraction in stocks)\n",
    "- Participation rate (fraction of eligible individuals participating)\n",
    "\n",
    "### Fractional Logit Model\n",
    "\n",
    "Even though $y$ is not binary, we can use:\n",
    "\n",
    "$$ E(y | \\mathbf{x}) = G(\\mathbf{x}'\\boldsymbol{\\beta}) $$\n",
    "\n",
    "where $G$ is the logistic CDF. This is a **quasi-MLE** approach:\n",
    "- Estimator is **consistent** even if $G$ is not the true conditional mean\n",
    "- Standard errors should be **robust** (heteroskedasticity is inherent)\n",
    "\n",
    "### Key Advantage\n",
    "\n",
    "Predictions are **automatically bounded** in (0, 1), unlike OLS which can produce nonsensical predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9422a62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FRACTIONAL RESPONSE EXAMPLE (Conceptual)\n",
    "print(\"\\n17.2 FRACTIONAL RESPONSE MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"USE CASE: Dependent variable is a proportion/fraction between 0 and 1\")\n",
    "print(\"\\nEXAMPLES:\")\n",
    "print(\"  - Savings rate: fraction of income saved\")\n",
    "print(\"  - Portfolio share: fraction invested in stocks\")\n",
    "print(\"  - 401(k) participation rate at firm level\")\n",
    "\n",
    "print(\"\\nMODEL:\")\n",
    "print(\"  E(y|x) = Λ(x'β) where 0 < y < 1\")\n",
    "print(\"  Estimated by quasi-MLE (same as logit)\")\n",
    "\n",
    "print(\"\\nADVANTAGES:\")\n",
    "print(\"  ✓ Predictions automatically in (0, 1)\")\n",
    "print(\"  ✓ Handles corner solutions (y = 0 or y = 1) naturally\")\n",
    "print(\"  ✓ Consistent estimator even if model misspecified\")\n",
    "\n",
    "print(\"\\nESTIMATION:\")\n",
    "print(\"  1. Use logit estimation with fractional y\")\n",
    "print(\"  2. Must use robust standard errors\")\n",
    "print(\"  3. Interpret via average marginal effects\")\n",
    "\n",
    "print(\"\\nNOTE: In Python/statsmodels, use smf.logit() with continuous y ∈ (0,1)\")\n",
    "print(\"      Some observations can be exactly 0 or 1 (corner solutions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a3ca58",
   "metadata": {},
   "source": [
    "## 17.3 An Exponential Mean Model and Poisson Regression\n",
    "\n",
    "When the dependent variable is a **count** (non-negative integer):\n",
    "\n",
    "Examples:\n",
    "- Number of arrests\n",
    "- Number of patents filed\n",
    "- Number of doctor visits\n",
    "- Number of children\n",
    "\n",
    "### Why Not OLS?\n",
    "\n",
    "1. $y$ is **discrete** (not continuous)\n",
    "2. $y \\geq 0$ (OLS can predict negative values)\n",
    "3. **Variance increases with mean** (heteroskedasticity)\n",
    "\n",
    "### Poisson Regression Model\n",
    "\n",
    "Assume $y | \\mathbf{x}$ follows a **Poisson distribution**:\n",
    "\n",
    "$$ P(y = h | \\mathbf{x}) = \\frac{e^{-\\mu} \\mu^h}{h!}, \\quad h = 0, 1, 2, \\ldots $$\n",
    "\n",
    "where the **mean** (and variance) is:\n",
    "\n",
    "$$ \\mu = E(y | \\mathbf{x}) = \\exp(\\mathbf{x}'\\boldsymbol{\\beta}) $$\n",
    "\n",
    "**Key feature**: Exponential mean ensures $\\mu > 0$ always!\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "$$ \\log E(y | \\mathbf{x}) = \\mathbf{x}'\\boldsymbol{\\beta} $$\n",
    "\n",
    "So $\\beta_j$ is the **semi-elasticity**:\n",
    "\n",
    "$$ \\frac{\\partial \\log E(y|\\mathbf{x})}{\\partial x_j} = \\beta_j $$\n",
    "\n",
    "**Interpretation**: A one-unit increase in $x_j$ changes $E(y|\\mathbf{x})$ by approximately $100 \\cdot \\beta_j$ percent.\n",
    "\n",
    "### Marginal Effects\n",
    "\n",
    "$$ \\frac{\\partial E(y|\\mathbf{x})}{\\partial x_j} = \\beta_j \\cdot \\exp(\\mathbf{x}'\\boldsymbol{\\beta}) = \\beta_j \\cdot E(y|\\mathbf{x}) $$\n",
    "\n",
    "Marginal effect is **proportional** to the predicted mean!\n",
    "\n",
    "### Example 17.3: Number of Arrests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa0295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load arrests data\n",
    "crime1 = wool.data(\"crime1\")\n",
    "\n",
    "print(\"\\n17.3 POISSON REGRESSION FOR COUNT DATA\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nEXAMPLE: Number of times arrested in 1986\")\n",
    "\n",
    "print(f\"\\nTotal observations: {len(crime1)}\")\n",
    "print(\"\\nDEPENDENT VARIABLE:\")\n",
    "print(\"  narr86 = number of times arrested in 1986\")\n",
    "\n",
    "# Distribution of arrests\n",
    "arrest_dist = crime1[\"narr86\"].value_counts().sort_index()\n",
    "print(\"\\nDISTRIBUTION OF ARRESTS:\")\n",
    "display(arrest_dist.head(10))\n",
    "\n",
    "print(f\"\\nMean arrests: {crime1['narr86'].mean():.3f}\")\n",
    "print(f\"Variance: {crime1['narr86'].var():.3f}\")\n",
    "print(f\"Variance/Mean ratio: {crime1['narr86'].var() / crime1['narr86'].mean():.3f}\")\n",
    "print(\"(Poisson assumes variance = mean; ratio > 1 suggests overdispersion)\")\n",
    "\n",
    "print(\"\\nEXPLANATORY VARIABLES:\")\n",
    "print(\"  pcnv     = proportion of prior arrests leading to conviction\")\n",
    "print(\"  avgsen   = average sentence length served (months)\")\n",
    "print(\"  tottime  = total time served in prison (months)\")\n",
    "print(\"  ptime86  = months in prison during 1986\")\n",
    "print(\"  qemp86   = quarters employed in 1986\")\n",
    "print(\"  inc86    = legal income in 1986 ($100s)\")\n",
    "print(\"  black    = 1 if black\")\n",
    "print(\"  hispan   = 1 if Hispanic\")\n",
    "print(\"  born60   = 1 if born in 1960\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68feb3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS for comparison\n",
    "print(\"\\nLINEAR MODEL (OLS) - For Comparison\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "ols_count = smf.ols(\n",
    "    formula=\"narr86 ~ pcnv + avgsen + tottime + ptime86 + qemp86 + inc86 + black + hispan + born60\",\n",
    "    data=crime1,\n",
    ").fit()\n",
    "\n",
    "table_ols = pd.DataFrame(\n",
    "    {\n",
    "        \"Coefficient\": ols_count.params,\n",
    "        \"Std. Error\": ols_count.bse,\n",
    "        \"t-statistic\": ols_count.tvalues,\n",
    "        \"p-value\": ols_count.pvalues,\n",
    "    },\n",
    ")\n",
    "\n",
    "display(table_ols.round(4))\n",
    "\n",
    "print(f\"\\nR-squared: {ols_count.rsquared:.4f}\")\n",
    "\n",
    "# Check for negative predictions\n",
    "ols_pred = ols_count.predict(crime1)\n",
    "n_negative = (ols_pred < 0).sum()\n",
    "print(f\"\\nPredictions < 0: {n_negative} ({100 * n_negative / len(crime1):.1f}%)\")\n",
    "if n_negative > 0:\n",
    "    print(\"✗ OLS can predict negative arrests!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad052d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poisson regression\n",
    "print(\"\\n\\nPOISSON REGRESSION MODEL\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Model: E(narr86|x) = exp(β₀ + β₁·pcnv + β₂·avgsen + ...)\")\n",
    "\n",
    "poisson = smf.poisson(\n",
    "    formula=\"narr86 ~ pcnv + avgsen + tottime + ptime86 + qemp86 + inc86 + black + hispan + born60\",\n",
    "    data=crime1,\n",
    ").fit(disp=0)\n",
    "\n",
    "table_poisson = pd.DataFrame(\n",
    "    {\n",
    "        \"Coefficient\": poisson.params,\n",
    "        \"Std. Error\": poisson.bse,\n",
    "        \"z-statistic\": poisson.tvalues,\n",
    "        \"p-value\": poisson.pvalues,\n",
    "    },\n",
    ")\n",
    "\n",
    "display(table_poisson.round(4))\n",
    "\n",
    "print(f\"\\nLog-Likelihood: {poisson.llf:.4f}\")\n",
    "print(f\"AIC: {poisson.aic:.4f}\")\n",
    "\n",
    "print(\"\\nINTERPRETATION (Semi-Elasticities):\")\n",
    "print(f\"  pcnv: {poisson.params['pcnv']:.4f}\")\n",
    "print(\n",
    "    f\"    → 0.1 increase in conviction rate → {100 * poisson.params['pcnv'] * 0.1:.2f}% change in expected arrests\"\n",
    ")\n",
    "print(f\"  qemp86: {poisson.params['qemp86']:.4f}\")\n",
    "print(\n",
    "    f\"    → 1 more quarter employed → {100 * poisson.params['qemp86']:.2f}% change in expected arrests\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103cf449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quasi-Poisson (overdispersion correction)\n",
    "print(\"\\n\\nQUASI-POISSON (Overdispersion Correction)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"When Var(y|x) > E(y|x), use quasi-Poisson to adjust standard errors\")\n",
    "\n",
    "quasi_poisson = smf.glm(\n",
    "    formula=\"narr86 ~ pcnv + avgsen + tottime + ptime86 + qemp86 + inc86 + black + hispan + born60\",\n",
    "    family=sm.families.Poisson(),\n",
    "    data=crime1,\n",
    ").fit(scale=\"X2\", disp=0)\n",
    "\n",
    "table_qpoisson = pd.DataFrame(\n",
    "    {\n",
    "        \"Coefficient\": quasi_poisson.params,\n",
    "        \"Std. Error\": quasi_poisson.bse,\n",
    "        \"z-statistic\": quasi_poisson.tvalues,\n",
    "        \"p-value\": quasi_poisson.pvalues,\n",
    "    },\n",
    ")\n",
    "\n",
    "display(table_qpoisson.round(4))\n",
    "\n",
    "print(f\"\\nDispersion parameter: {quasi_poisson.scale:.4f}\")\n",
    "print(\"(> 1 confirms overdispersion)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18890d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three models\n",
    "print(\"\\n\\nCOMPARISON: OLS vs POISSON vs QUASI-POISSON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "comparison_count = pd.DataFrame(\n",
    "    {\n",
    "        \"OLS\": ols_count.params,\n",
    "        \"OLS SE\": ols_count.bse,\n",
    "        \"Poisson\": poisson.params,\n",
    "        \"Poisson SE\": poisson.bse,\n",
    "        \"Quasi-Poisson\": quasi_poisson.params,\n",
    "        \"Quasi-Poisson SE\": quasi_poisson.bse,\n",
    "    },\n",
    ")\n",
    "\n",
    "display(comparison_count.round(4))\n",
    "\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(\"1. Poisson coefficients are semi-elasticities (not levels like OLS)\")\n",
    "print(\"2. Quasi-Poisson has larger SEs (accounts for overdispersion)\")\n",
    "print(\"3. Pattern of significance similar across models\")\n",
    "print(\"4. Poisson ensures non-negative predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1f5ebf",
   "metadata": {},
   "source": [
    "## 17.4 The Tobit Model for Corner Solution Responses\n",
    "\n",
    "A **corner solution** occurs when the dependent variable:\n",
    "- Takes value **zero for a substantial fraction** of observations\n",
    "- Takes **positive continuous values** for the rest\n",
    "\n",
    "Examples:\n",
    "- **Hours worked**: Many people don't work (hours = 0), others work positive hours\n",
    "- **Charitable contributions**: Many give nothing, others give positive amounts\n",
    "- **Savings**: Some save nothing, others save positive amounts\n",
    "\n",
    "### Why Not OLS?\n",
    "\n",
    "OLS treats zeros as just another value, but zeros are **qualitatively different** (corner solutions from optimization, not \"small positive values\").\n",
    "\n",
    "### Tobit Model (Type I Tobit / Censored Regression)\n",
    "\n",
    "**Latent variable** model:\n",
    "\n",
    "$$ y^* = \\mathbf{x}'\\boldsymbol{\\beta} + u, \\quad u | \\mathbf{x} \\sim N(0, \\sigma^2) $$\n",
    "\n",
    "**Observation rule**:\n",
    "\n",
    "$$ y = \\max(0, y^*) = \\begin{cases} y^* & \\text{if } y^* > 0 \\\\ 0 & \\text{if } y^* \\leq 0 \\end{cases} $$\n",
    "\n",
    "Think of $y^*$ as **desired hours** (can be negative = don't want to work).\n",
    "We observe $y$ = **actual hours** (cannot be negative).\n",
    "\n",
    "### Likelihood Function\n",
    "\n",
    "Two parts:\n",
    "\n",
    "1. **For $y = 0$**: $P(y = 0 | \\mathbf{x}) = P(y^* \\leq 0 | \\mathbf{x}) = \\Phi\\left(-\\frac{\\mathbf{x}'\\boldsymbol{\\beta}}{\\sigma}\\right)$\n",
    "\n",
    "2. **For $y > 0$**: $f(y | \\mathbf{x}) = \\frac{1}{\\sigma} \\phi\\left(\\frac{y - \\mathbf{x}'\\boldsymbol{\\beta}}{\\sigma}\\right)$\n",
    "\n",
    "Log-likelihood:\n",
    "\n",
    "$$ \\ell(\\boldsymbol{\\beta}, \\sigma) = \\sum_{y_i=0} \\log \\Phi\\left(-\\frac{\\mathbf{x}_i'\\boldsymbol{\\beta}}{\\sigma}\\right) + \\sum_{y_i>0} \\left[\\log \\phi\\left(\\frac{y_i - \\mathbf{x}_i'\\boldsymbol{\\beta}}{\\sigma}\\right) - \\log \\sigma\\right] $$\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "**Marginal effects** are complex:\n",
    "\n",
    "1. **Effect on latent variable**: $\\frac{\\partial E(y^* | \\mathbf{x})}{\\partial x_j} = \\beta_j$\n",
    "\n",
    "2. **Effect on observed variable**: \n",
    "$$ \\frac{\\partial E(y | \\mathbf{x})}{\\partial x_j} = \\Phi\\left(\\frac{\\mathbf{x}'\\boldsymbol{\\beta}}{\\sigma}\\right) \\cdot \\beta_j $$\n",
    "\n",
    "3. **Effect on positive outcomes** (given $y > 0$):\n",
    "$$ \\frac{\\partial E(y | \\mathbf{x}, y > 0)}{\\partial x_j} = \\left[1 - \\lambda\\left(\\frac{\\mathbf{x}'\\boldsymbol{\\beta}}{\\sigma}\\right) \\cdot \\frac{\\mathbf{x}'\\boldsymbol{\\beta}}{\\sigma}\\right] \\beta_j $$\n",
    "\n",
    "where $\\lambda(z) = \\phi(z) / \\Phi(z)$ is the **inverse Mills ratio**.\n",
    "\n",
    "### Example 17.2: Married Women's Hours Worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46700f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (all married women, including non-workers)\n",
    "mroz = wool.data(\"mroz\")\n",
    "\n",
    "print(\"\\n17.4 TOBIT MODEL FOR CORNER SOLUTION RESPONSES\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nEXAMPLE: Annual hours worked by married women\")\n",
    "\n",
    "print(f\"\\nTotal observations: {len(mroz)}\")\n",
    "print(\n",
    "    f\"Women with hours = 0: {(mroz['hours'] == 0).sum()} ({100 * (mroz['hours'] == 0).mean():.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"Women with hours > 0: {(mroz['hours'] > 0).sum()} ({100 * (mroz['hours'] > 0).mean():.1f}%)\"\n",
    ")\n",
    "\n",
    "print(\"\\nDEPENDENT VARIABLE:\")\n",
    "print(\"  hours = annual hours worked\")\n",
    "print(\"  Corner solution: hours ≥ 0, with many zeros\")\n",
    "\n",
    "print(\"\\nEXPLANATORY VARIABLES:\")\n",
    "print(\"  nwifeinc = husband's income\")\n",
    "print(\"  educ     = education\")\n",
    "print(\"  exper    = experience\")\n",
    "print(\"  age      = age\")\n",
    "print(\"  kidslt6  = number of young children\")\n",
    "print(\"  kidsge6  = number of older children\")\n",
    "\n",
    "# Distribution of hours\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram of all hours (including zeros)\n",
    "ax1.hist(mroz[\"hours\"], bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "ax1.axvline(x=0, color=\"red\", linestyle=\"--\", linewidth=2, label=\"Corner at 0\")\n",
    "ax1.set_xlabel(\"Annual Hours Worked\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.set_title(f\"Distribution of Hours (n={len(mroz)})\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram of positive hours only\n",
    "hours_positive = mroz[mroz[\"hours\"] > 0][\"hours\"]\n",
    "ax2.hist(hours_positive, bins=50, edgecolor=\"black\", alpha=0.7, color=\"green\")\n",
    "ax2.set_xlabel(\"Annual Hours Worked\")\n",
    "ax2.set_ylabel(\"Frequency\")\n",
    "ax2.set_title(\n",
    "    f\"Distribution of Hours (Conditional on Working, n={len(hours_positive)})\"\n",
    ")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSummary statistics (all women):\")\n",
    "display(mroz[\"hours\"].describe().round(2))\n",
    "\n",
    "print(\"\\nSummary statistics (working women only):\")\n",
    "display(hours_positive.describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c2847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS for comparison (wrong!)\n",
    "print(\"\\nOLS REGRESSION (WRONG - Ignores Corner Solution)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ols_hours = smf.ols(\n",
    "    formula=\"hours ~ nwifeinc + educ + exper + I(exper**2) + age + kidslt6 + kidsge6\",\n",
    "    data=mroz,\n",
    ").fit()\n",
    "\n",
    "table_ols_hours = pd.DataFrame(\n",
    "    {\n",
    "        \"Coefficient\": ols_hours.params,\n",
    "        \"Std. Error\": ols_hours.bse,\n",
    "        \"t-statistic\": ols_hours.tvalues,\n",
    "        \"p-value\": ols_hours.pvalues,\n",
    "    },\n",
    ")\n",
    "\n",
    "display(table_ols_hours.round(4))\n",
    "\n",
    "print(f\"\\nR-squared: {ols_hours.rsquared:.4f}\")\n",
    "print(\"\\nPROBLEM: OLS treats 0 hours like any other value\")\n",
    "print(\"         Doesn't account for corner solution nature of the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c498d6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tobit model - custom MLE class\n",
    "print(\"\\n\\nTOBIT MODEL\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Model: y* = x'β + u, u ~ N(0, σ²)\")\n",
    "print(\"       y = max(0, y*)\")\n",
    "\n",
    "# Prepare data using patsy\n",
    "y, X = pt.dmatrices(\n",
    "    \"hours ~ nwifeinc + educ + exper + I(exper**2) + age + kidslt6 + kidsge6\",\n",
    "    data=mroz,\n",
    "    return_type=\"dataframe\",\n",
    ")\n",
    "\n",
    "# Get starting values from OLS\n",
    "reg_ols_start = smf.ols(\n",
    "    formula=\"hours ~ nwifeinc + educ + exper + I(exper**2) + age + kidslt6 + kidsge6\",\n",
    "    data=mroz,\n",
    ").fit()\n",
    "\n",
    "sigma_start = np.log(sum(reg_ols_start.resid**2) / len(reg_ols_start.resid))\n",
    "params_start = np.concatenate(\n",
    "    (np.array(reg_ols_start.params), [sigma_start]), axis=None\n",
    ")\n",
    "\n",
    "\n",
    "# Define Tobit model class\n",
    "class Tobit(GenericLikelihoodModel):\n",
    "    \"\"\"Tobit model for corner solution responses.\"\"\"\n",
    "\n",
    "    def nloglikeobs(self, params):\n",
    "        \"\"\"Negative log-likelihood per observation for Tobit model.\n",
    "\n",
    "        For details see Wooldridge (2019), formula 17.22:\n",
    "        - For y = 0: contribution is log[Φ(-xβ/σ)]\n",
    "        - For y > 0: contribution is log[φ((y-xβ)/σ)] - log(σ)\n",
    "        \"\"\"\n",
    "        X = self.exog\n",
    "        y = self.endog\n",
    "        p = X.shape[1]\n",
    "\n",
    "        # Extract parameters\n",
    "        beta = params[0:p]\n",
    "        sigma = np.exp(params[p])  # Ensure σ > 0\n",
    "\n",
    "        # Predicted values\n",
    "        y_hat = np.dot(X, beta)\n",
    "\n",
    "        # Identify censored and uncensored observations\n",
    "        y_eq = y == 0  # Censored at zero\n",
    "        y_g = y > 0  # Uncensored\n",
    "\n",
    "        # Initialize log-likelihood array\n",
    "        ll = np.empty(len(y))\n",
    "\n",
    "        # Censored observations: log[Φ(-xβ/σ)]\n",
    "        ll[y_eq] = np.log(stats.norm.cdf(-y_hat[y_eq] / sigma))\n",
    "\n",
    "        # Uncensored observations: log[φ((y-xβ)/σ)] - log(σ)\n",
    "        ll[y_g] = np.log(stats.norm.pdf((y - y_hat)[y_g] / sigma)) - np.log(sigma)\n",
    "\n",
    "        # Return negative log-likelihood\n",
    "        return -ll\n",
    "\n",
    "\n",
    "# Estimate Tobit model\n",
    "print(\"\\nEstimating Tobit model via Maximum Likelihood...\")\n",
    "tobit = Tobit(endog=y, exog=X)\n",
    "tobit_results = tobit.fit(start_params=params_start, maxiter=10000, disp=0)\n",
    "\n",
    "print(\"\\nTOBIT MODEL RESULTS:\")\n",
    "print(tobit_results.summary())\n",
    "\n",
    "# Extract sigma\n",
    "sigma_hat = np.exp(tobit_results.params[-1])\n",
    "print(f\"\\nEstimated σ: {sigma_hat:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e27246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare OLS and Tobit\n",
    "print(\"\\nCOMPARISON: OLS vs TOBIT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get Tobit parameters (exclude sigma)\n",
    "tobit_coef = tobit_results.params[:-1]\n",
    "tobit_se = tobit_results.bse[:-1]\n",
    "\n",
    "comparison_tobit = pd.DataFrame(\n",
    "    {\n",
    "        \"OLS\": ols_hours.params,\n",
    "        \"OLS SE\": ols_hours.bse,\n",
    "        \"Tobit\": tobit_coef,\n",
    "        \"Tobit SE\": tobit_se,\n",
    "    },\n",
    ")\n",
    "\n",
    "display(comparison_tobit.round(4))\n",
    "\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(\"1. Tobit coefficients are typically LARGER than OLS\")\n",
    "print(\"2. Tobit accounts for corner solution (selection into working)\")\n",
    "print(\"3. OLS suffers from 'attenuation bias' (biased toward zero)\")\n",
    "print(\"4. Tobit interpretation: effect on LATENT variable y*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7c9a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate marginal effects\n",
    "print(\"\\n\\nTOBIT MARGINAL EFFECTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Marginal effect on E(y|x) = Φ(xβ/σ) * β\n",
    "# Calculate at sample means\n",
    "\n",
    "X_mean = X.mean(axis=0).values\n",
    "xb_mean = np.dot(X_mean, tobit_coef)\n",
    "prob_positive = stats.norm.cdf(xb_mean / sigma_hat)\n",
    "\n",
    "print(\"At sample means:\")\n",
    "print(f\"  x'β = {xb_mean:.4f}\")\n",
    "print(f\"  Φ(x'β/σ) = {prob_positive:.4f}\")\n",
    "\n",
    "# Marginal effects on E(y|x)\n",
    "me_tobit = prob_positive * tobit_coef\n",
    "\n",
    "me_tobit_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Tobit Coef (∂y*/∂x)\": tobit_coef,\n",
    "        \"ME on E(y|x)\": me_tobit,\n",
    "        \"OLS Coef\": ols_hours.params,\n",
    "    },\n",
    ")\n",
    "\n",
    "display(me_tobit_df.round(4))\n",
    "\n",
    "print(\"\\nINTERPRETATION:\")\n",
    "educ_idx = list(X.columns).index(\"educ\")\n",
    "kidslt6_idx = list(X.columns).index(\"kidslt6\")\n",
    "\n",
    "print(f\"  educ (Tobit coef): {tobit_coef[educ_idx]:.2f}\")\n",
    "print(\n",
    "    f\"    → 1 more year of education increases y* by {tobit_coef[educ_idx]:.2f} hours\"\n",
    ")\n",
    "print(f\"  educ (ME on E(y|x)): {me_tobit[educ_idx]:.2f}\")\n",
    "print(\n",
    "    f\"    → 1 more year of education increases E(hours|x) by {me_tobit[educ_idx]:.2f} hours\"\n",
    ")\n",
    "print(\n",
    "    \"    → Accounts for both: (1) more likely to work, (2) work more hours if working\"\n",
    ")\n",
    "\n",
    "print(f\"\\n  kidslt6 (Tobit coef): {tobit_coef[kidslt6_idx]:.2f}\")\n",
    "print(\n",
    "    f\"    → 1 more young child decreases y* by {abs(tobit_coef[kidslt6_idx]):.2f} hours\"\n",
    ")\n",
    "print(f\"  kidslt6 (ME on E(y|x)): {me_tobit[kidslt6_idx]:.2f}\")\n",
    "print(\n",
    "    f\"    → 1 more young child decreases E(hours|x) by {abs(me_tobit[kidslt6_idx]):.2f} hours\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558fa9da",
   "metadata": {},
   "source": [
    "## 17.5 Censored and Truncated Regression Models\n",
    "\n",
    "**Censoring** and **truncation** both involve incomplete observation, but they differ fundamentally:\n",
    "\n",
    "### Censored Regression\n",
    "\n",
    "**Censoring**: We observe **all** individuals, but the dependent variable is **censored** (cut off) at some value.\n",
    "\n",
    "Example: **Duration until re-arrest** (recidivism study)\n",
    "- Some ex-convicts are re-arrested during study period → observe exact duration\n",
    "- Others are NOT re-arrested by study end → observe only that duration > T (right-censored)\n",
    "\n",
    "**Censored regression model** (similar to Tobit):\n",
    "\n",
    "$$ y^* = \\mathbf{x}'\\boldsymbol{\\beta} + u, \\quad u \\sim N(0, \\sigma^2) $$\n",
    "\n",
    "$$ y = \\begin{cases} y^* & \\text{if } y^* < c \\text{ (uncensored)} \\\\ c & \\text{if } y^* \\geq c \\text{ (censored at } c) \\end{cases} $$\n",
    "\n",
    "**Key**: We know which observations are censored!\n",
    "\n",
    "### Truncated Regression\n",
    "\n",
    "**Truncation**: We **only observe** individuals for whom $y$ satisfies some condition.\n",
    "\n",
    "Example: **Wage study of working women**\n",
    "- Only observe wages for women who work (hours > 0)\n",
    "- Don't observe non-workers at all (missing from sample)\n",
    "\n",
    "**Truncated regression** accounts for non-random sampling:\n",
    "\n",
    "$$ f(y | \\mathbf{x}, y > 0) = \\frac{\\phi\\left(\\frac{y - \\mathbf{x}'\\boldsymbol{\\beta}}{\\sigma}\\right) / \\sigma}{\\Phi\\left(\\frac{\\mathbf{x}'\\boldsymbol{\\beta}}{\\sigma}\\right)} $$\n",
    "\n",
    "**Key**: Sample is **selected** based on $y$!\n",
    "\n",
    "### Key Difference\n",
    "\n",
    "|  | **Censored** | **Truncated** |\n",
    "|---|---|---|\n",
    "| **Sample** | Observe all individuals | Only some individuals |\n",
    "| **Information** | Know who is censored | Don't know truncated observations |\n",
    "| **Example** | Prison duration (some ongoing) | Wages (only workers) |\n",
    "| **Correction** | Model censoring explicitly | Condition on truncation |\n",
    "\n",
    "### Example 17.4: Censored Regression (Recidivism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e907ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load recidivism data\n",
    "recid = wool.data(\"recid\")\n",
    "\n",
    "print(\"\\n17.5 CENSORED AND TRUNCATED REGRESSION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nEXAMPLE 17.4: Duration Until Re-Arrest (Censored Regression)\")\n",
    "\n",
    "print(f\"\\nTotal observations: {len(recid)}\")\n",
    "print(\"\\nDEPENDENT VARIABLE:\")\n",
    "print(\"  durat = months until re-arrest (or end of study)\")\n",
    "print(\"  ldurat = log(durat)\")\n",
    "\n",
    "# Check censoring\n",
    "print(\"\\nCENSORING:\")\n",
    "print(\n",
    "    f\"  Uncensored (re-arrested): {(recid['cens'] == 0).sum()} ({100 * (recid['cens'] == 0).mean():.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Censored (not re-arrested): {(recid['cens'] == 1).sum()} ({100 * (recid['cens'] == 1).mean():.1f}%)\"\n",
    ")\n",
    "\n",
    "print(\"\\nEXPLANATORY VARIABLES:\")\n",
    "print(\"  workprg = 1 if participated in work program\")\n",
    "print(\"  priors  = number of prior convictions\")\n",
    "print(\"  tserved = months served in prison\")\n",
    "print(\"  felon   = 1 if felony conviction\")\n",
    "print(\"  alcohol = 1 if alcohol problems\")\n",
    "print(\"  drugs   = 1 if drug history\")\n",
    "print(\"  black   = 1 if black\")\n",
    "print(\"  married = 1 if married\")\n",
    "print(\"  educ    = years of education\")\n",
    "print(\"  age     = age at release\")\n",
    "\n",
    "# Summary statistics\n",
    "key_vars = [\"ldurat\", \"workprg\", \"priors\", \"tserved\", \"felon\", \"age\", \"educ\"]\n",
    "display(recid[key_vars].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48dd594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize censoring\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram by censoring status\n",
    "uncensored = recid[recid[\"cens\"] == 0][\"ldurat\"]\n",
    "censored = recid[recid[\"cens\"] == 1][\"ldurat\"]\n",
    "\n",
    "axes[0].hist(\n",
    "    uncensored, bins=30, alpha=0.7, label=\"Uncensored (re-arrested)\", edgecolor=\"black\"\n",
    ")\n",
    "axes[0].hist(\n",
    "    censored, bins=30, alpha=0.7, label=\"Censored (not re-arrested)\", edgecolor=\"black\"\n",
    ")\n",
    "axes[0].set_xlabel(\"Log Duration (months)\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "axes[0].set_title(\"Distribution of Log Duration by Censoring Status\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter: duration vs priors\n",
    "axes[1].scatter(\n",
    "    recid[recid[\"cens\"] == 0][\"priors\"],\n",
    "    recid[recid[\"cens\"] == 0][\"ldurat\"],\n",
    "    alpha=0.5,\n",
    "    s=20,\n",
    "    label=\"Uncensored\",\n",
    ")\n",
    "axes[1].scatter(\n",
    "    recid[recid[\"cens\"] == 1][\"priors\"],\n",
    "    recid[recid[\"cens\"] == 1][\"ldurat\"],\n",
    "    alpha=0.5,\n",
    "    s=20,\n",
    "    color=\"red\",\n",
    "    marker=\"^\",\n",
    "    label=\"Censored\",\n",
    ")\n",
    "axes[1].set_xlabel(\"Number of Prior Convictions\")\n",
    "axes[1].set_ylabel(\"Log Duration (months)\")\n",
    "axes[1].set_title(\"Duration vs Priors (by censoring status)\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4a0e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Censored regression model\n",
    "print(\"\\nCENSORED REGRESSION MODEL\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Model: log(duration)* = x'β + u, u ~ N(0, σ²)\")\n",
    "print(\"       Observe log(duration) if re-arrested\")\n",
    "print(\"       Censored if not re-arrested (duration > study period)\")\n",
    "\n",
    "# Prepare data\n",
    "censored = recid[\"cens\"] != 0\n",
    "y, X = pt.dmatrices(\n",
    "    \"ldurat ~ workprg + priors + tserved + felon + alcohol + drugs + black + married + educ + age\",\n",
    "    data=recid,\n",
    "    return_type=\"dataframe\",\n",
    ")\n",
    "\n",
    "# Get starting values from OLS\n",
    "reg_ols_cens = smf.ols(\n",
    "    formula=\"ldurat ~ workprg + priors + tserved + felon + alcohol + drugs + black + married + educ + age\",\n",
    "    data=recid,\n",
    ").fit()\n",
    "\n",
    "sigma_start = np.log(sum(reg_ols_cens.resid**2) / len(reg_ols_cens.resid))\n",
    "params_start = np.concatenate((np.array(reg_ols_cens.params), [sigma_start]), axis=None)\n",
    "\n",
    "\n",
    "# Define censored regression model class\n",
    "class CensoredRegression(GenericLikelihoodModel):\n",
    "    \"\"\"Censored regression model for right-censored data.\"\"\"\n",
    "\n",
    "    def __init__(self, endog, cens, exog):\n",
    "        self.cens = cens\n",
    "        super().__init__(endog, exog, missing=\"none\")\n",
    "\n",
    "    def nloglikeobs(self, params):\n",
    "        \"\"\"Negative log-likelihood per observation.\n",
    "\n",
    "        For uncensored: standard normal density\n",
    "        For censored: probability of being above censoring point\n",
    "        \"\"\"\n",
    "        X = self.exog\n",
    "        y = self.endog\n",
    "        cens = self.cens\n",
    "        p = X.shape[1]\n",
    "\n",
    "        beta = params[0:p]\n",
    "        sigma = np.exp(params[p])\n",
    "        y_hat = np.dot(X, beta)\n",
    "\n",
    "        ll = np.empty(len(y))\n",
    "\n",
    "        # Uncensored: log[φ((y-xβ)/σ)] - log(σ)\n",
    "        ll[~cens] = np.log(stats.norm.pdf((y - y_hat)[~cens] / sigma)) - np.log(sigma)\n",
    "\n",
    "        # Censored: log[Φ(-(y-xβ)/σ)]\n",
    "        ll[cens] = np.log(stats.norm.cdf(-(y - y_hat)[cens] / sigma))\n",
    "\n",
    "        return -ll\n",
    "\n",
    "\n",
    "# Estimate censored regression\n",
    "print(\"\\nEstimating Censored Regression model...\")\n",
    "censreg = CensoredRegression(endog=y, exog=X, cens=censored)\n",
    "censreg_results = censreg.fit(\n",
    "    start_params=params_start, maxiter=10000, method=\"BFGS\", disp=0\n",
    ")\n",
    "\n",
    "print(\"\\nCENSORED REGRESSION RESULTS:\")\n",
    "print(censreg_results.summary())\n",
    "\n",
    "# Extract sigma\n",
    "sigma_censreg = np.exp(censreg_results.params[-1])\n",
    "print(f\"\\nEstimated σ: {sigma_censreg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f016cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare OLS and Censored Regression\n",
    "print(\"\\nCOMPARISON: OLS vs CENSORED REGRESSION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get censored regression parameters (exclude sigma)\n",
    "censreg_coef = censreg_results.params[:-1]\n",
    "censreg_se = censreg_results.bse[:-1]\n",
    "\n",
    "comparison_censreg = pd.DataFrame(\n",
    "    {\n",
    "        \"OLS\": reg_ols_cens.params,\n",
    "        \"OLS SE\": reg_ols_cens.bse,\n",
    "        \"Censored Reg\": censreg_coef,\n",
    "        \"Censored Reg SE\": censreg_se,\n",
    "    },\n",
    ")\n",
    "\n",
    "display(comparison_censreg.round(4))\n",
    "\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(\"1. Censored regression accounts for right-censoring\")\n",
    "print(\"2. OLS treats censored observations as if uncensored (biased)\")\n",
    "print(\"3. workprg effect: Work program increases time to re-arrest\")\n",
    "print(\"4. priors effect: More prior convictions → shorter time to re-arrest\")\n",
    "\n",
    "print(\"\\nINTERPRETATION:\")\n",
    "workprg_idx = list(X.columns).index(\"workprg\")\n",
    "priors_idx = list(X.columns).index(\"priors\")\n",
    "\n",
    "print(f\"  workprg: {censreg_coef[workprg_idx]:.4f}\")\n",
    "print(f\"    → Work program increases log(duration) by {censreg_coef[workprg_idx]:.4f}\")\n",
    "print(\n",
    "    f\"    → Increases duration by {100 * (np.exp(censreg_coef[workprg_idx]) - 1):.1f}%\"\n",
    ")\n",
    "print(f\"  priors: {censreg_coef[priors_idx]:.4f}\")\n",
    "print(\n",
    "    f\"    → 1 more prior conviction decreases log(duration) by {abs(censreg_coef[priors_idx]):.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"    → Decreases duration by {100 * (1 - np.exp(censreg_coef[priors_idx])):.1f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469c0470",
   "metadata": {},
   "source": [
    "## 17.6 Sample Selection Corrections\n",
    "\n",
    "**Sample selection bias** occurs when the sample is **not randomly selected** from the population, leading to biased estimates if not corrected.\n",
    "\n",
    "### Classic Example: Wage Equation for Women\n",
    "\n",
    "We want to estimate the wage equation:\n",
    "\n",
    "$$ \\log(\\text{wage}) = \\beta_0 + \\beta_1 \\text{educ} + \\beta_2 \\text{exper} + \\beta_3 \\text{exper}^2 + u $$\n",
    "\n",
    "**Problem**: We only observe wages for **women who work**! Selection into working is not random:\n",
    "\n",
    "$$ \\text{inlf} = 1[\\gamma_0 + \\gamma_1 \\text{educ} + \\cdots + \\gamma_k \\mathbf{z} + v > 0] $$\n",
    "\n",
    "If $\\text{Cov}(u, v) \\neq 0$, then OLS on the working sample is **biased**!\n",
    "\n",
    "**Intuition**: Women with high unobserved wage offers ($u > 0$) are more likely to work. In the working sample, $E(u | \\text{work}) \\neq 0$ → OLS is biased.\n",
    "\n",
    "### Heckman's Two-Step Procedure\n",
    "\n",
    "**Step 1**: Estimate **probit selection equation** using all observations:\n",
    "\n",
    "$$ \\text{Pr}(\\text{inlf} = 1 | \\mathbf{z}) = \\Phi(\\mathbf{z}'\\boldsymbol{\\gamma}) $$\n",
    "\n",
    "Compute the **inverse Mills ratio**:\n",
    "\n",
    "$$ \\hat{\\lambda}_i = \\frac{\\phi(\\mathbf{z}_i'\\hat{\\boldsymbol{\\gamma}})}{\\Phi(\\mathbf{z}_i'\\hat{\\boldsymbol{\\gamma}})} $$\n",
    "\n",
    "**Step 2**: Estimate **outcome equation** including $\\hat{\\lambda}_i$ as a regressor (using only working women):\n",
    "\n",
    "$$ \\log(\\text{wage}_i) = \\beta_0 + \\beta_1 \\text{educ}_i + \\beta_2 \\text{exper}_i + \\beta_3 \\text{exper}_i^2 + \\theta \\hat{\\lambda}_i + \\text{error} $$\n",
    "\n",
    "**Key**:\n",
    "- $\\hat{\\lambda}_i$ **controls for selection bias**\n",
    "- If $\\theta$ is significant → evidence of selection bias\n",
    "- If $\\theta \\approx 0$ → OLS on working sample is approximately unbiased\n",
    "\n",
    "### Identification\n",
    "\n",
    "For identification, need at least one variable in $\\mathbf{z}$ (selection equation) that's **excluded** from $\\mathbf{x}$ (outcome equation).\n",
    "\n",
    "**Exclusion restriction**: This variable affects **selection** but not the **outcome directly** (only through selection).\n",
    "\n",
    "Common choices:\n",
    "- **Non-labor income** (affects work decision, not wage offer)\n",
    "- **Number of young children** (affects work decision, not wage offer)\n",
    "- **Husband's characteristics** (for married women's labor supply)\n",
    "\n",
    "### Example 17.5: Heckman Selection Correction for Wages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20ac75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full sample (including non-workers)\n",
    "mroz = wool.data(\"mroz\")\n",
    "\n",
    "print(\"\\n17.6 SAMPLE SELECTION CORRECTIONS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nEXAMPLE: Wage equation for married women with selection correction\")\n",
    "\n",
    "print(f\"\\nTotal sample: {len(mroz)} married women\")\n",
    "print(\n",
    "    f\"Working women (observed wages): {mroz['inlf'].sum()} ({100 * mroz['inlf'].mean():.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"Non-working women (no wages): {(1 - mroz['inlf']).sum()} ({100 * (1 - mroz['inlf']).mean():.1f}%)\"\n",
    ")\n",
    "\n",
    "print(\"\\nPROBLEM: Can only estimate wage equation for working women\")\n",
    "print(\"         But selection into working is non-random!\")\n",
    "print(\"         → OLS on working sample is biased (sample selection bias)\")\n",
    "\n",
    "print(\"\\nSOLUTION: Heckman two-step procedure\")\n",
    "print(\"  Step 1: Estimate probit for work decision (all women)\")\n",
    "print(\"  Step 2: Estimate wage equation including inverse Mills ratio (working women)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2568bd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive OLS (wrong - ignores selection)\n",
    "print(\"\\nNAIVE OLS (Working Women Only - BIASED)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ols_wage = smf.ols(\n",
    "    formula=\"lwage ~ educ + exper + I(exper**2)\",\n",
    "    data=mroz[mroz[\"inlf\"] == 1],\n",
    ").fit()\n",
    "\n",
    "table_ols_wage = pd.DataFrame(\n",
    "    {\n",
    "        \"Coefficient\": ols_wage.params,\n",
    "        \"Std. Error\": ols_wage.bse,\n",
    "        \"t-statistic\": ols_wage.tvalues,\n",
    "        \"p-value\": ols_wage.pvalues,\n",
    "    },\n",
    ")\n",
    "\n",
    "display(table_ols_wage.round(4))\n",
    "\n",
    "print(f\"\\nR-squared: {ols_wage.rsquared:.4f}\")\n",
    "print(f\"Observations: {ols_wage.nobs:.0f} (working women only)\")\n",
    "\n",
    "print(\"\\nPROBLEM: This ignores sample selection!\")\n",
    "print(\"         Returns to education may be biased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f79ab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Probit for labor force participation\n",
    "print(\"\\n\\nHECKMAN STEP 1: Probit Selection Equation\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Estimate: P(inlf=1) = Φ(γ₀ + γ₁·educ + ... + γₖ·z)\")\n",
    "print(\"Using ALL women (workers and non-workers)\")\n",
    "\n",
    "probit_selection = smf.probit(\n",
    "    formula=\"inlf ~ educ + exper + I(exper**2) + nwifeinc + age + kidslt6 + kidsge6\",\n",
    "    data=mroz,\n",
    ").fit(disp=0)\n",
    "\n",
    "print(\"\\nPROBIT RESULTS (Selection Equation):\")\n",
    "print(probit_selection.summary())\n",
    "\n",
    "# Compute inverse Mills ratio\n",
    "pred_index = probit_selection.predict(mroz, linear=True)\n",
    "mills_ratio = stats.norm.pdf(pred_index) / stats.norm.cdf(pred_index)\n",
    "\n",
    "# Add to dataframe\n",
    "mroz[\"inv_mills\"] = mills_ratio\n",
    "\n",
    "print(\"\\nInverse Mills Ratio (λ̂):\")\n",
    "print(f\"  Mean: {mills_ratio.mean():.4f}\")\n",
    "print(f\"  Std Dev: {mills_ratio.std():.4f}\")\n",
    "print(f\"  Min: {mills_ratio.min():.4f}\")\n",
    "print(f\"  Max: {mills_ratio.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffb60f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: OLS with inverse Mills ratio\n",
    "print(\"\\n\\nHECKMAN STEP 2: Wage Equation with Selection Correction\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Estimate: log(wage) = β₀ + β₁·educ + β₂·exper + β₃·exper² + θ·λ̂ + u\")\n",
    "print(\"Using working women only, but including λ̂ to control for selection\")\n",
    "\n",
    "heckit = smf.ols(\n",
    "    formula=\"lwage ~ educ + exper + I(exper**2) + inv_mills\",\n",
    "    data=mroz[mroz[\"inlf\"] == 1],\n",
    ").fit()\n",
    "\n",
    "table_heckit = pd.DataFrame(\n",
    "    {\n",
    "        \"Coefficient\": heckit.params,\n",
    "        \"Std. Error\": heckit.bse,\n",
    "        \"t-statistic\": heckit.tvalues,\n",
    "        \"p-value\": heckit.pvalues,\n",
    "    },\n",
    ")\n",
    "\n",
    "display(table_heckit.round(4))\n",
    "\n",
    "print(f\"\\nR-squared: {heckit.rsquared:.4f}\")\n",
    "print(f\"Observations: {heckit.nobs:.0f} (working women only)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcd7f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare OLS and Heckman\n",
    "print(\"\\n\\nCOMPARISON: OLS vs HECKMAN SELECTION CORRECTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "comparison_heckman = pd.DataFrame(\n",
    "    {\n",
    "        \"OLS (Biased)\": ols_wage.params,\n",
    "        \"OLS SE\": ols_wage.bse,\n",
    "        \"Heckman\": heckit.params.drop(\"inv_mills\"),\n",
    "        \"Heckman SE\": heckit.bse.drop(\"inv_mills\"),\n",
    "    },\n",
    ")\n",
    "\n",
    "display(comparison_heckman.round(4))\n",
    "\n",
    "print(\"\\nINVERSE MILLS RATIO:\")\n",
    "print(f\"  Coefficient (θ): {heckit.params['inv_mills']:.4f}\")\n",
    "print(f\"  t-statistic: {heckit.tvalues['inv_mills']:.4f}\")\n",
    "print(f\"  p-value: {heckit.pvalues['inv_mills']:.4f}\")\n",
    "\n",
    "if heckit.pvalues[\"inv_mills\"] < 0.05:\n",
    "    print(\"\\n✓ SIGNIFICANT selection bias detected!\")\n",
    "    print(\"  → Heckman correction is necessary\")\n",
    "    print(\"  → OLS estimates are biased\")\n",
    "else:\n",
    "    print(\"\\n✗ No significant selection bias\")\n",
    "    print(\"  → Heckman correction not essential\")\n",
    "    print(\"  → OLS and Heckman give similar results\")\n",
    "\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(\"1. RETURNS TO EDUCATION:\")\n",
    "print(f\"   OLS: {100 * ols_wage.params['educ']:.2f}% per year\")\n",
    "print(f\"   Heckman: {100 * heckit.params['educ']:.2f}% per year\")\n",
    "if abs(ols_wage.params[\"educ\"] - heckit.params[\"educ\"]) > 0.01:\n",
    "    print(\"   → Substantial difference! Selection bias is important\")\n",
    "else:\n",
    "    print(\"   → Similar estimates\")\n",
    "\n",
    "print(\"\\n2. SELECTION BIAS:\")\n",
    "theta = heckit.params[\"inv_mills\"]\n",
    "if theta < 0:\n",
    "    print(f\"   θ = {theta:.4f} < 0\")\n",
    "    print(\"   → Women with LOW unobserved wage offers are more likely to work\")\n",
    "    print(\"   → Positive selection: OLS overestimates returns\")\n",
    "else:\n",
    "    print(f\"   θ = {theta:.4f} > 0\")\n",
    "    print(\"   → Women with HIGH unobserved wage offers are more likely to work\")\n",
    "    print(\"   → Negative selection: OLS underestimates returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bf8d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize selection bias\n",
    "print(\"\\nVISUALIZING SELECTION BIAS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Panel A: Inverse Mills ratio by work status\n",
    "working = mroz[mroz[\"inlf\"] == 1][\"inv_mills\"]\n",
    "not_working = mroz[mroz[\"inlf\"] == 0][\"inv_mills\"]\n",
    "\n",
    "axes[0].hist(working, bins=30, alpha=0.7, label=\"Working\", edgecolor=\"black\")\n",
    "axes[0].hist(not_working, bins=30, alpha=0.7, label=\"Not Working\", edgecolor=\"black\")\n",
    "axes[0].set_xlabel(\"Inverse Mills Ratio (λ̂)\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "axes[0].set_title(\"Distribution of λ̂ by Work Status\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel B: Predicted wages vs inverse Mills ratio (working women)\n",
    "working_data = mroz[mroz[\"inlf\"] == 1].copy()\n",
    "axes[1].scatter(working_data[\"inv_mills\"], working_data[\"lwage\"], alpha=0.5, s=20)\n",
    "axes[1].set_xlabel(\"Inverse Mills Ratio (λ̂)\")\n",
    "axes[1].set_ylabel(\"Log Wage\")\n",
    "axes[1].set_title(\"Log Wage vs Selection Correction Term\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add regression line\n",
    "z = np.polyfit(working_data[\"inv_mills\"], working_data[\"lwage\"], 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(\n",
    "    working_data[\"inv_mills\"].min(), working_data[\"inv_mills\"].max(), 100\n",
    ")\n",
    "axes[1].plot(\n",
    "    x_line, p(x_line), \"r--\", linewidth=2, alpha=0.7, label=f\"Slope={z[0]:.3f}\"\n",
    ")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nINTERPRETATION:\")\n",
    "print(\"Panel A: λ̂ is higher for working women (higher selection pressure)\")\n",
    "print(\"Panel B: Correlation between λ̂ and wages shows selection effect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d26e51",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This chapter introduced **limited dependent variable models** for outcomes that are restricted in some way.\n",
    "\n",
    "### Key Models and Applications\n",
    "\n",
    "1. **Binary Response (Logit/Probit)**:\n",
    "   - Outcome: 0 or 1\n",
    "   - Models: Linear probability, logit, probit\n",
    "   - Interpretation: Marginal effects (not coefficients!)\n",
    "   - Applications: Labor force participation, college attendance, loan default\n",
    "\n",
    "2. **Fractional Response**:\n",
    "   - Outcome: Proportion in (0, 1)\n",
    "   - Model: Fractional logit (quasi-MLE)\n",
    "   - Applications: Savings rate, portfolio shares\n",
    "\n",
    "3. **Count Data (Poisson)**:\n",
    "   - Outcome: Non-negative integer\n",
    "   - Models: Poisson, quasi-Poisson (overdispersion)\n",
    "   - Interpretation: Semi-elasticities\n",
    "   - Applications: Number of arrests, patents, doctor visits\n",
    "\n",
    "4. **Corner Solution (Tobit)**:\n",
    "   - Outcome: Many zeros, positive continuous values\n",
    "   - Model: Tobit (Type I censored regression)\n",
    "   - Interpretation: Effects on latent variable and observed variable differ\n",
    "   - Applications: Hours worked, charitable contributions, R&D spending\n",
    "\n",
    "5. **Censored/Truncated Regression**:\n",
    "   - **Censored**: Observe all individuals, some outcomes censored\n",
    "   - **Truncated**: Only observe individuals meeting some condition\n",
    "   - Applications: Duration models, survival analysis\n",
    "\n",
    "6. **Sample Selection (Heckman)**:\n",
    "   - Problem: Sample is non-randomly selected\n",
    "   - Solution: Two-step procedure with inverse Mills ratio\n",
    "   - Applications: Wages (only observe workers), firm performance (only observe survivors)\n",
    "\n",
    "### Key Principles\n",
    "\n",
    "**Don't use OLS for limited dependent variables!**\n",
    "\n",
    "Why?\n",
    "- Predictions can be outside feasible range\n",
    "- Heteroskedasticity is inherent\n",
    "- Effects are nonlinear\n",
    "- Inefficient or biased estimates\n",
    "\n",
    "**Use specialized models** that respect the nature of the outcome:\n",
    "\n",
    "| Outcome Type | Model | Key Feature |\n",
    "|---|---|---|\n",
    "| Binary (0/1) | Logit/Probit | Probabilities in [0, 1] |\n",
    "| Fraction (0-1) | Fractional logit | Handles corner solutions |\n",
    "| Count (0, 1, 2, ...) | Poisson | Non-negative integers |\n",
    "| Corner (many 0s, positive) | Tobit | Two-part model |\n",
    "| Censored | Censored regression | Explicit censoring |\n",
    "| Truncated | Truncated regression | Sample selection |\n",
    "| Non-random sample | Heckman | Inverse Mills ratio |\n",
    "\n",
    "### Interpretation Guidelines\n",
    "\n",
    "1. **Binary models**: Report **marginal effects**, not coefficients\n",
    "2. **Poisson**: Coefficients are **semi-elasticities** (percentage changes)\n",
    "3. **Tobit**: Distinguish effects on **latent** vs **observed** variable\n",
    "4. **Heckman**: Test significance of **inverse Mills ratio** (selection bias)\n",
    "\n",
    "### Practical Recommendations\n",
    "\n",
    "✓ **Always check**:\n",
    "- Nature of dependent variable (binary, count, corner, etc.)\n",
    "- Distribution of outcomes (many zeros? censoring?)\n",
    "- Sample selection issues (non-random sample?)\n",
    "\n",
    "✓ **Use robust standard errors**:\n",
    "- Heteroskedasticity is inherent in limited dependent variable models\n",
    "- Always use HC or clustered standard errors\n",
    "\n",
    "✓ **Report marginal effects**:\n",
    "- Coefficients alone are hard to interpret\n",
    "- Calculate average marginal effects (AME) or marginal effects at the mean (MEM)\n",
    "\n",
    "✓ **Check predictions**:\n",
    "- Ensure predictions are in feasible range\n",
    "- Plot fitted vs actual values\n",
    "\n",
    "✗ **Don't use**:\n",
    "- OLS for binary outcomes (use logit/probit)\n",
    "- OLS for count data (use Poisson)\n",
    "- OLS on selected sample without correction (use Heckman)\n",
    "\n",
    "**Next Steps**: Chapter 18 covers **advanced time series topics** including cointegration, vector autoregressions, and forecasting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9210b4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Visual summary\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "ax.axis(\"off\")\n",
    "\n",
    "# Title\n",
    "ax.text(\n",
    "    0.5,\n",
    "    0.98,\n",
    "    \"Chapter 17: Limited Dependent Variables - Model Selection Guide\",\n",
    "    ha=\"center\",\n",
    "    fontsize=16,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "# Decision tree\n",
    "y_start = 0.92\n",
    "spacing = 0.09\n",
    "\n",
    "# Question 1: Nature of y\n",
    "ax.text(\n",
    "    0.5,\n",
    "    y_start,\n",
    "    \"What is the nature of your dependent variable?\",\n",
    "    ha=\"center\",\n",
    "    fontsize=12,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "# Binary\n",
    "ax.add_patch(\n",
    "    plt.Rectangle(\n",
    "        (0.05, y_start - 0.10), 0.18, 0.07, fill=True, alpha=0.2, color=\"blue\"\n",
    "    )\n",
    ")\n",
    "ax.text(\n",
    "    0.14, y_start - 0.065, \"Binary (0/1)\", ha=\"center\", fontsize=10, fontweight=\"bold\"\n",
    ")\n",
    "ax.text(\n",
    "    0.14, y_start - 0.09, \"→ Logit/Probit\", ha=\"center\", fontsize=9, color=\"darkblue\"\n",
    ")\n",
    "\n",
    "# Fraction\n",
    "ax.add_patch(\n",
    "    plt.Rectangle(\n",
    "        (0.24, y_start - 0.10), 0.18, 0.07, fill=True, alpha=0.2, color=\"green\"\n",
    "    )\n",
    ")\n",
    "ax.text(\n",
    "    0.33, y_start - 0.065, \"Fraction (0-1)\", ha=\"center\", fontsize=10, fontweight=\"bold\"\n",
    ")\n",
    "ax.text(\n",
    "    0.33,\n",
    "    y_start - 0.09,\n",
    "    \"→ Fractional Logit\",\n",
    "    ha=\"center\",\n",
    "    fontsize=9,\n",
    "    color=\"darkgreen\",\n",
    ")\n",
    "\n",
    "# Count\n",
    "ax.add_patch(\n",
    "    plt.Rectangle(\n",
    "        (0.43, y_start - 0.10), 0.18, 0.07, fill=True, alpha=0.2, color=\"orange\"\n",
    "    )\n",
    ")\n",
    "ax.text(\n",
    "    0.52,\n",
    "    y_start - 0.065,\n",
    "    \"Count (0,1,2,...)\",\n",
    "    ha=\"center\",\n",
    "    fontsize=10,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.text(0.52, y_start - 0.09, \"→ Poisson\", ha=\"center\", fontsize=9, color=\"darkorange\")\n",
    "\n",
    "# Corner\n",
    "ax.add_patch(\n",
    "    plt.Rectangle((0.62, y_start - 0.10), 0.18, 0.07, fill=True, alpha=0.2, color=\"red\")\n",
    ")\n",
    "ax.text(\n",
    "    0.71,\n",
    "    y_start - 0.065,\n",
    "    \"Corner (many 0s)\",\n",
    "    ha=\"center\",\n",
    "    fontsize=10,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.text(0.71, y_start - 0.09, \"→ Tobit\", ha=\"center\", fontsize=9, color=\"darkred\")\n",
    "\n",
    "# Censored/truncated\n",
    "ax.add_patch(\n",
    "    plt.Rectangle(\n",
    "        (0.81, y_start - 0.10), 0.14, 0.07, fill=True, alpha=0.2, color=\"purple\"\n",
    "    )\n",
    ")\n",
    "ax.text(0.88, y_start - 0.065, \"Censored\", ha=\"center\", fontsize=10, fontweight=\"bold\")\n",
    "ax.text(0.88, y_start - 0.09, \"→ Cens. Reg\", ha=\"center\", fontsize=9, color=\"purple\")\n",
    "\n",
    "# Question 2: Sample selection\n",
    "y_select = y_start - 0.18\n",
    "ax.text(\n",
    "    0.5,\n",
    "    y_select,\n",
    "    \"Is your sample randomly selected?\",\n",
    "    ha=\"center\",\n",
    "    fontsize=11,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "ax.add_patch(\n",
    "    plt.Rectangle(\n",
    "        (0.2, y_select - 0.08), 0.25, 0.05, fill=True, alpha=0.15, color=\"green\"\n",
    "    )\n",
    ")\n",
    "ax.text(0.325, y_select - 0.055, \"YES → Use standard model\", ha=\"center\", fontsize=9)\n",
    "\n",
    "ax.add_patch(\n",
    "    plt.Rectangle(\n",
    "        (0.55, y_select - 0.08), 0.25, 0.05, fill=True, alpha=0.15, color=\"red\"\n",
    "    )\n",
    ")\n",
    "ax.text(0.675, y_select - 0.055, \"NO → Heckman correction\", ha=\"center\", fontsize=9)\n",
    "\n",
    "# Interpretation guide\n",
    "y_interp = y_select - 0.18\n",
    "ax.text(\n",
    "    0.5, y_interp, \"INTERPRETATION GUIDE\", ha=\"center\", fontsize=12, fontweight=\"bold\"\n",
    ")\n",
    "\n",
    "interp_items = [\n",
    "    (\"Logit/Probit\", \"Marginal effects (∂P/∂x), not coefficients\"),\n",
    "    (\"Poisson\", \"Semi-elasticities (% change in E(y))\"),\n",
    "    (\"Tobit\", \"Effect on latent y* vs observed y\"),\n",
    "    (\"Heckman\", \"Test inv. Mills ratio for selection bias\"),\n",
    "]\n",
    "\n",
    "y_pos = y_interp - 0.06\n",
    "for model, interpretation in interp_items:\n",
    "    ax.text(0.15, y_pos, f\"{model}:\", ha=\"left\", fontsize=9, fontweight=\"bold\")\n",
    "    ax.text(0.35, y_pos, interpretation, ha=\"left\", fontsize=8)\n",
    "    y_pos -= 0.04\n",
    "\n",
    "# Common mistakes\n",
    "y_mistakes = y_interp - 0.28\n",
    "ax.text(\n",
    "    0.5,\n",
    "    y_mistakes,\n",
    "    \"⚠️ COMMON MISTAKES TO AVOID\",\n",
    "    ha=\"center\",\n",
    "    fontsize=11,\n",
    "    fontweight=\"bold\",\n",
    "    color=\"red\",\n",
    ")\n",
    "\n",
    "mistakes = [\n",
    "    \"✗ Using OLS for binary outcomes (wrong predictions, inefficient)\",\n",
    "    \"✗ Interpreting logit/probit coefficients as marginal effects\",\n",
    "    \"✗ Ignoring sample selection (biased estimates)\",\n",
    "    \"✗ Not using robust standard errors (all LDV models have heteroskedasticity)\",\n",
    "]\n",
    "\n",
    "y_pos = y_mistakes - 0.05\n",
    "for mistake in mistakes:\n",
    "    ax.text(0.5, y_pos, mistake, ha=\"center\", fontsize=8, color=\"darkred\")\n",
    "    y_pos -= 0.035\n",
    "\n",
    "# Best practices\n",
    "y_best = y_mistakes - 0.20\n",
    "ax.text(\n",
    "    0.5,\n",
    "    y_best,\n",
    "    \"✓ BEST PRACTICES\",\n",
    "    ha=\"center\",\n",
    "    fontsize=11,\n",
    "    fontweight=\"bold\",\n",
    "    color=\"green\",\n",
    ")\n",
    "\n",
    "practices = [\n",
    "    \"✓ Match model to outcome type (binary→logit, count→Poisson, etc.)\",\n",
    "    \"✓ Always report marginal effects (not raw coefficients)\",\n",
    "    \"✓ Use robust/clustered standard errors\",\n",
    "    \"✓ Check predictions are in feasible range\",\n",
    "    \"✓ Test for specification (overdispersion, selection bias)\",\n",
    "]\n",
    "\n",
    "y_pos = y_best - 0.05\n",
    "for practice in practices:\n",
    "    ax.text(0.5, y_pos, practice, ha=\"center\", fontsize=8, color=\"darkgreen\")\n",
    "    y_pos -= 0.035\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc834a5",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "notebooks//ipynb,markdown//md,scripts//py"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
