# Measurable Learning Objectives (MLOs)

This document contains the measurable learning objectives for each chapter of the econometrics course.

## Chapter 2: The Simple Regression Model

### 2.1
List the assumptions of the simple linear regression model and interpret the parameters in the model.

### 2.2
Apply the formulas for the slope and intercept estimates using ordinary least squares estimation.

### 2.3
Use the formulas to calculate the fitted values and residuals, and list the properties of these statistics on any sample of data.

### 2.4
Compute and interpret the R-squared of a simple regression.

### 2.5
List and interpret the assumptions used to obtain unbiasedness of the ordinary least squares (OLS) estimator.

### 2.6
Explain the meaning of the homoskedasticity (constant variance) assumption, and discuss its role in calculating the sampling variance of the OLS estimators.

### 2.7
Discuss the limitations of the simple regression model for uncovering causal effects.

### 2.8
Interpret the simple regression estimates in the case of a binary (dummy) explanatory variable.

### 2.9
Describe the potential outcomes framework for causal inference and explain how experimental interventions (randomized controlled trials) can be analyzed using simple regression.

## Chapter 3: Multiple Regression Analysis - Estimation

### 3.1
Explain why we move beyond the simple regression model to the multiple regression model.

### 3.2
List the names and meanings of all variables and parameters in the linear regression equation.

### 3.3
Interpret the parameters in a multiple linear regression model, including cases where independent variables are nonlinear functions of other variables.

### 3.4
Show the set of equations that are solved for the OLS estimates on any sample of data.

### 3.5
Interpret the OLS estimates in the sample regression function as ceteris paribus effects.

### 3.6
Use the OLS estimates to compute the fitted values and residuals for any observation in a sample.

### 3.7
Explain the relationship between multiple and simple regression coefficients, including the "partialling out" interpretation of multiple regression.

### 3.8
Interpret the R-squared from a multiple regression.

### 3.9
Interpret each of the five Gauss-Markov assumptions.

### 3.10
List the fewest assumptions under which the ordinary least squares estimators are unbiased.

### 3.11
Determine the bias caused by omitting an important variable when using OLS estimation.

### 3.12
Appraise the role of the homoskedasticity assumption in multiple regression analysis.

### 3.13
Produce the formula for the variance of the OLS estimators under the Gauss-Markov assumptions and explain the role of each of the three components in the variance formula.

### 3.14
Discuss the consequences of multicollinearity, and explain why multicollinearity does not violate any Gauss-Markov assumption.

### 3.15
Produce the formula for the unbiased estimator of the error variance and the standard error for each of the OLS slope estimators.

### 3.16
Explain the meaning of the Gauss-Markov theorem, and its important in justifying the use of OLS.

### 3.17
Provide examples of the different ways that multiple regression analysis is used in economics and the behavioral sciences.

## Chapter 4: Multiple Regression Analysis - Inference

### 4.1
Explain the classical linear model assumptions with a particular emphasis on why the normality assumption is needed for exact statistical inference.

### 4.2
Describe the distribution of the OLS estimators and the standardized estimators under the classical linear model assumptions.

### 4.3
State the null hypothesis that a population parameter equals a given value, and explain the difference between one-sided and two-sided alternatives.

### 4.4
Express the formula for a t statistic for testing a given null hypothesis.

### 4.5
Employ a software package or statistical tables to obtain critical values for one-tailed and two-tailed t tests.

### 4.6
Record the p-value for hypothesis testing using the output of regression software.

### 4.7
Interpret the meaning of a p-value and know how it is used in hypothesis testing.

### 4.8
Explain the difference between practical (or economic) significance and statistical significance.

### 4.9
Express mathematically a null hypothesis involving more than one parameter.

### 4.10
Explain the purpose of multiple hypothesis testing, especially for testing exclusion restrictions.

### 4.11
Produce the sum of squared residuals and R-squared forms of the F statistic, and know when the latter is applicable. Be able to use a regression software package to obtain an F statistic.

## Chapter 5: Multiple Regression Analysis - OLS Asymptotics

### 5.1
Define the notion of a consistent estimator and discuss why consistency is an important property of estimators.

### 5.2
List the weakest set of assumptions under which the OLS estimators are consistent for the population parameters.

### 5.3
Provide specific examples showing why the normality assumption for the error term is often too strong an assumption.

### 5.4
Explain how the central limit theorem allows us to relax the normality assumption when computing asymptotic confidence intervals and conducting asymptotic inference via t, F, and LM tests.

### 5.5
Articulate why testing whether the errors are normally distributed does not lead to an actionable response.

## Chapter 6: Multiple Regression Analysis - Further Issues

### 6.1
Determine the effects on coefficient estimates, standard errors, t statistics, and the R-squared when changing the units of measurement of the dependent variable or an explanatory variable.

### 6.2
Interpret the coefficients in a regression model where all variables have been standardized, and explain why such an equation can be useful.

### 6.3
Explain why the logarithmic transformation is commonly used for dependent variables and explanatory variables that are strictly positive.

### 6.4
Interpret the coefficients in a regression model when the dependent variable is in logarithmic form and one or more explanatory variables is in logarithmic form.

### 6.5
Explain why changing the units of measurement of any strictly positive variable does not change the slope coefficients when that variable appears in logarithmic form.

### 6.6
List the problems with the transformation log(1 + y) when y can take the value zero.

### 6.7
Interpret coefficients in a model containing quadratic terms, and explain how to test the hypothesis that the effect of a variable is constant.

### 6.8
Interpret the coefficients in a model that includes interaction terms.

### 6.9
Define an average partial effect and explain the role of centering explanatory variables about their averages before creating interaction terms.

### 6.10
Produce the formula for the adjusted R-squared, and explain why it is sometimes preferred as a goodness-of-fit measure over the usual R-squared.

### 6.11
Demonstrate by example how it is possible to control for too many variables in an regression analysis.

### 6.12
Compare a confidence interval for the average value of a dependent variable and a prediction interval for a future value of the dependent variable.

### 6.13
List the steps to obtain a prediction interval given a set of values for the explanatory variables.

### 6.14
List the steps for obtaining a predicted value for the level of y when log(y) is the dependent variable in the linear regression.

## Chapter 7: Multiple Regression Analysis - Qualitative Regressors

### 7.1
Define a binary variable (or dummy variable) and give examples of how such variables are used in regression analysis.

### 7.2
Interpret the coefficient on a dummy variable in a multiple regression equation, including when the dependent variable is in logarithmic form.

### 7.3
Explain the purpose of including interactions among dummy variables in a regression model, and interpret the coefficients in such an equation.

### 7.4
Explain why one might want to include interactions between a dummy variable and a continuous variable in a regression model, and interpret the coefficients in such an equation.

### 7.5
Demonstrate how to define dummy variables for multiple categories, and how to include them in regression analysis.

### 7.6
Explain the purpose of the Chow test and compute the test statistic.

### 7.7
Explain the purpose of the Chow test that allows different intercepts under the null hypothesis and compute the statistic.

### 7.8
Define the linear probability model and interpret the estimated coefficients in such a model.

### 7.9
Explain how dummy variables are used in regression adjustment for program evaluation, including the distinction between restricted and unrestricted regression adjustment.

### 7.10
Interpret the coefficients in a regression model when the outcome variable is generally discrete.

## Chapter 8: Heteroskedasticity

### 8.1
Define the problem of heteroskedasticity in a multiple regression model.

### 8.2
List the statistical properties of the ordinary least squares (OLS) estimator in the presence of heteroskedasticity.

### 8.3
Reproduce the formula for the heteroskedasticity-robust standard error for an OLS coefficient, and describe how it differs from the usual OLS standard error.

### 8.4
Explain why one might want to test for heteroskedasticity in the error term.

### 8.5
List the steps for obtaining the Breusch-Pagan and White tests for the null hypothesis of homoskedasticity.

### 8.6
Obtain the p-value for the Breusch-Pagan and White tests.

### 8.7
Explain why one might wish to use weighted least squares (WLS) in addition to ordinary least squares for estimating the parameters in a linear model.

### 8.8
Explain how to estimate the weights when using the feasible version of generalized least squares.

### 8.9
Summarize the conclusions if OLS and WLS give significantly different parameter estimates.

### 8.10
Describe the consquences of misspecifying the heteroskedasticity function when using WLS.

### 8.11
Apply WLS to the linear probability model and list the shortcomings of doing so.

## Chapter 9: Specification and Data Issues

### 9.1
Explain the purpose of tests to detect functional form misspecification and implement the regression specification error test (RESET).

### 9.2
Give an example of nonnested models and explain what it means for two models to be nonnested.

### 9.3
Compute the Davidson-MacKinnon test of a model against a nonnested alternative.

### 9.4
Describe the proxy variable solution to the omitted variables problem, including the properties of a good proxy variable.

### 9.5
Explain the implications of allowing random (rather than constant) coefficients in a linear model when the coefficients are independent of the explanatory variables.

### 9.6
Discuss the statistical implications for OLS in the presence of measurement error in the outcome variable.

### 9.7
Derive the attenuation bias of the OLS estimator in the simple regression model when the explanatory variable follows the classical errors-in-variables assumptions.

### 9.8
Describe the issues caused by data missing completely at random, and apply the missing indicator method to such cases.

### 9.9
In the absence of random sampling, explain the implications for OLS of both exogenous and endogenous sampling schemes.

### 9.10
Describe the issues caused by outliers in data and use the studentized residuals to detect outliers.

### 9.11
Explain the purpose of and the difference between the least absolute deviations (LAD) estimator and the OLS estimator.
